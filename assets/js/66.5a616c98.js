(window.webpackJsonp=window.webpackJsonp||[]).push([[66],{788:function(a,r,e){"use strict";e.r(r);var s=e(70),t=Object(s.a)({},(function(){var a=this,r=a.$createElement,e=a._self._c||r;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h2",{attrs:{id:"常用配置"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#常用配置"}},[a._v("#")]),a._v(" 常用配置")]),a._v(" "),e("p",[a._v("Spark 的参数调优主要集中在执行计算任务的 Executor 端，分为"),e("strong",[a._v("硬件资源类")]),a._v("、"),e("strong",[a._v("Shuffle 类")]),a._v("和 "),e("strong",[a._v("Spark SQL")]),a._v(" 三大类。")]),a._v(" "),e("p",[a._v("Spark 调优的本质是对物力资源的平衡利用。")]),a._v(" "),e("h2",{attrs:{id:"_1-硬件资源类"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-硬件资源类"}},[a._v("#")]),a._v(" 1. 硬件资源类")]),a._v(" "),e("p",[a._v("硬件资源类包含的是与 CPU、内存、磁盘有关的配置项，调优的切入点是瓶颈，定位瓶颈的有效方法之一，就是从硬件的角度出发。")]),a._v(" "),e("h3",{attrs:{id:"_1-1-cpu-相关"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-cpu-相关"}},[a._v("#")]),a._v(" 1.1 CPU 相关")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.cores.max")]),a._v("：指定集群范围内 Spark 使用的最大核心数")]),a._v(" "),e("li",[e("strong",[a._v("spark.executor.cores")]),a._v("：指定 Executor 范围内 Spark 使用的核心数")]),a._v(" "),e("li",[e("strong",[a._v("spark.task.cpus")]),a._v("：指定 task 级别使用的核心数")])]),a._v(" "),e("p",[a._v("为了充分利用划拨给 Spark 集群的每一颗 CPU 核，需要设置恰当的并行度：")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.default.parallelism")]),a._v("：未指定明确分区时使用的默认并行度，Yarn 模式下默认取所有 executor core 的数量。")]),a._v(" "),e("li",[e("strong",[a._v("spark.sql.shuffle.partitions")]),a._v("：明确指定数据关联或聚合操作中 Reduce 端的分区数量，默认 "),e("code",[a._v("200")]),a._v("。")]),a._v(" "),e("li",[e("strong",[a._v("spark.sql.files.maxPartitionBytes")]),a._v("：指定 Spark SQL 读取文件时每个分区的最大大小，可以影响 partition 数，默认 128MB，仅对基于文件的数据源生效，如 parquet、json、orc。")])]),a._v(" "),e("p",[a._v("Spark 需要区分数据的"),e("strong",[a._v("并行度（Parallelism）"),e("strong",[a._v("以及")]),a._v("并行计算任务（Paralleled Tasks）")]),a._v(",前者针对数据分片，后者针对计算任务，决定了在任一时刻整个集群能够同时计算的任务数量。")]),a._v(" "),e("p",[a._v("一个 Executor 中，task 并行计算的上限是 spark.executor.cores 与 spark.task.cpus 的商；一个 Spark 作业中，task 并行计算的上限则是在前面的基础上再乘以 Executor 数。")]),a._v(" "),e("h3",{attrs:{id:"_1-2-内存相关"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-内存相关"}},[a._v("#")]),a._v(" 1.2 内存相关")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.executor.memory")]),a._v("：单个 Executor 堆内内存总大小")]),a._v(" "),e("li",[e("strong",[a._v("spark.memory.offHeap.size")]),a._v("：单个 Executor 堆外内存总大小")]),a._v(" "),e("li",[e("strong",[a._v("spark.memory.fraction")]),a._v("：堆内内存中 Storage 和 Execution 总共占的内存比例，默认 0.75")]),a._v(" "),e("li",[e("strong",[a._v("spark.memory.storageFraction")]),a._v("： Storage 与 Execution 之间的内存比例，默认 0.5")]),a._v(" "),e("li",[e("strong",[a._v("spark.rdd.compress")]),a._v("：是否开启 RDD 压缩，会对以序列化方式进行缓存的 RDD 进行压缩，默认 false")]),a._v(" "),e("li",[e("strong",[a._v("spark.broadcast.compress")]),a._v("：是否开启 Broadcast 压缩，默认 false")])]),a._v(" "),e("p",[a._v("Spark 的内存调整需要注意几个地方：")]),a._v(" "),e("ol",[e("li",[e("p",[a._v("堆内与堆外内存比例，Spark 在使用堆外内存时，应用了一种紧凑的二进制格式来存储对象，"),e("strong",[a._v("对于定长类型的数据格式十分有效")]),a._v("，而对于非定长的数据，则会通过指针、偏移量、长度等进行记录：")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://static001.geekbang.org/resource/image/51/2c/516c0e41e6757193533c8dfa33f9912c.jpg",alt:""}})]),a._v(" "),e("p",[a._v("因此在表示复杂类型时，反而会因为过多的指针与偏移地址导致访问效率大打折扣，同时更加难以管理。")])]),a._v(" "),e("li",[e("p",[a._v("User Memory 与 Spark 可用内存的比例，"),e("strong",[a._v("spark.memory.fraction")]),a._v(" 参数决定着两者如何瓜分堆内内存，它的系数越大，Spark 可支配的内存越多，如果自定义数据结构生成的对象较少，可以考虑适当调大该值。")])]),a._v(" "),e("li",[e("p",[a._v("平衡 Execution Memory 与 Storage Memory 比例，在统一内存管理模式下，spark.memory.storageFraction 显得不是那么无关紧要，但如果作业是偏向缓存型的，那么就有必要调大该值，以保证数据的全量缓存。但缓存的 RDD 会对作业造成以下几个方面的影响：")]),a._v(" "),e("ul",[e("li",[a._v("挤占 Execution Memory 的空间")]),a._v(" "),e("li",[a._v("RDD 缓存会占用老年代空间，一旦触发 Full GC，会有较大的开销")]),a._v(" "),e("li",[a._v("可以通过开启缓存序列化以及 RDD 压缩（spark.rdd.compress）来节省内存，但代价则是对 CPU 的消耗。")])])])]),a._v(" "),e("h3",{attrs:{id:"_1-3-磁盘相关"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-磁盘相关"}},[a._v("#")]),a._v(" 1.3 磁盘相关")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.local.dir")]),a._v("：指定 RDD 缓存的磁盘目录")])]),a._v(" "),e("h2",{attrs:{id:"_2-shuffle-类"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2-shuffle-类"}},[a._v("#")]),a._v(" 2. Shuffle 类")]),a._v(" "),e("p",[a._v("Shuffle 类是专门针对 Shuffle 操作的。在绝大多数场景下，Shuffle 都是性能瓶颈。")]),a._v(" "),e("p",[a._v("官方地址："),e("a",{attrs:{href:"http://spark.apache.org/docs/latest/configuration.html#shuffle-behavior",target:"_blank",rel:"noopener noreferrer"}},[a._v("Shuffle Behavior"),e("OutboundLink")],1),a._v("。")]),a._v(" "),e("p",[a._v("Shuffle 能进行的调优参数较少，分为 Mapper 和 Reducer 两部分。")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.shuffle.file.buffer")]),a._v("： Map 输出端写入缓冲区大小，默认 32KB，通过增大 buffer ，减少落盘频率")]),a._v(" "),e("li",[e("strong",[a._v("spark.io.compression.lz4.block Size")]),a._v("： 压缩时的 block 大小，默认 32 KB，增大该值可以减小 Shuffle file 大小，推荐 512KB")]),a._v(" "),e("li",[e("strong",[a._v("spark.reducer.maxSizeInFlight")]),a._v("： Reduce 端的读取缓冲区大小，默认 48MB，通过增大 buffer 增加每次拉拉取的数据，减少网络请求")]),a._v(" "),e("li",[e("strong",[a._v("spark.shuffle.sort.bypassMergeThreshold")]),a._v("：默认 "),e("code",[a._v("200")]),a._v("，当 Reducer 并行度低于该阈值时，Shuffle 在 Map 端均不会引入排序，但每个 partition 都需要维护一个 buffer")])]),a._v(" "),e("p",[a._v("关于 Bypass 与 Unsafe 的：")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.file.transferTo")]),a._v("： 开启后会以 NIO 传输文件，默认 true")]),a._v(" "),e("li",[e("strong",[a._v("spark.shuffle.unsafe.file.output.buffer")]),a._v("： 在 Shuffle 期间合并文件可用的 buffer，默认 32KB，对于高负载推荐 1MB")])]),a._v(" "),e("p",[a._v("同时还有一些稳定性参数设置：")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.shuffle.compress")]),a._v("：开启 shuffle 压缩，默认开启")]),a._v(" "),e("li",[e("strong",[a._v("spark.io.compression.codec")]),a._v("：指定 shuffle 中间文件压缩所使用压缩类型，默认 lz4")]),a._v(" "),e("li",[e("strong",[a._v("spark.shuffle.io.maxRetries")]),a._v("： shuffle I/O 失败的最大重试次数，默认 "),e("code",[a._v("3")])]),a._v(" "),e("li",[e("strong",[a._v("spark.shuffle.io.retryWait")]),a._v("： shuffle I/O 失败后的重试间隔，默认 "),e("code",[a._v("5s")])])]),a._v(" "),e("h2",{attrs:{id:"_3-spark-sql"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-spark-sql"}},[a._v("#")]),a._v(" 3. Spark SQL")]),a._v(" "),e("p",[a._v("Spark SQL 早已演化为新一代的底层优化引擎，专门梳理出一类配置项，去充分利用 Spark SQL 的先天性能优势。")]),a._v(" "),e("p",[a._v("Spark SQL 的配置项中，AQE 引入的三个特性（自动分区合并、自动数据倾斜处理和 Join 策略调整）对执行性能影响最大。")]),a._v(" "),e("p",[a._v("AQE 是 Spark SQL 的一种动态优化机制，在运行时，每当 Shuffle Map 阶段执行完毕，AQE 都会结合这个阶段的统计信息，基于既定的规则动态地调整、修正尚未执行的逻辑计划和物理计划，来完成对原始查询语句的运行时优化。")]),a._v(" "),e("p",[a._v("AQE 优化机制触发的时机是 Shuffle Map 阶段执行完毕，也就是说 AQE 优化的频次与执行计划中 Shuffle 的频率一样。")]),a._v(" "),e("p",[a._v("AQE 默认是关闭的，需要设置 "),e("strong",[a._v("spark.sql.adaptive.enabled")]),a._v(" 为 true 开启。")]),a._v(" "),e("p",[a._v("AQE 既定的规则和策略主要有 4 个，分为 1 个逻辑规则和 3 个物理优化策略：")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://static001.geekbang.org/resource/image/1c/d5/1cfef782e6dfecce3c9252c6181388d5.jpeg",alt:""}})]),a._v(" "),e("h3",{attrs:{id:"_3-1-自动分区合并"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-自动分区合并"}},[a._v("#")]),a._v(" 3.1 自动分区合并")]),a._v(" "),e("p",[a._v("首先设置 "),e("code",[a._v("spark.sql.adaptive.coalescePartitions.enabled")]),a._v(" 为 true 开启自动分区合并功能。")]),a._v(" "),e("p",[e("strong",[a._v("自动分区合并发生在 Reducer 端")]),a._v("，受到两个参数影响：")]),a._v(" "),e("ul",[e("li",[e("strong",[a._v("spark.sql.adaptive.advisoryPartitionSizeInBytes")]),a._v("：指定 RDD 分区的推荐大小。")]),a._v(" "),e("li",[e("strong",[a._v("spark.sql.adaptive.coalescePartitions.minPartitionNum")]),a._v("：指定 RDD 最小分区数，避免无法充分利用 CPU，不指定则取默认并行度。")])]),a._v(" "),e("p",[a._v("Spark 可以在 Shuffle 过程中获取数据大小相关信息，然后通过这两个维度计算出各自的 RDD 分区数据大小，取其中的较小值作为指定的尺寸，接着按照分区号进行扫描，一旦扫描的数据超过该值就会进行合并：")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://static001.geekbang.org/resource/image/da/4f/dae9dc8b90c2d5e0cf77180ac056a94f.jpg",alt:""}})]),a._v(" "),e("h3",{attrs:{id:"_3-2-自动数据倾斜处理"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-自动数据倾斜处理"}},[a._v("#")]),a._v(" 3.2 自动数据倾斜处理")]),a._v(" "),e("p",[a._v("Spark 3.0 的自动数据倾斜暂且只支持 Sort Merge Join，并且需要结合几个参数实现大分区划分为小分区。")]),a._v(" "),e("p",[a._v("首先需要设置 "),e("strong",[a._v("spark.sql.adaptive.skewJoin.enabled")]),a._v(" 为 true 开启该功能。")]),a._v(" "),e("p",[a._v("AQE 会统计所有数据分区大小并排序，得到中位数作为"),e("strong",[a._v("放大基数")]),a._v("：")]),a._v(" "),e("ul",[e("li",[e("p",[e("strong",[a._v("spark.sql.adaptive.skewJoin.skewedPartitionFactor")]),a._v("：大于放大基数一定倍率的分区有可能判定为倾斜分区，默认取 "),e("code",[a._v("5")]),a._v("。")])]),a._v(" "),e("li",[e("p",[e("strong",[a._v("spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes")]),a._v("：有可能被认定为倾斜分区，分区大小还需要大于该设定值（默认 "),e("code",[a._v("256m")]),a._v("），才能判定为倾斜分区")])])]),a._v(" "),e("p",[a._v("倾斜分区最后又会根据 "),e("code",[a._v("spark.sql.adaptive.advisoryPartitionSizeInBytes")]),a._v(" 指定的分区大小进行拆分。")]),a._v(" "),e("h3",{attrs:{id:"_3-3-join-策略调整"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-join-策略调整"}},[a._v("#")]),a._v(" 3.3 Join 策略调整")]),a._v(" "),e("p",[a._v("在 AQE 之前，开发人员可以指定 "),e("code",[a._v("spark.sql.autoBroadcastJoinThreshold")]),a._v("（默认 "),e("code",[a._v("10m")]),a._v("）对数据关联操作进行主动降级，只要有其中一张表小于该值就可以将降级为 Broadcast Join。该参数存在两个问题：")]),a._v(" "),e("ul",[e("li",[a._v("Spark 对小表尺寸的误判时有发生，导致降级失败，可靠性较差")]),a._v(" "),e("li",[a._v("某张表可能在 filter 后满足了 Broadcast Join 的条件，但 Spark 先前选择了 Shuffle Join 的执行计划，无法动态改变。")])]),a._v(" "),e("p",[a._v("AQE 解决了上述问题，AQE 可以动态调整 Join 策略，并且在运行时获取表数据的大小比起编译时更加准确。")]),a._v(" "),e("p",[a._v("AQE 动态调整 Join 策略有以下几个组合：")]),a._v(" "),e("ul",[e("li",[a._v("只要存在一个表小于广播阈值，且该表非空的数据分区比例大于 "),e("code",[a._v("spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin")]),a._v("（默认 "),e("code",[a._v("0.2")]),a._v("），则触发 Broadcast Join 降级，当前只支持 SMJ 降级。")]),a._v(" "),e("li",[a._v("只要有一个表非空 partition 比例小于 "),e("code",[a._v("spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin")]),a._v("（默认 "),e("code",[a._v("0.2")]),a._v("），则不做 Boradcast Join。")])]),a._v(" "),e("p",[a._v("由于 AQE 是在 Shuffle Map 阶段才能进行，AQE 开始优化之前，Shuffle 其实已经执行过半了（如落盘动作）。在 Shuffle Map 之前 AQE 并无法知道哪个是大表哪个是小表。表面上看 Join 策略调整毫无意义，但在 Shuffle 之后，"),e("strong",[a._v("OptimizeLocalShuffleReader")]),a._v(" 策略就开始生效了，它可以省去 Shuffle 常规步骤中的网络分发，Reduce Task 可以就近读取本地（Local）的中间文件，完成与小表的关联操作。")]),a._v(" "),e("p",[a._v("OptimizeLocalShuffleReader 是由 "),e("code",[a._v("spark.sql.adaptive.localShuffleReader.enabled")]),a._v(" 控制开关的，默认为 True；如果设置为 False，那么 Join 策略调整就失去了意义。")]),a._v(" "),e("p",[e("img",{attrs:{src:"https://static001.geekbang.org/resource/image/31/6a/31356505a2c36bac10de0e06d7e4526a.jpg",alt:""}})]),a._v(" "),e("p",[a._v("参考：")]),a._v(" "),e("ul",[e("li",[a._v("[What's new in Apache Spark 3.0 - demote broadcast hash join]("),e("a",{attrs:{href:"https://www.waitingforcode.com/apache-spark-sql/whats-new-apache-spark-3-demote-broadcast-hash-join/read",target:"_blank",rel:"noopener noreferrer"}},[a._v("What's new in Apache Spark 3.0 - demote broadcast hash join on waitingforcode.com - articles about Apache Spark SQL"),e("OutboundLink")],1),a._v(")")])])])}),[],!1,null,null,null);r.default=t.exports}}]);