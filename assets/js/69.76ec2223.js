(window.webpackJsonp=window.webpackJsonp||[]).push([[69],{791:function(t,a,s){"use strict";s.r(a);var n=s(70),r=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h2",{attrs:{id:"前言"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#前言"}},[t._v("#")]),t._v(" 前言")]),t._v(" "),s("p",[t._v("Spark 的资源调整，本质上就是平衡"),s("strong",[t._v("并行度、并发度、内存空间")]),t._v("三者之间的关系。")]),t._v(" "),s("h2",{attrs:{id:"_1-cpu"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-cpu"}},[t._v("#")]),t._v(" 1. CPU")]),t._v(" "),s("h3",{attrs:{id:"_1-1-spark-中的-cpu"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-spark-中的-cpu"}},[t._v("#")]),t._v(" 1.1 Spark 中的 CPU")]),t._v(" "),s("p",[t._v("在 RDD 存储到内存中进行展开之前，RDD 实际占用的还是 Execution Memory，因此 "),s("strong",[t._v("CPU 与内存的平衡，其实就是 CPU 与执行内存之间的协同与配比")]),t._v("。")]),t._v(" "),s("p",[t._v("协调 CPU 与 Execution Memory 的使用分为三类参数，分别控制着"),s("strong",[t._v("并行度")]),t._v("、"),s("strong",[t._v("执行内存大小")]),t._v("和"),s("strong",[t._v("集群的并行计算能力")]),t._v("。")]),t._v(" "),s("p",[t._v("并行度明确了数据的划分，并行度越高，数据的分片（partition）越多，可以通过两个参数来设置：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("spark.default.parallelism")]),t._v("：指定了 RDD 的默认并行度")]),t._v(" "),s("li",[s("strong",[t._v("spark.sql.shuffle.partitions")]),t._v("：指定了 Spark SQL 开发框架下，Reduce 阶段默认的并行度。")])]),t._v(" "),s("p",[t._v("并发度，指一个 Executor 中用于执行 task 的线程数，由 "),s("code",[t._v("spark.executor.cores")]),t._v(" 指定，"),s("code",[t._v("spark.task.cpus")]),t._v(" 则指定了每个 task 使用的线程数，通常不需要调整，默认为 1。所以并发度基本由 "),s("strong",[t._v("spark.executor.cores")]),t._v(" 进行控制。")]),t._v(" "),s("p",[t._v("对于一个 Executor 来说，一个线程同一时间只能计算一个 task。因此，在"),s("strong",[t._v("运行时")]),t._v("线程、task、partition 是一一对应的。")]),t._v(" "),s("p",[t._v("Executor 在接收到 Driver 端分发的任务后，会将其封装为 "),s("code",[t._v("TaskRunner")]),t._v("，交给线程池处理，处理的线程会像 Executor Memory 申请内存，然后开始执行任务。")]),t._v(" "),s("p",[t._v("假设有 N 个线程，每个线程可以申请到的内存空间，在 （Execution Memory / N） ~ （Execution Memory / 2N）之间。")]),t._v(" "),s("h3",{attrs:{id:"_1-2-cpu-低效的原因"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-cpu-低效的原因"}},[t._v("#")]),t._v(" 1.2 CPU 低效的原因")]),t._v(" "),s("p",[t._v("主要原因分为线程挂起与调度开销。")]),t._v(" "),s("h4",{attrs:{id:"_1-2-1-线程挂起"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-1-线程挂起"}},[t._v("#")]),t._v(" 1.2.1 线程挂起")]),t._v(" "),s("p",[t._v("Spark 使用 HashMap 记录每个线程消耗的内存大小，在给定执行内存总量 M 和线程总数 N 的情况下，，确保每个线程占用的执行内存不超过 M / N。")]),t._v(" "),s("p",[t._v("在一些极端情况下，有些线程连 M / 2N 甚至一点内存都申请不到导致挂起，主要原因有以下几个：")]),t._v(" "),s("ul",[s("li",[s("strong",[t._v("动态变化的执行内存总量 M")]),t._v("，Storage Memory 与 Execution Memory 是可以相互转化的。")]),t._v(" "),s("li",[s("strong",[t._v("动态变化的实际并发度 N~")]),t._v("，当 Executor 刚开始执行 task 时，并不是可以立刻到达最大并发度 N 的，影响的因素有很多，如 task 的先后到来，task 之间的依赖等。但随着任务执行以及调度的推进，N~ 会迅速的趋近于 N，CPU 线程挂起和内存分配也会得到改善。")]),t._v(" "),s("li",[t._v("分布式数据集的数据分布，每个数据分片数据量的大小决定了每个 task 需要申请多少内存。")])]),t._v(" "),s("h4",{attrs:{id:"_1-2-2-调度开销"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-2-调度开销"}},[t._v("#")]),t._v(" 1.2.2 调度开销")]),t._v(" "),s("p",[t._v("Spark 可以通过增加并行度，来减少每个数据分片的数据量大小，但也会带来一个副作用："),s("strong",[t._v("调度开销骤增")]),t._v("。")]),t._v(" "),s("p",[t._v("Driver 会将每个任务封装为 "),s("strong",[t._v("TaskDescription")]),t._v("，然后分发给各个 Executor。TaskDescription 包含着与任务运行有关的所有信息，如任务 ID、尝试 ID、要处理的数据分片 ID、开发者添加的本地文件和 Jar 包、任务属性、序列化的任务代码等等。Executor 接收到 TaskDescription 之后，首先需要对 TaskDescription 反序列化才能读取任务信息，然后将任务代码再反序列化得到可执行代码，最后再结合其他任务信息创建 TaskRunner。")]),t._v(" "),s("p",[t._v("当数据过于分散，分布式任务数量会大幅增加，但每个任务需要处理的数据量却少之又少，CPU 花在数据处理以及任务调度上的开销几乎相同。")]),t._v(" "),s("h4",{attrs:{id:"_1-2-3-优化-cpu-利用率"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-3-优化-cpu-利用率"}},[t._v("#")]),t._v(" 1.2.3 优化 CPU 利用率")]),t._v(" "),s("p",[t._v("提高 CPU 的利用率，需要设置好一个合适的并行度，需要综合多个方面的考量，并没有一个绝对的设置值。")]),t._v(" "),s("p",[t._v("一种常用的手段是先确定每个 Executor 的执行内存总量 M 与并发度 N，去计算一个能够让数据分片平均大小在 （M/2N, M/N）之间的并行度。或者反过来，在确定并行度的大小后调整 Executor 的内存大小。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://static001.geekbang.org/resource/image/4a/ce/4a5dc54813346924ec5611f6d1fa8fce.jpg",alt:""}})]),t._v(" "),s("h2",{attrs:{id:"_2-内存"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-内存"}},[t._v("#")]),t._v(" 2. 内存")]),t._v(" "),s("h3",{attrs:{id:"_2-1-内存区域的规划"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-内存区域的规划"}},[t._v("#")]),t._v(" 2.1 内存区域的规划")]),t._v(" "),s("p",[t._v("用户可以管理的内存有 User Memory、Storage Memory、Execution Memory 三块，需要衡量好三者的比例以充分利用内存。")]),t._v(" "),s("p",[t._v("User Memory 主要用于存放用户的自定义数据结构，如果经常分发一个不变的对象到各个 task， Spark 无法识别出该变量是否为同一个，会耗费 User Memory 去存储多个（task 数量）相同内容的变量。这种场景下，通常可以使用广播变量进行优化。")]),t._v(" "),s("p",[t._v("使用广播变量后，该变量会存储到 Storage Memory 中，并且一个 Executor 只存储一个，而不会受到 task 数量的影响。")]),t._v(" "),s("p",[t._v("为了充分使用 Spark 的内存，需要平衡好不同区域内的消耗占比，具体分为两步：")]),t._v(" "),s("ol",[s("li",[t._v("预估内存占用，假设共有 #N 个 Executor：\n"),s("ul",[s("li",[t._v("计算 User Memory 的内存消耗，汇总应用中包含的自定义数据结构，预估其大小并乘以 Executor 的并发度得到该区域的内存消耗 "),s("strong",[t._v("#User")])]),t._v(" "),s("li",[t._v("计算 Storage Memory 的内存消耗，每个 Executor 中 Storage Memory 区域的内存消耗为广播变量 + RDD 缓存，即 "),s("strong",[t._v("#Storage = #bc + #cache / #Executors")])]),t._v(" "),s("li",[t._v("计算 Execution Memory 的内存消耗，可以根据数据分片的大小得出，即 "),s("strong",[t._v("#Execution Memory = #threads * #dataset / #partitions")])])])]),t._v(" "),s("li",[t._v("调整内存配置，根据上述三者的估算大小，对 "),s("code",[t._v("spark.memory.fraction")]),t._v(" 和 "),s("code",[t._v("spark.memory.storageFraction")])])]),t._v(" "),s("p",[s("strong",[t._v("内存规划要达到的效果和目的，是确保不同内存区域的占比与不同类型的数据消耗保持一致，从而实现内存利用率的最大化")]),t._v("。")]),t._v(" "),s("h3",{attrs:{id:"_2-2-缓存"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-缓存"}},[t._v("#")]),t._v(" 2.2 缓存")]),t._v(" "),s("p",[t._v("RDD 的缓存需要关注三点：")]),t._v(" "),s("ul",[s("li",[s("p",[s("strong",[t._v("存的存储级别")]),t._v("：限定了数据缓存的存储介质（如内存、磁盘等）、存储形式（序列化、费序列化）以及副本数量（默认 1）。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://static001.geekbang.org/resource/image/4e/e2/4ecdfd4b62b1c6e151d029c38088yye2.jpeg",alt:""}})])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("缓存的计算过程")]),t._v("：将 RDD 以 Block Iterator 形式展开，暂存到 ValueHolder 中，然后转化为 MemoryEntry，接着存储到一个 key 为 BlockId 的 LinkedHashMap 中。")])]),t._v(" "),s("li",[s("p",[s("strong",[t._v("缓存的销毁过程")]),t._v("：缓存数据以主动或是被动的方式，被驱逐出内存或是磁盘的过程。")])])]),t._v(" "),s("p",[t._v("Spark 使用一个 LinkedHashMap 来记录 RDD 在缓存中的对应信息，在尝试缓存 RDD 却发现内存不足时，会根据 LRU 算法，从头开始扫描 Map 挑选可淘汰的 MemoryEntry，同属于当前 RDD 的 MemoryEntry 则会跳过。当可淘汰的 MemoryEntry 大小超过需要缓存的 RDD 大小，则会淘汰掉那部分 MemoryEntry。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://static001.geekbang.org/resource/image/b7/14/b73308328ef549579d02c72afb2ab114.jpg",alt:""}})]),t._v(" "),s("h3",{attrs:{id:"_2-3-缓存的时机"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-缓存的时机"}},[t._v("#")]),t._v(" 2.3 缓存的时机")]),t._v(" "),s("p",[t._v("对 RDD/DataFrame/Dataset 进行缓存，需要遵循以下 2 条基本原则：")]),t._v(" "),s("ul",[s("li",[t._v("如果 RDD/DataFrame/Dataset 在应用中的引用次数为 1，就坚决不使用 Cache")]),t._v(" "),s("li",[s("strong",[t._v("如果引用次数大于 1，且运行成本占比超过 30%，应当考虑启用 Cache")])])]),t._v(" "),s("p",[t._v("运行成本占比，"),s("strong",[t._v("指的是计算某个分布式数据集所消耗的总时间与作业执行时间的比值")]),t._v("。")]),t._v(" "),s("p",[t._v("例如，一个作业的执行时间规定为 1 小时，如果某个 DataFrame 需要用到多次，且生成该 DataFrame 需要耗时 12 分钟，那么该 DataFrame 的运行成本占比就是 "),s("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[s("svg",{staticStyle:{"vertical-align":"-0.566ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"16.717ex",height:"2.262ex",viewBox:"0 -750 7389 1000"}},[s("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[s("g",{attrs:{"data-mml-node":"math"}},[s("g",{attrs:{"data-mml-node":"mn"}},[s("path",{attrs:{"data-c":"31",d:"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"}}),s("path",{attrs:{"data-c":"32",d:"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z",transform:"translate(500, 0)"}})]),s("g",{attrs:{"data-mml-node":"mo",transform:"translate(1222.2, 0)"}},[s("path",{attrs:{"data-c":"D7",d:"M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z"}})]),s("g",{attrs:{"data-mml-node":"mn",transform:"translate(2222.4, 0)"}},[s("path",{attrs:{"data-c":"32",d:"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"}})]),s("g",{attrs:{"data-mml-node":"TeXAtom",transform:"translate(2722.4, 0)"}},[s("g",{attrs:{"data-mml-node":"mo"}},[s("path",{attrs:{"data-c":"2F",d:"M423 750Q432 750 438 744T444 730Q444 725 271 248T92 -240Q85 -250 75 -250Q68 -250 62 -245T56 -231Q56 -221 230 257T407 740Q411 750 423 750Z"}})])]),s("g",{attrs:{"data-mml-node":"mn",transform:"translate(3222.4, 0)"}},[s("path",{attrs:{"data-c":"36",d:"M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"}}),s("path",{attrs:{"data-c":"30",d:"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z",transform:"translate(500, 0)"}})]),s("g",{attrs:{"data-mml-node":"mo",transform:"translate(4500.2, 0)"}},[s("path",{attrs:{"data-c":"3D",d:"M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"}})]),s("g",{attrs:{"data-mml-node":"mn",transform:"translate(5556, 0)"}},[s("path",{attrs:{"data-c":"34",d:"M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z"}}),s("path",{attrs:{"data-c":"30",d:"M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z",transform:"translate(500, 0)"}})]),s("g",{attrs:{"data-mml-node":"mi",transform:"translate(6556, 0)"}},[s("path",{attrs:{"data-c":"25",d:"M465 605Q428 605 394 614T340 632T319 641Q332 608 332 548Q332 458 293 403T202 347Q145 347 101 402T56 548Q56 637 101 693T202 750Q241 750 272 719Q359 642 464 642Q580 642 650 732Q662 748 668 749Q670 750 673 750Q682 750 688 743T693 726Q178 -47 170 -52Q166 -56 160 -56Q147 -56 142 -45Q137 -36 142 -27Q143 -24 363 304Q469 462 525 546T581 630Q528 605 465 605ZM207 385Q235 385 263 427T292 548Q292 617 267 664T200 712Q193 712 186 709T167 698T147 668T134 615Q132 595 132 548V527Q132 436 165 403Q183 385 203 385H207ZM500 146Q500 234 544 290T647 347Q699 347 737 292T776 146T737 0T646 -56Q590 -56 545 0T500 146ZM651 -18Q679 -18 707 24T736 146Q736 215 711 262T644 309Q637 309 630 306T611 295T591 265T578 212Q577 200 577 146V124Q577 -18 647 -18H651Z"}})])])])])]),t._v("。")],1),t._v(" "),s("p",[t._v("在对业务、资源情况还不熟悉时，可以借助 Spark 3.0 的 noop 指令来查询某个 DataFrame 的耗时，该指令只触发计算而不会产生落盘动作：")]),t._v(" "),s("div",{staticClass:"language-scala line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-scala"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 利用 noop 精确计算DataFrame运行时间")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("write\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("format"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"noop"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("save"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br")])]),s("h3",{attrs:{id:"_2-4-缓存的注意事项"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-缓存的注意事项"}},[t._v("#")]),t._v(" 2.4 缓存的注意事项")]),t._v(" "),s("p",[t._v("缓存操作 .cache() 是一个惰性操作，需要注意触发计算的算子，也会影响实际缓存的数据，如 "),s("code",[t._v("count")]),t._v(" 算子会触发缓存的全量物化，而 "),s("code",[t._v("first")]),t._v("、"),s("code",[t._v("take")]),t._v(" 和 "),s("code",[t._v("show")]),t._v(" 这 3 个算子只会把涉及的数据物化。")]),t._v(" "),s("p",[t._v("Cache Manager 要求"),s("strong",[t._v("两个查询的 Analyzed Logical Plan 必须完全一致")]),t._v("，否则无法复用 cache：")]),t._v(" "),s("div",{staticClass:"language-scala line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-scala"}},[s("code",[t._v("df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" col2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("cache\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//数据分析")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" col2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\ndf"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("select"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" col2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("filter"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("col2 "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("100")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br")])]),s("p",[t._v("Analyzed Logical Plan 是比较初级的逻辑计划，主要负责 AST 查询语法树的语义检查，确保查询中引用的表、列等元信息的有效性。像谓词下推、列剪枝这些比较智能的推理，要等到制定 Optimized Logical Plan 才会生效。")]),t._v(" "),s("p",[t._v("因此，即使是同一个查询语句，仅仅是调换了select和filter的顺序，在 Analyzed Logical Plan 阶段也会被判定为不同的逻辑计划。")]),t._v(" "),s("p",[s("strong",[t._v("为什么 Cache Manager 不在 Optimized Logical Plan 执行 cache 复用的管理？")])]),t._v(" "),s("blockquote",[s("p",[t._v("Spark 使用 "),s("code",[t._v("（LogicalPlan，InMemoryRelation）")]),t._v(" 来表示 Logical Plan 及其对应的 cache 数据信息，两个执行结果相同的查询能否复用 cache，就转化为了两个查询的 Logical Plan 是否能够一致。")]),t._v(" "),s("p",[t._v("Spark 会对不同查询的 AST 进行归一化，尽可能消除一些无关痛痒的小细节。但对于执行结果相同的不同查询，Spark 无法保证归一化后两者完全相同，并且 Spark 需要保证执行结果不同的两个查询的 Logical Plan 必定是不同的，因此只能一并包含进去，统一不做 cache 复用。")]),t._v(" "),s("p",[t._v("cache 是否复用的判断越早做越省事。如果使用 Optimized Plan 作为 key，那么到了后面要么很难命中，要么即使命中了发现优化白做了。")]),t._v(" "),s("p",[t._v("一个优化思考，社区暂且没有：对 SQL 按照传统 DBMS 进行 SQL Re-write，这样可以很大程度的让执行结果相同的不同查询生成的 Logical Plan 相同。")])]),t._v(" "),s("h2",{attrs:{id:"_3-磁盘"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-磁盘"}},[t._v("#")]),t._v(" 3. 磁盘")]),t._v(" "),s("p",[t._v("磁盘在 Spark 中通常是起到两个作用：")]),t._v(" "),s("ol",[s("li",[t._v("溢出临时文件")]),t._v(" "),s("li",[t._v("存储 Shuffle 中间文件")])]),t._v(" "),s("p",[t._v("除了 cache 之外，Shuffle 落盘也能作为计算失败时的一个回溯点，而不必从头计算，减少失败重算的代价。")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://static001.geekbang.org/resource/image/35/86/35c13d9f2eba5d23dabe05249ccb9486.jpg",alt:""}})]),t._v(" "),s("p",[s("strong",[t._v("ReuseExchange")]),t._v(" 是 Spark SQL 众多优化策略中的一种，"),s("strong",[t._v("相同或是相似的物理计划可以共享 Shuffle 计算的中间结果")]),t._v("。")]),t._v(" "),s("p",[t._v("使用 ReuseExchange 有两个条件：")]),t._v(" "),s("ol",[s("li",[t._v("多个查询所依赖的分区规则要与 Shuffle 中间数据的分区规则保持一致")]),t._v(" "),s("li",[t._v("多个查询所涉及的字段（Attributes）要保持一致")])]),t._v(" "),s("p",[t._v("例如以下代码：")]),t._v(" "),s("div",{staticClass:"language-scala line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-scala"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//版本1：分别计算PV、UV，然后合并")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Data schema (userId: String, accessTime: Timestamp, page: String)")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" filePath"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parquet"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filePath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" dfPV"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupBy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userId"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agg"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"page"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("withColumn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"metrics"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PV"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" dfUV"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupBy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userId"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agg"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("countDistinct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"page"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("withColumn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"metrics "')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"UV"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" resultDF"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dfPV"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Union"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dfUV"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Result样例")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" userId "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" metrics "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" value "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" user0  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" PV      "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" user0  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" UV      "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br")])]),s("p",[t._v("dfPV 与 dfUV 都会有各自的执行路径，两者仅是数据来源一致，可能不同 executor 读取的文件并不一致，Shuffle 的中间数据不一致，无法利用到 ReuseExchange 机制：")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://static001.geekbang.org/resource/image/dd/28/dd150e0863812522a6f2ee9102678928.jpg",alt:""}})]),t._v(" "),s("p",[t._v("修改后如下：")]),t._v(" "),s("div",{staticClass:"language-scala line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-scala"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//版本2：分别计算PV、UV，然后合并")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Data schema (userId: String, accessTime: Timestamp, page: String)")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" filePath"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" _\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" spark"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("read"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parquet"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("filePath"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("repartition"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("$"),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userId"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" dfPV"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupBy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userId"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agg"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("count"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"page"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("withColumn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"metrics"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"PV"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" dfUV"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" df"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("groupBy"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"userId"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("agg"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("countDistinct"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"page"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("alias"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("withColumn"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"metrics "')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"UV"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" resultDF"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" DataFrame "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" dfPV"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Union"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("dfUV"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n \n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Result样例")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" userId "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" metrics "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" value "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" user0  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" PV      "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("25")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" user0  "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" UV      "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("12")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("|")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br"),s("span",{staticClass:"line-number"},[t._v("7")]),s("br"),s("span",{staticClass:"line-number"},[t._v("8")]),s("br"),s("span",{staticClass:"line-number"},[t._v("9")]),s("br"),s("span",{staticClass:"line-number"},[t._v("10")]),s("br"),s("span",{staticClass:"line-number"},[t._v("11")]),s("br"),s("span",{staticClass:"line-number"},[t._v("12")]),s("br"),s("span",{staticClass:"line-number"},[t._v("13")]),s("br"),s("span",{staticClass:"line-number"},[t._v("14")]),s("br"),s("span",{staticClass:"line-number"},[t._v("15")]),s("br")])]),s("p",[t._v("修改后的执行路径如下：")]),t._v(" "),s("p",[s("img",{attrs:{src:"https://static001.geekbang.org/resource/image/00/b2/008e691de73eefc6daa4886017fa33b2.jpg",alt:""}})]),t._v(" "),s("p",[t._v("这样一来就能充分利用 ReuseExchange 机制，从数据角度来看，ReuseExchange 其实起到了跟持久化到磁盘一致的功能。")]),t._v(" "),s("p",[t._v("需要注意的是，如果将 agg 操作中涉及的字段修改为其它字段，由于涉及的字段不一致，也无法使用 ReuseExchange 机制。")]),t._v(" "),s("h2",{attrs:{id:"_4-总结"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-总结"}},[t._v("#")]),t._v(" 4. 总结")]),t._v(" "),s("p",[t._v("CPU 的充分利用，主要在于调节好"),s("strong",[t._v("并发度")]),t._v("、"),s("strong",[t._v("并行度")]),t._v("以及"),s("strong",[t._v("每个 Executor 的内存大小")]),t._v("。")]),t._v(" "),s("p",[t._v("内存方面，需要考虑较多：")]),t._v(" "),s("ul",[s("li",[t._v("不同内存区域的划分，其中 Execution Memory 需要根据实际并发度考虑，并且通常 OOM 的原因不是内存不够，而是某个数据分片能超过了线程可用内存")]),t._v(" "),s("li",[t._v("缓存需要考虑好时机，某个反复用到的数据分片计算成本大于 30% 即可考虑进行缓存")])]),t._v(" "),s("p",[t._v("磁盘方面，可以考虑增大一些 buffer 减少落盘次数，同时磁盘的 shuffle 也起到了类似于存档点的作用（如 ReuseExchange 机制）。")])])}),[],!1,null,null,null);a.default=r.exports}}]);