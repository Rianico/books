(window.webpackJsonp=window.webpackJsonp||[]).push([[179],{902:function(t,s,a){"use strict";a.r(s);var n=a(70),e=Object(n.a)({},(function(){var t=this,s=t.$createElement,a=t._self._c||s;return a("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[a("h2",{attrs:{id:"_1-前置知识"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_1-前置知识"}},[t._v("#")]),t._v(" 1. 前置知识")]),t._v(" "),a("p",[t._v("要先了解 SparkStreaming 提供的语义，需要先了解 Spark RDD 的容错语义：")]),t._v(" "),a("ol",[a("li",[t._v("一个 RDD 是一个不可变，可溯源重算的分布式数据集。")]),t._v(" "),a("li",[t._v("如果 RDD 的某个分区数据由于 executor 失败丢失了，那么可以通过"),a("strong",[t._v("原始的容错数据集")]),t._v("进行重算。")]),t._v(" "),a("li",[t._v("假设 RDD 的全部转化操作都是确定的，那么无论发生什么故障，最终转化出来的 RDD 总是相同的。")])]),t._v(" "),a("p",[t._v("从上面的语义来看，如果 Spark 的数据来自于一个可靠的文件系统（如 HDFS、S3），那么 Spark 的所有 RDD 也是可靠的。")]),t._v(" "),a("p",[t._v("但 SparkStreaming 的绝大部分使用场景都跟上面有所区别，SparkStreaming 是基于网络来接收数据的，数据源并非一定可靠。")]),t._v(" "),a("p",[t._v("SparkStreaming 要给所有生成的 RDD 实现同样的容错机制，通常有以下两种方法：")]),t._v(" "),a("ol",[a("li",[t._v("接收数据，并在多个节点上做副本")]),t._v(" "),a("li",[t._v("接收数据并进行缓存，但故障发生时只能重新从 Source 获取数据。")])]),t._v(" "),a("p",[t._v("此外，还有两种类型的失败需要我们关注：")]),t._v(" "),a("ol",[a("li",[t._v("Worker 节点故障，此时 Worker 节点上所有的 executor 都会失败，并且存在内存中的数据都会丢失。")]),t._v(" "),a("li",[t._v("Driver 端故障，SparkContext 会丢失，此时所有 executor 都会失败且丢失数据。")])]),t._v(" "),a("p",[t._v("对于任何流式处理系统，都可以简单抽象为三步：")]),t._v(" "),a("ol",[a("li",[a("strong",[t._v("接收数据（Source）")]),t._v("，使用 Receivers 或者其他方式从数据源接收数据")]),t._v(" "),a("li",[a("strong",[t._v("数据处理")]),t._v("，接收的数据会进行转化处理")]),t._v(" "),a("li",[a("strong",[t._v("输出数据（Sink）")]),t._v("，最终处理好的数据会被推到外部存储系统，如文件系统、数据库、图表等。")])]),t._v(" "),a("p",[t._v("如果一个流式应用必须实现"),a("strong",[t._v("端到端")]),t._v("的 exactly-once 的保证，那么每个步骤都需要提供一个 exactly-once 的保证：")]),t._v(" "),a("ol",[a("li",[t._v("接收数据，不同的数据源有不同的保证方式。")]),t._v(" "),a("li",[t._v("数据处理，借助 RDD 提供的容错机制，只要数据源是可访问的，最后转化出来的 RDD 总会是一样的。")]),t._v(" "),a("li",[t._v("输出数据，输出操作默认是 at-least-once 语义，这取决于输出操作（"),a("strong",[t._v("是否幂等")]),t._v("）以及外部系统提供的语义（"),a("strong",[t._v("是否支持事务")]),t._v("）。单用户也可以自行实现一套事务机制来实现 exactly-once 语义。")])]),t._v(" "),a("blockquote",[a("p",[t._v("NOTE：通常并不需要做到端到端的 exactly-once 语义，最后输出时能够做到 exactly-once 即可满足绝大部分场景。")])]),t._v(" "),a("h2",{attrs:{id:"_2-source-端-流式程序"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-source-端-流式程序"}},[t._v("#")]),t._v(" 2. Source 端 + 流式程序")]),t._v(" "),a("h3",{attrs:{id:"_2-1-文件系统数据源"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-文件系统数据源"}},[t._v("#")]),t._v(" 2.1 文件系统数据源")]),t._v(" "),a("p",[t._v("如果所有文件都已经存放于一个可靠、容错的文件系统（如 HDFS），那么 SparkStreaming 总是能够从故障中恢复并处理所有的数据，实现 at-least-once 语义。")]),t._v(" "),a("h3",{attrs:{id:"_2-2-基于-receiver-的数据源"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-基于-receiver-的数据源"}},[t._v("#")]),t._v(" 2.2 基于 Receiver 的数据源")]),t._v(" "),a("p",[t._v("对基于 Receiver 的输入源，容错的语义取决于故障场景以及 Receiver 的类型：")]),t._v(" "),a("ol",[a("li",[t._v("可靠的 Receiver，这类 Receiver 会在确保数据做了多副本备份后再做确认。如果这类 Receiver 失败了，则数据源将不会接收到相关的 ack。因此，如果 Receiver 失败后重启，数据源会重新发送数据，此时不会有消息丢失。")]),t._v(" "),a("li",[t._v("不可靠的 Receiver，这类 Receiver 不会发送 ack ，因此一旦出现故障，数据将会丢失。")])]),t._v(" "),a("p",[t._v("为了避免数据丢失，Spark 在 1.2 退推出了 wal（"),a("em",[t._v("write ahead logs")]),t._v("）机制，可以将接收到的数据保存在一个可容错的存储中，可以通过 "),a("code",[t._v("spark.streaming.receiver.writeAheadLog.enable")]),t._v(" 开启，Spark 会定期存储接收到的数据。这种方式下吞吐量会有影响。")]),t._v(" "),a("p",[t._v("SparkStreaming 消费 Kafka 的其中一种方式就是使用 Receiver 方式。")]),t._v(" "),a("p",[t._v("executor 拉取数据后会组装为 block，并将 block metadata 报告给 Driver。如果 executor 来不及将 block metadata 汇报给 Driver 就挂了，则会通过 zk 获取 offset 然后重新读取数据。")]),t._v(" "),a("p",[t._v("Driver 会根据这些信息进行管理，executor 端也会利用 wal 机制将数据存储到可靠的外部存储系统，实现 at-least 语义，因此需要输出操作实现幂等性")]),t._v(" "),a("h3",{attrs:{id:"_2-3-kafka-direct-api"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-kafka-direct-api"}},[t._v("#")]),t._v(" 2.3 Kafka Direct API")]),t._v(" "),a("p",[t._v("从 Spark 1.3 开始推出了 Kafka 低阶 API，只需要实现 exactly-once 的输出操作，即可实现端到端 exactly-once 的保障。")]),t._v(" "),a("p",[t._v("以下面的 SparkStreaming + Kafka 为例：")]),t._v(" "),a("div",{staticClass:"language-scala line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("kafka"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("clients"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("consumer")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ConsumerRecord\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("kafka"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("common"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("serialization")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("StringDeserializer\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("streaming"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("kafka010")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("_\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("streaming"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("kafka010")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("LocationStrategies"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("PreferConsistent\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("import")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token namespace"}},[t._v("org"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apache"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("spark"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("streaming"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("kafka010")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("ConsumerStrategies"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Subscribe\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" kafkaParams "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" Object"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"bootstrap.servers"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"localhost:9092,anotherhost:9092"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"key.deserializer"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" classOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("StringDeserializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"value.deserializer"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" classOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("StringDeserializer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"group.id"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"use_a_separate_group_id_for_each_stream"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"auto.offset.reset"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"latest"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"enable.auto.commit"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("false")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" java"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("lang"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("Boolean")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" topics "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Array"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"topicA"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"topicB"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" stream "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" KafkaUtils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDirectStream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  streamingContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  PreferConsistent"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  Subscribe"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("topics"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kafkaParams"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nstream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("record "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("record"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("key"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" record"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("value"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br")])]),a("p",[t._v("LocationStrategies 的几种策略：")]),t._v(" "),a("ul",[a("li",[a("strong",[t._v("PreferConsistent")]),t._v("，绝大部分场景下都是使用该方式。这种方式会将 partition 均匀的分发到可用的 executor 上。")]),t._v(" "),a("li",[a("strong",[t._v("PreferBrokers")]),t._v("，如果允许在 Broker 上起 executor，则优先将 partition 分配到其 leader 节点上的 executor 上。")]),t._v(" "),a("li",[a("strong",[t._v("PreferFixed")]),t._v("，允许用户显式指定 host 与 partition 的映射关系，如果未指定则默认使用 PreferConsistent。")])]),t._v(" "),a("p",[t._v("ConsumerStrategies 的几种策略：")]),t._v(" "),a("ul",[a("li",[t._v("Subscribe，允许订阅一组固定的 topic")]),t._v(" "),a("li",[t._v("SubscribePattern ，允许通过正则订阅一组固定的 topic")]),t._v(" "),a("li",[t._v("Assign，允许指定固定的 partition。")])]),t._v(" "),a("p",[t._v("SparkStreaming 可以通过以下方式获取当前消费的 offset：")]),t._v(" "),a("div",{staticClass:"language-scala line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[t._v("stream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreachRDD "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" rdd "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" offsetRanges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("asInstanceOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("HasOffsetRanges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("offsetRanges\n  rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreachPartition "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" iter "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" o"),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),t._v(" OffsetRange "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" offsetRanges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("TaskContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("get"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("partitionId"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    println"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("s"),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br")])]),a("blockquote",[a("p",[t._v("NOTE： HasOffsetRanges 只能在作为 "),a("code",[t._v("createDirectStream")]),t._v(" 生成结果调用的第一个方法才能成功转化。")])]),t._v(" "),a("p",[t._v("Kafka 在故障场景下实现的语义"),a("strong",[t._v("取决于 offset 是如何以及何时存储")]),t._v("。Spark 的输出操作是 at-least-once，因此如果想要实现 exactly-once，"),a("strong",[t._v("要么在进行一个幂等性操作后存储 offset，要么将 offset 与输出以事务的方式进行存储")]),t._v("。")]),t._v(" "),a("p",[t._v("这里讨论几种 offset 存储的方式。")]),t._v(" "),a("p",[a("strong",[t._v("Checkpoint")]),t._v(" 方式，Spark 会将 offset 存储在 checkpoint 中，但存在以下几个缺点：")]),t._v(" "),a("ul",[a("li",[t._v("要求"),a("strong",[t._v("输出操作必须是幂等的")]),t._v("，checkpoint 保证的时 at-least-once 语义，因此会存在重复的输出。")]),t._v(" "),a("li",[t._v("代码升级时会导致 checkpoint 失效，此时需要其他方式来定位 offset。")])]),t._v(" "),a("p",[a("strong",[t._v("Kafka 方式")]),t._v("，其自身也提供一个提交 offset 的 API，并会将 offset 存储到一个特定的 topic 里。默认情况下，新版本的 consumer 会定期自动提交 offset。通常用户并不希望会自动提交，而是手动控制。")]),t._v(" "),a("p",[t._v("由于 Kafka 提交 offset 与数据输出操作并不在一个事务中，只能保证消费是 at-least-once，因此需要"),a("strong",[t._v("接收方能支持幂等性")]),t._v("来实现 exactly-once。")]),t._v(" "),a("div",{staticClass:"language-scala line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[t._v("stream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreachRDD "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" rdd "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" offsetRanges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("asInstanceOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("HasOffsetRanges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("offsetRanges\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// your computing ...")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// some time later, after outputs have completed")]),t._v("\n  stream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("asInstanceOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("CanCommitOffsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("commitAsync"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("offsetRanges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br")])]),a("p",[a("strong",[t._v("自定义的数据存储")]),t._v("方式，这种方式最为万能，但也最为复杂，通过加数据输出与 offset 存储放到同个事务中，一旦故障发生，则会数据与 offset 会一同回滚，从而实现 exactly-once 语义。"),a("strong",[t._v("这种方式最为万能，甚至可以针对存在 Shuffle、Aggregation 这类通常难以实现输出幂等性的场景")]),t._v("。")]),t._v(" "),a("div",{staticClass:"language-scala line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" fromOffsets "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" selectOffsetsFromYourDatabase"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("map "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" resultSet "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("new")]),t._v(" TopicPartition"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("resultSet"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("string"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"topic"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" resultSet"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("int"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"partition"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("->")]),t._v(" resultSet"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("long"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"offset"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("toMap\n\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" stream "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" KafkaUtils"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("createDirectStream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  streamingContext"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  PreferConsistent"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  Assign"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("String")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("fromOffsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("keys"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("toList"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" kafkaParams"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" fromOffsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\nstream"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreachRDD "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" rdd "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" offsetRanges "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("asInstanceOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("HasOffsetRanges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("offsetRanges\n\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("val")]),t._v(" results "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" yourCalculation"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("rdd"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// begin your transaction")]),t._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// update results")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// update offsets where the end of existing offsets matches the beginning of this batch of offsets")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// assert that offsets were updated correctly")]),t._v("\n\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// end your transaction")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br"),a("span",{staticClass:"line-number"},[t._v("14")]),a("br"),a("span",{staticClass:"line-number"},[t._v("15")]),a("br"),a("span",{staticClass:"line-number"},[t._v("16")]),a("br"),a("span",{staticClass:"line-number"},[t._v("17")]),a("br"),a("span",{staticClass:"line-number"},[t._v("18")]),a("br"),a("span",{staticClass:"line-number"},[t._v("19")]),a("br"),a("span",{staticClass:"line-number"},[t._v("20")]),a("br"),a("span",{staticClass:"line-number"},[t._v("21")]),a("br"),a("span",{staticClass:"line-number"},[t._v("22")]),a("br"),a("span",{staticClass:"line-number"},[t._v("23")]),a("br")])]),a("blockquote",[a("p",[t._v("NOTE：幂等性解决方案通常只能应对 Map-Only 的场景，存在 Shuffle、Aggregation 等操作（即一个 executor 对应的数据会有变化）的场景，通常得使用事务来实现。")])]),t._v(" "),a("h2",{attrs:{id:"_3-流式程序-sink-端"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-流式程序-sink-端"}},[t._v("#")]),t._v(" 3. 流式程序 + Sink 端")]),t._v(" "),a("p",[t._v("前面提到，Source + SparkStreaming 能够实现 at-least-once 的输出操作，也就是说可能存在重复多次的输出。若要实现端到端的 exactly-once，则最后需要 Sink 端提供幂等性操作或者实现事务功能。")]),t._v(" "),a("p",[t._v("幂等性存在一种局限，就是有些操作会打乱分区对应关系，导致故障恢复后重新执行计算，分区的数据不一定与上次对应。因此当遇到并非 Map-Only 的输出时，需要注意是否会由于该问题导致 exactly-once 失效。")]),t._v(" "),a("p",[t._v("事务功能则可以应对一切场景，当数据成功输出时，SparkStreaming 数据处理成功；当数据输出出现问题时，SparkStreaming 与输出数据一起回滚，重新计算，流程如下：")]),t._v(" "),a("ul",[a("li",[t._v("使用 batch time 以及 RDD 的分区编号创建一个唯一标识，用于识别流式应用中的数据。")]),t._v(" "),a("li",[t._v("以事务方式原子性地提交这个唯一标识与数据；如果发现标识已被提交，则跳过更新。")])]),t._v(" "),a("p",[t._v("下面讨论几种常见的场景，均以 SparkStreaming + Kafka 为前提。")]),t._v(" "),a("h3",{attrs:{id:"_3-1-文件系统实现幂等性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-文件系统实现幂等性"}},[t._v("#")]),t._v(" 3.1 文件系统实现幂等性")]),t._v(" "),a("p",[t._v("如果是输出到外部文件系统，那么只需要简单的执行 "),a("code",[t._v("saveAs***Files")]),t._v(" 操作，通过 overwriten 覆盖，那么无论输出多少次都会是同样的结果，overwriten 是天生幂等性的，从而实现 exactly-once 语义。")]),t._v(" "),a("h3",{attrs:{id:"_3-2-mysql-实现幂等性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-mysql-实现幂等性"}},[t._v("#")]),t._v(" 3.2 MySql 实现幂等性")]),t._v(" "),a("p",[t._v("传统的 OLTP 数据库基本都提供了该功能，只要是通过主键 ID 或者指定某些 Key 进行去重。")]),t._v(" "),a("p",[t._v("首先通过 sbt 引入相关依赖包：")]),t._v(" "),a("div",{staticClass:"language-scala line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[t._v("scalaVersion "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2.11.11"')]),t._v("\n\nlibraryDependencies "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("++")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" Seq"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.apache.spark"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark-streaming"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2.2.0"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"provided"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.apache.spark"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"spark-streaming-kafka-0-10"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"2.2.0"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"org.scalikejdbc"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"scalikejdbc"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"3.0.1"')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mysql"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"mysql-connector-java"')]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token string"}},[t._v('"5.1.43"')]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br")])]),a("p",[t._v("建表语句如下：")]),t._v(" "),a("div",{staticClass:"language-sql line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-sql"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("create")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("table")]),t._v(" error_log "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("\n  log_time "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("datetime")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("primary")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("key")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v("\n  log_count "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("int")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[t._v("not")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("null")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("default")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v("\n"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br")])]),a("p",[t._v("借助主键实现幂等性输出：")]),t._v(" "),a("div",{staticClass:"language-scala line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[t._v("  DB"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("autoCommit "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("implicit")]),t._v(" session "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n    result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreach "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n      sql"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n      insert into error_log (log_time, log_count)\n      value (${time}, ${count})\n      on duplicate key update log_count = log_count + values(log_count)\n      """')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apply"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 提交 offset")]),t._v("\n  messages"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("asInstanceOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("CanCommitOffsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("commitAsync"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("offsetRanges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br")])]),a("h3",{attrs:{id:"_3-3-mysql-实现事务"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-mysql-实现事务"}},[t._v("#")]),t._v(" 3.3 Mysql 实现事务")]),t._v(" "),a("p",[t._v("Mysql 在存储数据时查询一下 SparkStreamig 对数据的唯一标识是否已经提交，如果是跳过更新。")]),t._v(" "),a("div",{staticClass:"language-scala line-numbers-mode"},[a("pre",{pre:!0,attrs:{class:"language-scala"}},[a("code",[t._v("  DB"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("autoCommit "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("implicit")]),t._v(" session "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n    result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("foreach "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("case")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("time"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" count"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("=>")]),t._v("\n      sql"),a("span",{pre:!0,attrs:{class:"token triple-quoted-string string"}},[t._v('"""\n      insert into error_log (log_time, log_count)\n      value (${time}, ${count})\n      where not exists (\n      select batch_time, partitionId from topic_offset\n      )\n      """')]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("update"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("apply"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n  "),a("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 提交 offset")]),t._v("\n  messages"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("asInstanceOf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("CanCommitOffsets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("commitAsync"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("offsetRanges"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),a("div",{staticClass:"line-numbers-wrapper"},[a("span",{staticClass:"line-number"},[t._v("1")]),a("br"),a("span",{staticClass:"line-number"},[t._v("2")]),a("br"),a("span",{staticClass:"line-number"},[t._v("3")]),a("br"),a("span",{staticClass:"line-number"},[t._v("4")]),a("br"),a("span",{staticClass:"line-number"},[t._v("5")]),a("br"),a("span",{staticClass:"line-number"},[t._v("6")]),a("br"),a("span",{staticClass:"line-number"},[t._v("7")]),a("br"),a("span",{staticClass:"line-number"},[t._v("8")]),a("br"),a("span",{staticClass:"line-number"},[t._v("9")]),a("br"),a("span",{staticClass:"line-number"},[t._v("10")]),a("br"),a("span",{staticClass:"line-number"},[t._v("11")]),a("br"),a("span",{staticClass:"line-number"},[t._v("12")]),a("br"),a("span",{staticClass:"line-number"},[t._v("13")]),a("br")])]),a("h3",{attrs:{id:"_3-4-kafka-实现幂等性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-kafka-实现幂等性"}},[t._v("#")]),t._v(" 3.4 Kafka 实现幂等性")]),t._v(" "),a("p",[t._v("http://matt33.com/2018/10/24/kafka-idempotent/")]),t._v(" "),a("h3",{attrs:{id:"_3-5-kafka-实现事务性"}},[a("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-kafka-实现事务性"}},[t._v("#")]),t._v(" 3.5 Kafka 实现事务性")]),t._v(" "),a("p",[t._v("http://matt33.com/2018/11/04/kafka-transaction/")]),t._v(" "),a("h2",{attrs:{id:"参考："}},[a("a",{staticClass:"header-anchor",attrs:{href:"#参考："}},[t._v("#")]),t._v(" 参考：")]),t._v(" "),a("ul",[a("li",[a("a",{attrs:{href:"https://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#fault-tolerance-semantics",target:"_blank",rel:"noopener noreferrer"}},[t._v("Fault-tolerance Semantics"),a("OutboundLink")],1)]),t._v(" "),a("li",[a("a",{attrs:{href:"http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Spark Streaming 中如何实现 Exactly-Once 语义"),a("OutboundLink")],1)])])])}),[],!1,null,null,null);s.default=e.exports}}]);