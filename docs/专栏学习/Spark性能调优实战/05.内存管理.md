---
title: 05.内存管理
date: 2021-04-10
---

## 内存管理

## 1. Spark 内存管理模式

在管理方式上，Spark 会区分**堆内内存（On-heap Memory）**和**堆外内存（Off-heap Memory）**，其中堆内内存由 JVM 统一管理，堆外内存则由 Spark 自行管理（调用 Java 的 Unsafe）。

![Executor Container](https://iamluminousmen-media.s3.amazonaws.com/media/dive-into-spark-memory/dive-into-spark-memory-10.jpg)

在堆内存的管理上，Spark 创建、删除对象都是交由 JVM 管理，这种方式下，Spark 对内存的估计是存在一个误差的，容易导致 OOM。

Overhead 属于 Container 的一部分，以堆外的形式存在，主要承担了一些额外的开销，如虚拟机的开销，内部字符串或者一些本地开销。3.0 之前，该部分内存包含了 off-heap，3.0 开始则将两者分离开来。

## 2. 内存区域的划分

### 2.1 堆内内存

Spark Executor 内存划分如下：

![img](https://www.tutorialdocs.com/upload/2018/08/spark-memory-01.svg)

- **Storage Memory**：用于缓存 RDD、广播变量等数据
- **Execution Memory**：用于执行分布式任务，如 Shuffle、Sort 和 Aggregate 等操作：
  - Shuffle Map 阶段的数据转换、映射、排序、聚合、归并等操作
  - Shuffle Reduce 阶段的数据排序和聚合操作
- **User Memory**：存储用户自定义数据结构
- **Reserved Memory**：用于存储 Spark 自身的一些对象，如 BlockManager 等

从 Spark 1.6 开始，Storage Memory 和 Execution Memory 之间可以相互转化，并且 Execution Memory 拥有更高的优先级，这是由于如果 Execution Memory 在执行完之前，如果将内存归还给 Storage Memory，很可能出现 OOM 的问题。

Execution Memory 和 Storage Memory 之间的转化规则如下：

- 如果双方的内存都有空闲，则双方都可以抢占
- 如果 Storage Memory 占用了部分 Execution Memory，当计算任务有需要时，需要立刻归还占用的 Execution Memory
- 如果 Execution Memory 占用了部分 Storage Memory，即使需要缓存数据，也需要等待计算任务完成再释放 Execution Memory。

如下代码：

```scala
val dict: List[String] = List(“spark”, “scala”)
val words: RDD[String] = sparkContext.textFile(“~/words.csv”)
val keywords: RDD[String] = words.filter(word => dict.contains(word))
keywords.cache
keywords.count
keywords.map((_, 1)).reduceByKey(_ + _).collect
```

第 1 行代码的 `dict` 对象使用了 User Memory，而由于第四行的 `cache` 方法，因此第 4、5 行都使用了 Storage Memory，第 6 行涉及到 shuffle，因此会消耗 Execution Memory。

### 2.2 非堆内存

非堆内存的划分则较为简单，直接划分为了 Storage Memory 以及 Execution Memory，两者之间可以相互转化：

![img](https://www.tutorialdocs.com/upload/2018/08/spark-memory-02.svg)

非堆内存使得 Spark 可以直接操作 JVM 之外的内存，减少不必要的内存消耗，GC 负载等，从而提升性能。

尽管非堆内存可以提供性能上的提升，但使用起来需要非常小心。在某些场景下，也会带来堆内——堆外对象拷贝的开销，同时也带来了 GC 负载。

> NOTE：堆内堆外的内存空间不共享，task 往往会优先使用堆外，再使用堆内，但如果一个 task 堆外空间不够用，则会导致 OOM。



参考：

- [**Dive into Spark memory**](https://luminousmen.com/post/dive-into-spark-memory)