---
title: 05.内存管理
date: 2021-04-10
---

## 1. Spark 内存管理模式

在管理方式上，Spark 会区分**堆内内存（On-heap Memory）**和**堆外内存（Off-heap Memory）**，其中堆内内存由 JVM 统一管理，堆外内存则由 Spark 自行管理（调用 Java 的 Unsafe）。

![Executor Container](https://iamluminousmen-media.s3.amazonaws.com/media/dive-into-spark-memory/dive-into-spark-memory-10.jpg)

在堆内存的管理上，Spark 创建、删除对象都是交由 JVM 管理，这种方式下，Spark 对内存的估计是存在一个误差的，容易导致 OOM。

Overhead 属于 Container 的一部分，以堆外的形式存在，主要承担了一些额外的开销，如虚拟机的开销，内部字符串或者一些本地开销。3.0 之前，该部分内存包含了 off-heap，3.0 开始则将两者分离开来。

## 2. 内存区域的划分

### 2.1 堆内内存

Spark Executor 内存划分如下：

![img](https://www.tutorialdocs.com/upload/2018/08/spark-memory-01.svg)

- **Reserved Memory**：用于存储 Spark 自身的一些对象，如 BlockManager 等
- **User Memory**：存储用户自定义数据结构
- **Storage Memory**：用于缓存 RDD、广播变量等数据
- **Execution Memory**：用于执行分布式任务，如 Shuffle、Sort 和 Aggregate 等操作，并存储一些运行时需要的数据：
  - Shuffle Map 阶段的数据转换、映射、排序、聚合、归并等操作
  - Shuffle Reduce 阶段的数据排序和聚合操作

Execution Memory 和 Storage Memory 之间的转化规则如下：

- 双方都只能抢占彼此空闲的内存
- 如果 Storage Memory 占用了部分 Execution Memory，当计算任务有需要时，需要立刻归还占用的 Execution Memory
- 如果 Execution Memory 占用了部分 Storage Memory，即使需要缓存数据，也需要等待计算任务完成再释放 Execution Memory。

Execution Memory 存储了中间计算结果，由于需要参与后续计算，因此不能像 Storage Memory 一样简单的将其淘汰出内存。

```scala
val dict: List[String] = List(“spark”, “scala”)
val words: RDD[String] = sparkContext.textFile(“~/words.csv”)
val keywords: RDD[String] = words.filter(word => dict.contains(word))
keywords.cache
keywords.count
keywords.map((_, 1)).reduceByKey(_ + _).collect
```

第 1 行代码的 `dict` 对象使用了 User Memory，而由于第四行的 `cache` 方法，因此第 4、5 行都使用了 Storage Memory，第 6 行涉及到 shuffle，因此会消耗 Execution Memory。

### 2.2 非堆内存

非堆内存的划分则较为简单，直接划分为了 Storage Memory 以及 Execution Memory，两者之间可以相互转化：

![img](https://www.tutorialdocs.com/upload/2018/08/spark-memory-02.svg)

非堆内存使得 Spark 可以直接操作 JVM 之外的内存，减少不必要的内存消耗，GC 负载等，从而提升性能。

尽管非堆内存可以提供性能上的提升，但使用起来需要非常小心。在某些场景下，也会带来**堆内到堆外对象拷贝以及序列化的开销**，同时也带来了 GC 负载。

> NOTE：堆内堆外的内存空间不共享，task 往往会优先使用堆外，再使用堆内，但如果一个 task 堆外空间不够用，不会使用堆内而是直接 OOM。

## 3. OOM

Spark 中的 OOM 通常分为 Driver 与 Executor 端。

### 3.1 Driver 端

Driver 端情况较为简单，通常是处于以下两类问题：

- 创建的数据集过大，如使用 parallelize、createDataFrame 等 API
- 收集的数据集过大，显式的如 take、show、collect 等，隐式的有从 Executor 端汇聚广播变量

其中广播变量导致的 OOM 常见信息如下：

```scala
java.lang.OutOfMemoryError: Not enough memory to build and broadcast
```

可以通过以下代码先预估需要的内存大小：

```scala
val df: DataFrame = _
df.cache.count
val plan = df.queryExecution.logical
val estimated: BigInt = spark
.sessionState
.executePlan(plan)
.optimizedPlan
.stats
.sizeInBytes
```

### 3.2 Executor 端

Executor 端情况较为复杂， 由于 Spark 将其划分为了多块内存，因此需要逐个分析。

通常可以先排除 Reserved Memory 以及 Storage Memory，前者是固定大小，用户无法控制，后者在内存不足时，会选择落盘。

对于 User Memory 的 OOM，常见信息如下：

```scala
java.lang.OutOfMemoryError: Java heap space at java.util.Arrays.copyOf
 
java.lang.OutOfMemoryError: Java heap space at java.lang.reflect.Array.newInstance
```

用户自定义的数据结构大小 #size，需要结合 Executor 线程数量 #threads，得出自定义数据总的消耗内存： #size * #threads，一旦消耗内存超过 User Memory 则会 OOM。

对于 Execution Memory 的 OOM，情况较为复杂，**通常导致其 OOM 的原因并不是数据量过大，而是没有平衡好分片大小与并发度（线程可分配内存）之间的关系**。

假设一个 Executor 的并发度为 N，那么最理想的情况下，每个线程可分配的内存在 1/2N ~ 1/N 之间，如果某个 task 所需内存大小大于线程所能获取的内存，则很可能会产生 OOM。同时这里也要考虑真实并发度 ~N 并不总是等于 Executor 设定的线程数（<= N），但最终会逐渐趋于 N。

通常有以下两种场景会导致数据分片大小超过线程内存大小：

- 数据倾斜，某个数据分片过大
- 数据膨胀，数据读取到内存后，往往会有一定程度的膨胀，这是由于数据在加载之前可能进行了序列化、压缩等，同时 JVM 里的对象也往往大于原始数据
- Execution 剩余内存无法满足 Reducer task 所需内存 `spark.reducer.maxSizeInFlight`

解决思路通常有三种：

- 对于某个数据分片过大的，可以通过打散数据，降低数据粒度
- 减少 Executor 并发度以增加线程单个线程所能够分配到的内存（需结合业务场景）
- 增加 Executor 内存，提高每个线程所能够分配到的内存

### 3.3 堆外内存

堆外内存 OOM 较为少见，通常发生在 Shuffle Reduce 阶段，Reducer task 通过 Netty 拉取远端数据到堆外内存中。

首先确认是否数据倾斜问题，某个数据分片如果特别大，很可能会把堆外内存用光（JDK8 开始不指定的话默认最大值不超过堆内存），解决了数据倾斜大概率这个问题也会消失。

如果仍然存在这个问题，那么可以通过控制 Shuffle Reduce 落到磁盘来解决，设置 `spark.maxRemoteBlockSizeFetchToMem` 来触发 buffer 的尽早落盘。

参考：

- [**Dive into Spark memory**](https://luminousmen.com/post/dive-into-spark-memory)

