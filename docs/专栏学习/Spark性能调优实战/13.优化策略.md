---
title: 13. 实战优化
date: 2021-05-16
---

## 实战优化

### 大表 Join 小表

通常说的大小表，都是相对比较的，在实际工作中，总会遇到一些场景，小表超过了 Broadcast 的阈值，这时需要采取一些手段进行优化。

1. 针对 Join Key 远远大于需要关联起来的值的时候，通常一个较好的做法就是**将 Join Key 替换为 Hash**，并且为了避免 Hash 碰撞，还可以通过双重哈希，几乎不可能存在碰撞。

   ![](https://static001.geekbang.org/resource/image/bb/bf/bb79544467e98f9d2b6f13a437bb2dbf.jpg)

2. 借助 Spark AQE 的 Join 策略调整，当维度表过滤了部分数据达到 Broadcast 阈值后触发 Broadcast Join。

3. 如果 Join Key 能够过滤事实表较多的数据，或者种类较少，那么可以提前按照该 Key 值进行分区，充分利用 DPP 特性。

4. 如果小表数据分布均匀，那么可以考虑将其做 Shuffle Hash Join；如果分布不均，则可以借助 AQE 的自动倾斜处理切割数据分片再做 Shuffle Hash Join。

### 数据倾斜

当大表 Join 大表时，调优策略跟上面类似，但当存在数据倾斜时，则通常需要采取两种措施：

1. 分治，将查询拆分为多次查询，最后做 union
2. 针对倾斜的 key 做加盐操作，之后进行**两阶段 Shuffle 关联**

分治较好理解，如果需要查询一个月的数据，那么通过一天天查询后再做 union 结果合并即可。

针对大表的数据倾斜，AQE 只能解决其 task 之间的倾斜，但对同个 executor 来说，仍存在负载不均衡的问题，这时就可以通过两阶段 Shuffle 关联来解决问题。

两阶段 Shuffle 关联主要分为两阶段：

1. 筛选出倾斜的 key，对两个需要关联数据集的倾斜 key 按照相同规则进行加盐，经过 Shuffle 后进行一次预关联
2. 对 key 值去盐后再次进行 Shuffle 关联

以 orders 和 transactions 两张表为例，其中数据倾斜为单边倾斜：

```scala
//统计订单交易额的代码实现
val txFile: String = _
val orderFile: String = _
 
val transactions: DataFrame = spark.read.parquent(txFile)
val orders: DataFrame = spark.read.parquent(orderFile)
 
transactions.createOrReplaceTempView(“transactions”)
orders.createOrReplaceTempView(“orders”)
 
val query: String = "
select sum(tx.price * tx.quantity) as revenue, o.orderId
from transactions as tx inner join orders as o
on tx.orderId = o.orderId
where o.status = 'COMPLETE'
and o.date between '2020-01-01' and '2020-03-31'
group by o.orderId
"
 
val outFile: String = _
spark.sql(query).save.parquet(outFile)
```

接着按照倾斜的 key，将数据集分为倾斜与非倾斜两部分：

```scala
//根据Join Keys是否倾斜、将内外表分别拆分为两部分
import org.apache.spark.sql.functions.array_contains
 
//将Join Keys分为两组，存在倾斜的、和分布均匀的
val skewOrderIds: Array[Int] = _
val evenOrderIds: Array[Int] = _
 
val skewTx: DataFrame = transactions.filter(array_contains(lit(skewOrderIds),$"orderId"))
val evenTx: DataFrame = transactions.filter(array_contains(lit(evenOrderIds),$"orderId"))
 
val skewOrders: DataFrame = orders.filter(array_contains(lit(skewOrderIds),$"orderId"))
val evenOrders: DataFrame = orders.filter(array_contains(lit(evenOrderIds),$"orderId"))
```

对于数据分布均匀的数据集，可以采用 Shuffle Hash Join 来进行关联：

```scala
//将分布均匀的数据分别注册为临时表
evenTx.createOrReplaceTempView(“evenTx”)
evenOrders.createOrReplaceTempView(“evenOrders”)
 
val evenQuery: String = “
select /*+ shuffle_hash(orders) */ sum(tx.price * tx.quantity) as revenue, o.orderId
from evenTx as tx inner join evenOrders as o
on tx.orderId = o.orderId
where o.status = ‘COMPLETE’
and o.date between ‘2020-01-01’ and ‘2020-03-31’
group by o.orderId
”
val evenResults: DataFrame = spark.sql(evenQuery)
```

对于数据倾斜的数据集，则进行两阶段 Shuffle 关联：

```scala
import org.apache.spark.sql.functions.udf
 
//定义获取随机盐粒的UDF
val numExecutors: Int = _
val rand = () => scala.util.Random.nextInt(numExecutors)
val randUdf = udf(rand)
 
//第一阶段的加盐操作。注意：保留orderId字段，用于后期第二阶段的去盐化
 
//外表随机加盐
val saltedSkewTx = skewTx.withColumn(“joinKey”, concat($“orderId”, lit(“_”), randUdf()))
 
//内表复制加盐
var saltedskewOrders = skewOrders.withColumn(“joinKey”, concat($“orderId”, lit(“_”), lit(1)))
for (i <- 2 to numExecutors) {
saltedskewOrders = saltedskewOrders union skewOrders.withColumn(“joinKey”, concat($“orderId”, lit(“_”), lit(i)))
}
```

接着进行第一阶段的 Shuffle 关联：

```scala
//将加盐后的数据分别注册为临时表
saltedSkewTx.createOrReplaceTempView(“saltedSkewTx”)
saltedskewOrders.createOrReplaceTempView(“saltedskewOrders”)
 
val skewQuery: String = “
select /*+ shuffle_hash(orders) */ sum(tx.price * tx.quantity) as initialRevenue, o.orderId, o.joinKey
from saltedSkewTx as tx inner join saltedskewOrders as o
on tx.joinKey = o.joinKey
where o.status = ‘COMPLETE’
and o.date between ‘2020-01-01’ and ‘2020-03-31’
group by o.joinKey
”
//第一阶段加盐、Shuffle、关联、聚合后的初步结果
val skewInitialResults: DataFrame = spark.sql(skewQuery)
```

得到第一阶段的关联结果后，接着进行第二阶段的去盐化关联：

```scala
val skewResults: DataFrame = skewInitialResults.select(“initialRevenue”, “orderId”)
.groupBy(col(“orderId”)).agg(sum(col(“initialRevenue”)).alias(“revenue”))
```

最后合并倾斜与非倾斜的结果集：

```scala
evenResults union skewResults
```

两阶段 Shuffle 主要是开发成本与执行性能的博弈，当 executor 因为数据倾斜而称为整个执行性能瓶颈的时候，则这种开发投入就是值得的。
