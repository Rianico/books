---
title: 00.附录
date: 2021-04-05
---

## 相关问答

**Q：DAGScheduler 在创建 Tasks 的过程中，是如何设置每一个任务的本地性级别？**

A：DAGScheduler 会通过 SchedulerBackend 获取集群资源的可用信息：

- 如果 RDD 被缓存
- 如果 RDD 有 preferredLocations 属性
- 找到属于窄依赖的父 RDD

DAGScheduler将生成好的TaskSet提交给TaskSetManager进行任务的本地性级别计算

C：待看源码

---

**Q：为什么 Spark 使用 LinkedHashMap 来记录 Block 的元数据？**

A：Spark 出于存储空间的考虑，会采用 LRU 算法自动淘汰一段时间内用不到的存储对象，而 LinkedHashMap 的 put/get 特性，正好可以实现这种淘汰策略。

---

**Q：Broadcast 广播变量的存储？**

C：待看源码，直到 Spark 3.0 为止，无论广播变量在哪生成，都会先收集到 driver 端在进行分发。

---

Q：什么是火山迭代模型和阶段代码生成（Whole Stage Code Generation）？

A：

---

**Q：AQE 的分区合并算法略显简单粗暴，如果让你来重新实现分区合并特性的话，你都有哪些思路呢？**

---

**Q：AQE 中数据倾斜的处理机制，你认为有哪些潜在的隐患？**

---

Q：自动数据倾斜处理

https://www.waitingforcode.com/apache-spark-sql/whats-new-apache-spark-3-join-skew-optimization/read

---

**Q：Spark 3.0 中，除了 Broadcast 关键字之外还支持那些 Join Hints？**

A：参考 JoinStrategyHint.scala

---

**Q：从 Executor 并发度、执行内存大小和分布式任务并行度出发，你认为在什么情况下会出现 OOM 的问题？**

并发度决定了数据分片的大小，当一个数据分片需要的内存大于（执行内存 / 并行度）时，则会发生 OOM。

由于 Storage Memory 跟 Execution Memory 使用的不是同一把锁，Executor 如果并发度过高，在极端情况下，当一个 task 请求 Storage Memory 的内存成功后，如果在 allocation 之前那块内存被其他 task 抢占了，且此时没有其余内存空闲，那么会导致 OOM。

---

**Q：由于执行内存总量 M 是动态变化的，并发任务数 N~ 也是动态变化的，因此每个线程申请内存的上下限也是动态调整的，你知道这个调整周期以什么为准？**

A：Executor内正在运行的这批任务执行完，下一批任务被执行前，就进行资源调整。根据此时（执行内存/min(待分配任务数，executor.cores)）进行内存分配，所以如果有多个 core，但只有一个 task，该 task 也是可以获得全部执行内存的。

----

**Q：User Memory、Execution Memory、Storage Memory 是属于 Spark 自身对内存区域的划分，但 Spark 的 executor 实际上又是一个 JVM，假如我把 User Memory 设置的非常小，又自定义了一个很大的数据结构，此时 User Memory 不够用了，而 Execution Memory、Storage Memory 还有很大的空闲，那么这时候会不会 OOM？**

A：Spark 并不会立刻 OOM，如果其余部分的内存仍有剩余，那么 User Memory 也是会占用其内存的，Spark 对内存的预估并没有那么精准。

Spark 对各个区域内存的划分本质上更加倾向于一种软限制，并不会突破 JVM 的限制。而 User Memory 这种额外的抢占，很可能出现有时看起来是 rdd 缓存或者执行内存导致了 OOM，实际上是由于 User Memory 抢占了这部分内存空间才导致的。

---

**Q：DAG 以 Shuffle 为边界划分 Stages，Spark 是根据什么来判断一个操作是否会引入 Shuffle 的呢？**

A：从 Action 开始溯源，如果子 RDD 依赖父 RDD 的数据，并且两者的分区器不同，则会产生 Shuffle，常见表现为宽依赖，窄依赖则不一定，所以本质上还是取决于分区器。

---

**Q：在 Spark 中，同一 Stage 内的所有算子会融合为一个函数是如何实现的？**

A：task 启动后，会在 Stages 内部最后的 rdd 处，将其 compute 作用于父 rdd 之上，父 RDD 则继续往上传递并作用，如此反复直到根 rdd。

https://www.jianshu.com/p/45c9ee55eea6

---

**Q：既然 Catalyst 在逻辑优化阶段有 81 条优化规则，我们还需要遵循“能省则省、能拖则拖”的开发原则吗？**

A：Catalyst 的优化只是一种普世的优化逻辑，不可能覆盖所有的场景，因此仍需要我们遵循特定的开发规则，结合业务特点，达到自我优化+底层框架优化的双重效果。

---

**Q：你能说说 Spark 为什么用偏函数，而不是普通函数来定义 Catalyst 的优化规则吗？**

A：普通的函数可以理解为输入一个变量 x，就会执行特定的运算得到一个 y，可以把 y 看成是执行了一个特定计算的 x。Scala 中的偏函数更类似于一种匹配，如果匹配到一个特定的计算则继续执行下一个指定的计算，否则执行其他计算，可以灵活拆分组合。

Catalyst 使用偏函数来实现，可以很灵活的制定优化规则，只关注需要优化的场景，一旦部分场景满足了 Catalyst 的优化规则，则应用该优化，并且也可以随时添加新的优化规则增加覆盖的场景。

---

**Q：为什么 Spark SQL 没有实现 Broadcast Sort Merge Join 策略？**

A：Broadcast 已支持的两种 Join 策略分别为：

- Hash Join：主要用于等值连接
- Nested Loop Join：笛卡尔积，不等值等

这两种方式其实已经覆盖了 Broadcast Sort Merge Join 的需求，当能够进行 Broadcast 的时候，也说明数据量相对较少较少，排序带来的收益也并不高，因此没必要实现。

---

**Q：为什么 Spark SQL 仅支持 RBO（Rule Based Optimization，基于规则的优化）而不是重点依赖于 CBO（Cost Based Optimization，基于成本的优化）？**

A：Spark 在 2.2 版本推出了 CBO，CBO 的特点是实事求是，但应用存在三个问题：

1. **窄**：引用范围太小，仅支持注册到 Hive Metastore 的数据表，而不支持其它数据源
2. **慢**：CBO 的统计信息收集效率较低，对于注册到 Hive Me他store 的数据表，开发者需要调用 ANALYZE TABLE COMPUTE STATISTICS 消耗大量时间
3. **静**：静态优化，与 RBO 一样，不会动态的修改执行计划