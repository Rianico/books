---
title: 07.常用配置项
date: 2021-04-11
---



Spark 的参数调优主要集中在执行计算任务的 Executor 端，分为**硬件资源类**、**Shuffle 类**和 **Spark SQL** 三大类。

Spark 调优的本质是对物力资源的平衡利用。

## 1. 硬件资源类

硬件资源类包含的是与 CPU、内存、磁盘有关的配置项，调优的切入点是瓶颈，定位瓶颈的有效方法之一，就是从硬件的角度出发。

### 1.1 CPU 相关

- **spark.cores.max**：指定集群范围内 Spark 使用的最大核心数
- **spark.executor.cores**：指定 Executor 范围内 Spark 使用的核心数
- **spark.task.cpus**：指定 task 级别使用的核心数

为了充分利用划拨给 Spark 集群的每一颗 CPU 核，需要设置恰当的并行度：

- **spark.default.parallelism**：未指定明确分区时使用的默认并行度，Yarn 模式下默认取所有 executor core 的数量。
- **spark.sql.shuffle.partitions**：明确指定数据关联或聚合操作中 Reduce 端的分区数量，默认 `200`。

Spark 需要区分数据的**并行度（Parallelism）**以及**并行计算任务（Paralleled Tasks）**,前者针对数据分片，后者针对计算任务，决定了在任一时刻整个集群能够同时计算的任务数量。

一个 Executor 中，task 并行计算的上限是 spark.executor.cores 与 spark.task.cpus 的商；一个 Spark 作业中，task 并行计算的上限则是在前面的基础上再乘以 Executor 数。

### 1.2 内存相关

- **spark.executor.memory**：单个 Executor 堆内内存总大小
- **spark.memory.offHeap.size**：单个 Executor 堆外内存总大小
- **spark.memory.fraction**：堆内内存中 Storage 和 Execution 总共占的内存比例，默认 0.6
- **spark.memory.storageFraction**： Storage 与 Execution 之间的内存比例，默认 0.5
- **spark.rdd.compress**：是否开启 RDD 压缩，会对以序列化方式进行缓存的 RDD 进行压缩，默认 false
- **spark.broadcast.compress**：是否开启 Broadcast 压缩，默认 false

Spark 的内存调整需要注意几个地方：

1. 堆内与堆外内存比例，Spark 在使用堆外内存时，应用了一种紧凑的二进制格式来存储对象，**对于定长类型的数据格式十分有效**，而对于非定长的数据，则会通过指针、偏移量、长度等进行记录：

   ![](https://static001.geekbang.org/resource/image/51/2c/516c0e41e6757193533c8dfa33f9912c.jpg)

   因此在表示复杂类型时，反而会因为过多的指针与偏移地址导致访问效率大打折扣，同时更加难以管理。

2. User Memory 与 Spark 可用内存的比例，**spark.memory.fraction** 参数决定着两者如何瓜分堆内内存，它的系数越大，Spark 可支配的内存越多，如果自定义数据结构生成的对象较少，可以考虑适当调大该值。

3. 平衡 Execution Memory 与 Storage Memory 比例，在统一内存管理模式下，spark.memory.storageFraction 显得不是那么无关紧要，但如果作业是偏向缓存型的，那么就有必要调大该值，以保证数据的全量缓存。但缓存的 RDD 会对作业造成以下几个方面的影响：

   - 挤占 Execution Memory 的空间
   - RDD 缓存会占用老年代空间，一旦触发 Full GC，会有较大的开销
   - 可以通过开启缓存序列化以及 RDD 压缩（spark.rdd.compress）来节省内存，但代价则是对 CPU 的消耗。

### 1.3 磁盘相关

- **spark.local.dir**：指定 RDD 缓存的磁盘目录

## 2. Shuffle 类

Shuffle 类是专门针对 Shuffle 操作的。在绝大多数场景下，Shuffle 都是性能瓶颈。

官方地址：[Shuffle Behavior](http://spark.apache.org/docs/latest/configuration.html#shuffle-behavior)。

Shuffle 能进行的调优参数较少，分为 Mapper 和 Reducer 两部分。

- **spark.shuffle.file.buffer**： Map 输出端写入缓冲区大小，默认 `32k`，通过增大 buffer ，减少落盘频率
- **spark.reducer.maxSizeInFlight**： Reduce 端的读取缓冲区大小，默认 `48m`，通过增大 buffer 增加每次拉拉取的数据，减少网络请求
- **spark.shuffle.sort.bypassMergeThreshold**：默认 `200`，当 Reducer 并行度低于该阈值时，Shuffle 在 Map 端均不会引入排序

同时还有一些稳定性参数设置：

- **spark.shuffle.compress**：开启 shuffle 压缩，默认开启
- **spark.io.compression.codec**：指定 shuffle 中间文件压缩所使用压缩类型
- **spark.shuffle.io.maxRetries**： shuffle I/O 失败的最大重试次数，默认 `3`
- **spark.shuffle.io.retryWait**： shuffle I/O 失败后的重试间隔，默认 `5s`

## 3. Spark SQL

Spark SQL 早已演化为新一代的底层优化引擎，专门梳理出一类配置项，去充分利用 Spark SQL 的先天性能优势。

Spark SQL 的配置项中，AQE 引入的三个特性（自动分区合并、自动数据倾斜处理和 Join 策略调整）对执行性能影响最大。

AQE 是 Spark SQL 的一种动态优化机制，在运行时，每当 Shuffle Map 阶段执行完毕，AQE 都会结合这个阶段的统计信息，基于既定的规则动态地调整、修正尚未执行的逻辑计划和物理计划，来完成对原始查询语句的运行时优化。

AQE 优化机制触发的时机是 Shuffle Map 阶段执行完毕，也就是说 AQE 优化的频次与执行计划中 Shuffle 的频率一样。

AQE 默认是关闭的，需要设置 **spark.sql.adaptive.enabled** 为 true 开启。

AQE 既定的规则和策略主要有 4 个，分为 1 个逻辑规则和 3 个物理优化策略：

![](https://static001.geekbang.org/resource/image/1c/d5/1cfef782e6dfecce3c9252c6181388d5.jpeg)

### 3.1 自动分区合并

首先设置 `spark.sql.adaptive.coalescePartitions.enabled` 为 true 开启自动分区合并功能。

**自动分区合并发生在 Reducer 端**，受到两个参数影响：

- **spark.sql.adaptive.advisoryPartitionSizeInBytes**：指定 RDD 分区的推荐大小
- **spark.sql.adaptive.coalescePartitions.minPartitionNum**：指定 RDD 最小分区数，避免无法充分利用 CPU

Spark 可以在 Shuffle 过程中获取数据大小相关信息，然后通过这两个维度计算出各自的 RDD 分区数据大小，取其中的较小值作为指定的尺寸，接着按照分区号进行扫描，一旦扫描的数据超过该值就会进行合并：

![](https://static001.geekbang.org/resource/image/da/4f/dae9dc8b90c2d5e0cf77180ac056a94f.jpg)

### 3.2 自动数据倾斜处理

Spark 3.0 的自动数据倾斜暂且只支持 Sort Merge Join，并且需要结合几个参数实现大分区划分为小分区。

首先需要设置 **spark.sql.adaptive.skewJoin.enabled** 为 true 开启该功能。

AQE 会统计所有数据分区大小并排序，得到中位数作为**放大基数**：

- **spark.sql.adaptive.skewJoin.skewedPartitionFactor**：大于放大基数一定倍率的分区有可能判定为倾斜分区，默认取 `5`。

- **spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes**：有可能被认定为倾斜分区，分区大小还需要大于该设定值（默认 `256m`），才能判定为倾斜分区

倾斜分区最后又会根据 `spark.sql.adaptive.advisoryPartitionSizeInBytes` 指定的分区大小进行拆分。

### 3.3 Join 策略调整

在 AQE 之前，开发人员可以指定 `spark.sql.autoBroadcastJoinThreshold`（默认 `10m`）对数据关联操作进行主动降级，只要有其中一张表小于该值就可以将降级为 Broadcast Join。该参数存在两个问题：

- Spark 对小表尺寸的误判时有发生，导致降级失败，可靠性较差
- 某张表可能在 filter 后满足了 Broadcast Join 的条件，但 Spark 先前选择了 Shuffle Join 的执行计划，无法动态改变。

AQE 解决了上述问题，AQE 可以动态调整 Join 策略，并且在运行时获取表数据的大小比起编译时更加准确。

AQE 动态调整 Join 策略的前提为，在表经过过滤后，非空的数据分区必须小于 `spark.sql.adaptive.nonEmptyPartitionRatioForBroadcastJoin`（默认 `0.2`），才能触发 Broadcast Join 降级，可以适当调高该值，并且当前只支持 SMJ 降级。

由于 AQE 是在 Shuffle Map 阶段才能进行，AQE 开始优化之前，Shuffle 其实已经执行过半了（如落盘动作）。在 Shuffle Map 之前 AQE 并无法知道哪个是大表哪个是小表。表面上看 Join 策略调整毫无意义，但在 Shuffle 之后，**OptimizeLocalShuffleReader** 策略就开始生效了，它可以省去 Shuffle 常规步骤中的网络分发，Reduce Task 可以就近读取本地（Local）的中间文件，完成与小表的关联操作。

OptimizeLocalShuffleReader 是由 `spark.sql.adaptive.localShuffleReader.enabled` 控制开关的，默认为 True；如果设置为 False，那么 Join 策略调整就失去了意义。

![](https://static001.geekbang.org/resource/image/31/6a/31356505a2c36bac10de0e06d7e4526a.jpg)

