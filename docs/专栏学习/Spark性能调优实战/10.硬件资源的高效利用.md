---
title: 10.硬件资源的高效利用
date: 2021-04-27
---

## 前言

Spark 的资源调整，本质上就是平衡**并行度、并发度、内存空间**三者之间的关系。

## 1. CPU

### 1.1 Spark 中的 CPU

在 RDD 存储到内存中进行展开之前，RDD 实际占用的还是 Execution Memory，因此 **CPU 与内存的平衡，其实就是 CPU 与执行内存之间的协同与配比**。

协调 CPU 与 Execution Memory 的使用分为三类参数，分别控制着**并行度**、**执行内存大小**和**集群的并行计算能力**。

并行度明确了数据的划分，并行度越高，数据的分片（partition）越多，可以通过两个参数来设置：

- **spark.default.parallelism**：指定了 RDD 的默认并行度
- **spark.sql.shuffle.partitions**：指定了 Spark SQL 开发框架下，Reduce 阶段默认的并行度。

并发度，指一个 Executor 中用于执行 task 的线程数，由 `spark.executor.cores` 指定，`spark.task.cpus` 则指定了每个 task 使用的线程数，通常不需要调整，默认为 1。所以并发度基本由 **spark.executor.cores** 进行控制。

对于一个 Executor 来说，一个线程同一时间只能计算一个 task。因此，在**运行时**线程、task、partition 是一一对应的。

Executor 在接收到 Driver 端分发的任务后，会将其封装为 `TaskRunner`，交给线程池处理，处理的线程会像 Executor Memory 申请内存，然后开始执行任务。

假设有 N 个线程，每个线程可以申请到的内存空间，在 （Execution Memory / N） ~ （Execution Memory / 2N）之间。

### 1.2 CPU 低效的原因

主要原因分为线程挂起与调度开销。

#### 1.2.1 线程挂起

Spark 使用 HashMap 记录每个线程消耗的内存大小，在给定执行内存总量 M 和线程总数 N 的情况下，，确保每个线程占用的执行内存不超过 M / N。

在一些极端情况下，有些线程连 M / 2N 甚至一点内存都申请不到导致挂起，主要原因有以下几个：

- **动态变化的执行内存总量 M**，Storage Memory 与 Execution Memory 是可以相互转化的。
- **动态变化的实际并发度 N~**，当 Executor 刚开始执行 task 时，并不是可以立刻到达最大并发度 N 的，影响的因素有很多，如 task 的先后到来，task 之间的依赖等。但随着任务执行以及调度的推进，N~ 会迅速的趋近于 N，CPU 线程挂起和内存分配也会得到改善。
- 分布式数据集的数据分布，每个数据分片数据量的大小决定了每个 task 需要申请多少内存。

#### 1.2.2 调度开销

Spark 可以通过增加并行度，来减少每个数据分片的数据量大小，但也会带来一个副作用：**调度开销骤增**。

Driver 会将每个任务封装为 **TaskDescription**，然后分发给各个 Executor。TaskDescription 包含着与任务运行有关的所有信息，如任务 ID、尝试 ID、要处理的数据分片 ID、开发者添加的本地文件和 Jar 包、任务属性、序列化的任务代码等等。Executor 接收到 TaskDescription 之后，首先需要对 TaskDescription 反序列化才能读取任务信息，然后将任务代码再反序列化得到可执行代码，最后再结合其他任务信息创建 TaskRunner。

当数据过于分散，分布式任务数量会大幅增加，但每个任务需要处理的数据量却少之又少，CPU 花在数据处理以及任务调度上的开销几乎相同。

#### 1.2.3 优化 CPU 利用率

提高 CPU 的利用率，需要设置好一个合适的并行度，需要综合多个方面的考量，并没有一个绝对的设置值。

一种常用的手段是先确定每个 Executor 的执行内存总量 M 与并发度 N，去计算一个能够让数据分片平均大小在 （M/2N, M/N）之间的并行度。或者反过来，在确定并行度的大小后调整 Executor 的内存大小。

![](https://static001.geekbang.org/resource/image/4a/ce/4a5dc54813346924ec5611f6d1fa8fce.jpg)

## 2. 内存

### 2.1 内存区域的规划

用户可以管理的内存有 User Memory、Storage Memory、Execution Memory 三块，需要衡量好三者的比例以充分利用内存。

User Memory 主要用于存放用户的自定义数据结构，如果经常分发一个不变的对象到各个 task， Spark 无法识别出该变量是否为同一个，会耗费 User Memory 去存储多个（task 数量）相同内容的变量。这种场景下，通常可以使用广播变量进行优化。

使用广播变量后，该变量会存储到 Storage Memory 中，并且一个 Executor 只存储一个，而不会受到 task 数量的影响。

为了充分使用 Spark 的内存，需要平衡好不同区域内的消耗占比，具体分为两步：

1. 预估内存占用，假设共有 #N 个 Executor：
   - 计算 User Memory 的内存消耗，汇总应用中包含的自定义数据结构，预估其大小并乘以 Executor 的并发度得到该区域的内存消耗 **#User**
   - 计算 Storage Memory 的内存消耗，每个 Executor 中 Storage Memory 区域的内存消耗为广播变量 + RDD 缓存，即 **#Storage = #bc + #cache / #Executors**
   - 计算 Execution Memory 的内存消耗，可以根据数据分片的大小得出，即 **#Execution Memory = #threads * #dataset / #partitions**
2. 调整内存配置，根据上述三者的估算大小，对 `spark.memory.fraction` 和 `spark.memory.storageFraction`

**内存规划要达到的效果和目的，是确保不同内存区域的占比与不同类型的数据消耗保持一致，从而实现内存利用率的最大化**。

### 2.2 缓存

RDD 的缓存需要关注三点：

- **存的存储级别**：限定了数据缓存的存储介质（如内存、磁盘等）、存储形式（序列化、费序列化）以及副本数量（默认 1）。

  ![](https://static001.geekbang.org/resource/image/4e/e2/4ecdfd4b62b1c6e151d029c38088yye2.jpeg)

- **缓存的计算过程**：将 RDD 以 Block Iterator 形式展开，暂存到 ValueHolder 中，然后转化为 MemoryEntry，接着存储到一个 key 为 BlockId 的 LinkedHashMap 中。

- **缓存的销毁过程**：缓存数据以主动或是被动的方式，被驱逐出内存或是磁盘的过程。

Spark 使用一个 LinkedHashMap 来记录 RDD 在缓存中的对应信息，在尝试缓存 RDD 却发现内存不足时，会根据 LRU 算法，从头开始扫描 Map 挑选可淘汰的 MemoryEntry，同属于当前 RDD 的 MemoryEntry 则会跳过。当可淘汰的 MemoryEntry 大小超过需要缓存的 RDD 大小，则会淘汰掉那部分 MemoryEntry。

![](https://static001.geekbang.org/resource/image/b7/14/b73308328ef549579d02c72afb2ab114.jpg)

### 2.3 缓存的时机

对 RDD/DataFrame/Dataset 进行缓存，需要遵循以下 2 条基本原则：

- 如果 RDD/DataFrame/Dataset 在应用中的引用次数为 1，就坚决不使用 Cache
- **如果引用次数大于 1，且运行成本占比超过 30%，应当考虑启用 Cache**

运行成本占比，**指的是计算某个分布式数据集所消耗的总时间与作业执行时间的比值**。

例如，一个作业的执行时间规定为 1 小时，如果某个 DataFrame 需要用到多次，且生成该 DataFrame 需要耗时 12 分钟，那么该 DataFrame 的运行成本占比就是 $12*2/60=40\%$。

在对业务、资源情况还不熟悉时，可以借助 Spark 3.0 的 noop 指令来查询某个 DataFrame 的耗时，该指令只触发计算而不会产生落盘动作：

```scala
// 利用 noop 精确计算DataFrame运行时间
df.write
.format("noop")
.save()
```

### 2.4 缓存的注意事项

缓存操作 .cache() 是一个惰性操作，需要注意触发计算的算子，也会影响实际缓存的数据，如 `count` 算子会触发缓存的全量物化，而 `first`、`take` 和 `show` 这 3 个算子只会把涉及的数据物化。

Cache Manager 要求**两个查询的 Analyzed Logical Plan 必须完全一致**，否则无法复用 cache：

```scala
df.select(col1, col2).filter(col2 > 0).cache
//数据分析
df.filter(col2 > 0).select(col1, col2)
df.select(col1, col2).filter(col2 > 100)
```

Analyzed Logical Plan 是比较初级的逻辑计划，主要负责 AST 查询语法树的语义检查，确保查询中引用的表、列等元信息的有效性。像谓词下推、列剪枝这些比较智能的推理，要等到制定 Optimized Logical Plan 才会生效。

因此，即使是同一个查询语句，仅仅是调换了select和filter的顺序，在 Analyzed Logical Plan 阶段也会被判定为不同的逻辑计划。

**为什么 Cache Manager 不在 Optimized Logical Plan 执行 cache 复用的管理？**

> Spark 使用 `（LogicalPlan，InMemoryRelation）` 来表示 Logical Plan 及其对应的 cache 数据信息，两个执行结果相同的查询能否复用 cache，就转化为了两个查询的 Logical Plan 是否能够一致。
>
> Spark 会对不同查询的 AST 进行归一化，尽可能消除一些无关痛痒的小细节。但对于执行结果相同的不同查询，Spark 无法保证归一化后两者完全相同，并且 Spark 需要保证执行结果不同的两个查询的 Logical Plan 必定是不同的，因此只能一并包含进去，统一不做 cache 复用。
>
> cache 是否复用的判断越早做越省事。如果使用 Optimized Plan 作为 key，那么到了后面要么很难命中，要么即使命中了发现优化白做了。
>
> 一个优化思考，社区暂且没有：对 SQL 按照传统 DBMS 进行 SQL Re-write，这样可以很大程度的让执行结果相同的不同查询生成的 Logical Plan 相同。

## 3. 磁盘

磁盘在 Spark 中通常是起到两个作用：

1. 溢出临时文件
2. 存储 Shuffle 中间文件

除了 cache 之外，Shuffle 落盘也能作为计算失败时的一个回溯点，而不必从头计算，减少失败重算的代价。

![](https://static001.geekbang.org/resource/image/35/86/35c13d9f2eba5d23dabe05249ccb9486.jpg)

**ReuseExchange** 是 Spark SQL 众多优化策略中的一种，**相同或是相似的物理计划可以共享 Shuffle 计算的中间结果**。

使用 ReuseExchange 有两个条件：

1. 多个查询所依赖的分区规则要与 Shuffle 中间数据的分区规则保持一致
2. 多个查询所涉及的字段（Attributes）要保持一致

例如以下代码：

```scala
//版本1：分别计算PV、UV，然后合并
// Data schema (userId: String, accessTime: Timestamp, page: String)
 
val filePath: String = _
val df: DataFrame = spark.read.parquet(filePath)
 
val dfPV: DataFrame = df.groupBy("userId").agg(count("page").alias("value")).withColumn("metrics", lit("PV"))
val dfUV: DataFrame = df.groupBy("userId").agg(countDistinct("page").alias("value")).withColumn("metrics ", lit("UV"))
 
val resultDF: DataFrame = dfPV.Union(dfUV)
 
// Result样例
| userId | metrics | value |
| user0  | PV      | 25 |
| user0  | UV      | 12 |
```

dfPV 与 dfUV 都会有各自的执行路径，两者仅是数据来源一致，可能不同 executor 读取的文件并不一致，Shuffle 的中间数据不一致，无法利用到 ReuseExchange 机制：

![](https://static001.geekbang.org/resource/image/dd/28/dd150e0863812522a6f2ee9102678928.jpg)

修改后如下：

```scala
//版本2：分别计算PV、UV，然后合并
// Data schema (userId: String, accessTime: Timestamp, page: String)
 
val filePath: String = _
val df: DataFrame = spark.read.parquet(filePath).repartition($"userId")
 
val dfPV: DataFrame = df.groupBy("userId").agg(count("page").alias("value")).withColumn("metrics", lit("PV"))
val dfUV: DataFrame = df.groupBy("userId").agg(countDistinct("page").alias("value")).withColumn("metrics ", lit("UV"))
 
val resultDF: DataFrame = dfPV.Union(dfUV)
 
// Result样例
| userId | metrics | value |
| user0  | PV      | 25 |
| user0  | UV      | 12 |
```

修改后的执行路径如下：

![](https://static001.geekbang.org/resource/image/00/b2/008e691de73eefc6daa4886017fa33b2.jpg)

这样一来就能充分利用 ReuseExchange 机制，从数据角度来看，ReuseExchange 其实起到了跟持久化到磁盘一致的功能。

需要注意的是，如果将 agg 操作中涉及的字段修改为其它字段，由于涉及的字段不一致，也无法使用 ReuseExchange 机制。

## 4. 总结

CPU 的充分利用，主要在于调节好**并发度**、**并行度**以及**每个 Executor 的内存大小**。

内存方面，需要考虑较多：

- 不同内存区域的划分，其中 Execution Memory 需要根据实际并发度考虑，并且通常 OOM 的原因不是内存不够，而是某个数据分片能超过了线程可用内存
- 缓存需要考虑好时机，某个反复用到的数据分片计算成本大于 30% 即可考虑进行缓存

磁盘方面，可以考虑增大一些 buffer 减少落盘次数，同时磁盘的 shuffle 也起到了类似于存档点的作用（如 ReuseExchange 机制）。

