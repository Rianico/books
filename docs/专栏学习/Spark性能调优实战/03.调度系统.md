---
title: 03.调度系统
date: 2021-03-28
---

## 1. DAG 与流水线式计算模式

Spark 中，内存计算有两层含义：

1. 分布式数据缓存，即对频繁访问数据的缓存操作。
2. **Spark 内部的流水线式计算模式**。

DAG（Direct Acyclic Graph，有向无环图），在 Spark 的 DAG 中，顶点是一个个 RDD，边则是 RDD 之间通过 dependencies 属性构成的父子关系。

DAG 会根据分布式数据集上调用的算子来进行构建。

Spark 会将 DAG 转化为分布式任务，期间会经历四个步骤：

1. 回溯 DAG 并划分 Stages
2. 在 Stages 中创建**分布式任务**
3. 分布式任务的分发
4. 分布式任务的执行

Spark 的内存计算第二层含义就在 Stages 内部，**以 Actions 算子为起点，从后向前回溯 DAG，以 Shuffle 操作为边界去划分 Stages**。

Spark 的流水线计算模式高效的地方在于：**同一 Stage 内部，所有算子融合为一个函数，Stage 的输出结果由这个函数一次性作用在输入数据集而产生**。

> NOTE：对比起 MapReduce 计算引擎，Map 之间使用磁盘交换数据，Map 与 Reducer 之间通过磁盘交换数据；Spark 则是直接在一个 Stage 内将所有函数融合为一个函数一次性作用并在内存中进行计算。

Stages 是以 Shuffle 为边界，因此 Shuffle 极大降低性能除了 I/O 方面的原因，也有 Stage 方面的原因。

## 2. 调度系统

Spark 的调度系统主要由三个部分组成：**DAGScheduler、TaskScheduler、SchedulerBackend**。

Spark 的调度过程如下：

1. DAGScheduler 将 DAG 划分为不同的 Stages；
2. DAGScheduler 创建分布式任务 Tasks 和任务组 TaskSet，并交给 TaskScheduler；
3. SchedulerBackend 获取集群内可用的计算资源；
4. TaskScheduler 根据优先级策略设置 Task 调度信息，并根据集群可用计算资源，优先提交满足本地性的 Task 到 SchedulerBackend。
5. SchedulerBackend 根据 Task 信息，分发 Task 到 Executor 执行。

Spark 调度系统的核心职责是，**先将用户构建的 DAG 转化为分布式任务，结合分布式集群资源的可用性，基于调度规则依序把分布式任务分发到执行器**。

**DAGScheduler** 的职责有二：

1. 根据 DAG 划分 Stages；
2. 在 Stage 内创建 Task，这些 Task 囊括了用户指定的数据转换逻辑，并负责将同一 Stage 内的 Task 合并为一个函数。

**SchedulerBackend** 负责获取集群资源的相关信息，是对资源调度器的封装与抽象，支持 Standalone、YARN 和 Mesos 等，均提供了具体实现。

SchedulerBackend 使用 **ExecutorDataMap** 的数据结构来存储集群资源信息，其中 key 标记了 Executor，value 是一种叫做 **ExecutorData** 的数据结构，ExecutorData 用于封装 Executor 的资源状态，如 RPC 地址、主机地址、可用 CPU 核数和满配 CPU 核数等。 

![](https://static001.geekbang.org/resource/image/a7/a9/a7f8d49bbf1f8b0a125ffca87f079aa9.jpg)

对内，SchedulerBackend 用 ExecutorData 对 Executor 进行资源画像；

对外，SchedulerBackend 以 **WorkerOffer** 为粒度提供计算资源，WorkerOffer 封装了 Executor ID、主机地址和 CPU 核数，用来表示一份可用于调度任务的空闲资源。

DAGScheduler 生成了需要执行的 Task，SchedulerBackend 提供了可以运行 Task 的集群资源，TaskScheduler 则会基于既定的策略与规则，对 Task 进行一些设置（如本地性、调度优先级等），并提交给 SchedulerBackend，由其根据一定规则将其分发到 Executor 执行。

![](https://static001.geekbang.org/resource/image/82/yy/82e86e1b3af101100015bcfd81f0f7yy.jpg)

**TaskScheduler** 的调度优先级分为 Stage 和 Task 两方面：

- 对于 Stage 的调度，TaskScheduler 提供了 FIFO 和 Fair 两种调度方式：
  - **FIFO**：按照 Stage 创建的先后时间进行调度
  - **Fair**：设置多个不同优先级的调度池，将 Stage 与调度池进行关联，从而能够按照指定的优先级策略对 Stage 进行调度。
- 对于 Task 的调度，TaskScheduler 的处理主要包含以下几方面：
  - TaskScheduler 会根据接收到的 WorkerOffer，优先选择那些满足数据本地性（Process local < Node local < Rack local < Any）的 task 进行分发。
  - TaskScheduler 根据本地性挑选出待执行的 task 后，会将其序列化后交给 SchedulerBackend，由其分发到 Executor 上执行。

Task 自带调度意愿，通过本地性级别告诉 TaskScheduler 自身偏向于在哪些 Executor 上进行调度。

Spark 的调度系统做了很多优化，在编写 Spark 程序时，需要了解其调度原理，从而写出可以让 Spark 做出更多优化的程序。

考虑此场景：根据传入的字典文件路径加载字典，之后根据传入的 key 值获取对应的值，通常实现如下：

```scala
/**
 *实现方式1
 *输入参数：模板文件路径，用户兴趣字符串
 *返回值：用户兴趣字符串对应的索引值
*/
 
//函数定义
def findIndex(templatePath: String, interest: String): Int = {
val source = Source.fromFile(filePath, "UTF-8")
val lines = source.getLines().toArray
source.close()
val searchMap = lines.zip(0 until lines.size).toMap
searchMap.getOrElse(interest, -1)
}
 
//Dataset中的函数调用
findIndex(filePath, "体育-篮球-NBA-湖人")
```

这种写法有个问题，在 Spark 调度系统中，会将 `findIndex()` 函数传递到各个 Executor 上执行，那么每个 Executor 都会加载一份字典文件，效率十分低下。

改写后的代码如下：

```scala
/**
 *实现方式2
 *输入参数：模板文件路径，用户兴趣字符串
 *返回值：用户兴趣字符串对应的索引值
*/
 
//函数定义
val findIndex: (String) => (String) => Int = {
(filePath) =>
val source = Source.fromFile(filePath, "UTF-8")
val lines = source.getLines().toArray
source.close()
val searchMap = lines.zip(0 until lines.size).toMap
(interest) => searchMap.getOrElse(interest, -1)
}
val partFunc = findIndex(filePath)
 
//Dataset中的函数调用
partFunc("体育-篮球-NBA-湖人")
```

改写后，将原来的函数改造为了高阶函数，这样一来，Spark 会先在 driver 端先完成字典文件的加载，再在各个 Executor 上根据 key 值查询字典获取对应的值。

