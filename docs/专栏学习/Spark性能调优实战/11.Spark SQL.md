---
title: 11. Spark SQL
date: 2021-04-30
---

## 1. Spark RDD 与 Spark SQL

Spark 3.0 中对 Spark SQL 进行了大量的优化更新：

![](https://static001.geekbang.org/resource/image/47/75/479cd67e687a3a7cdc805b55b5bdef75.jpg)

Spark SQL 取代了 Spark Core，成为了新一代的引擎，其他组件都能从中获的收益。

Spark RDD 主要存在以下痛点：

1. **优化空间有限**，对于 Spark 来说，它可以感知到我们做了 map、filter 等操作，但我们执行的函数操作，对于 Spark 来说是一个黑盒，只能简单的将其分发到各个 Executor 执行
2. **数据元信息过少**，Spark 对 RDD 形式的数据集能感知的信息很少，并且 RDD 能存储各种半结构、非结构化的数据，Spark 无法对其进行优化

Spark SQL 中的 DataFrame 中所携带的 Schema 可以让 Spark 根据明确的字段类型设计定制化的数据结构（**Tungsten**），从而大幅提升数据的存储和访问效率，同时也可以让 Spark  基于启发式的规则和策略（**Catalyst**），甚至是动态的运行时信息（**AQE**）去优化 DataFrame 的计算过程。

Spark Core 能够提供很灵活的操作，但也正因此 Spark 也无法获取关于数据太多的数据，也就无法针对性的做出优化。

## 2. Spark SQL 核心组件

### 2.1 Catalyst

Spark 会基于 DataFrame 确切的计算逻辑，使用第三方的 SQL 解析器 ANTLR 来生成抽象语法树（AST，Abstract Syntax Tree），节点记录的是**标量算子（如 select、filter）的处理逻辑**，边携带的是数据信息：**关系表和数据列**。

![](https://static001.geekbang.org/resource/image/1c/6f/1c0a5e8c1ccdc5eb6ecc29cc45d3f96f.jpg)

Spark 中的语法树也称为 `Unresolved Logical Plan`，是 Catalyst 优化的起点，这个阶段中记录的信息仅仅是一些字符串，这些数据来与 Dataframe 中的 DSL 语句，还未跟实际的数据格式对应起来。

Catalyst 分两步走：

**逻辑计划部分**：

1. **逻辑计划解析**：结合 DataFrame 中的 Schema 信息，确认计划中的表名、字段名、字段类型是否与实际一致，经过这个步骤转换后，`Unresolved Logical Plan` 会转为 `Analyzed Logical Plan`。
2. **逻辑计划优化**，基于解析过后的 `Analyzed Logical Plan`，Catalyst 会利用**启发式的规则**和**执行策略**，最终把逻辑计划转换为可执行的物理计划  `Optimized Logical Plan`。

```scala
== Parsed Logical Plan ==
Relation[age#7L,name#8] json

== Analyzed Logical Plan ==
age: bigint, name: string
Relation[age#7L,name#8] json

== Optimized Logical Plan ==
Relation[age#7L,name#8] json

== Physical Plan ==
FileScan json [age#7L,name#8] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex[file:/usr/local/src/spark-3.1.1-bin-hadoop3.2/examples/src/main/resources/peopl..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<age:bigint,name:string>
```

![](https://static001.geekbang.org/resource/image/f3/72/f3ffb5fc43ae3c9bca44c1f4f8b7e872.jpg)

**物理计划部分**：

1. **优化 Spark Plan**：在优化 Spark Plan 的过程中，Catalyst **基于既定的优化策略**（Strategies），把逻辑计划中的关系操作符一一映射成物理操作符，生成 Spark Plan。
2. **生成 Physical Plan**：在生成 Physical Plan 过程中，Catalyst 再**基于事先定义的 Preparation Rules**，对 Spark Plan 做进一步的完善、生成可执行的 Physical Plan。

可以看出，Catalyst 的优化操作来源于 Dataframe 的开发方式。

对于同样一种计算逻辑，实现方式可以有多种，按照不同的顺序对算子做排列组合，我们就可以演化出不同的实现方式。最好的方式是，我们遵循“能省则省、能拖则拖”的开发原则，去选择所有实现方式中最优的那个。

#### 2.1.1 逻辑计划

Catalyst 在 Spark 3.0 版本中，共有 81 条优化规则（Rules），分为 27 组（Batches），不考虑规则的重复性，27 组共有 129 个优化规则，但主要可以分为三大类：

- **谓词下推**，结合列式存储格式的文件注脚（Footer）中的统计信息，能够大幅度减少扫描数据量，降低 I/O 开销
  - 将过滤规则提前到接近数据源的地方，从源头减少扫描量
  - 对一些谓词做优化，如 `in` 替换为 `=`，多个谓词进行合并等。
- **动态裁剪**，扫描数据源的时候，只读取与查询相关的字段，节省 I/O 开销
- **常量替换**，将查询条件中的一些计算替换为常量

Catalyst 除了会基于优化规则进行优化外，还会借助 Cache Manager 中记录的信息减少计算成本。

Cache Manager 维护了一个 Mapping 映射字段，字典的 Key 值是逻辑计划，Value 是对应的 Cache 元信息。

Catalyst 在对逻辑计划进行优化前，会先从 Cache Manager 中查找当前的逻辑计划或者分支是否存在已被记录，如果可以获取到相应信息，则会采用 **InMemoryRelation** 节点替换整个计划或者计划的一部分，充分利用缓存。

```scala
== Physical Plan ==
CollectLimit (3)
+- * Project (2)
   +- InMemoryTableScan (1)
         +- InMemoryRelation (2)
               +- Scan json  (3)
```

Catalyst 逻辑计划优化的类主要是借助 **QueryPlan** 以及 **TreeNode** 实现的，其中 QueryPlan 是 TreeNode 的子类，TreeNode 中有一个叫做 **children** 的字段，类型为 **Seq[TreeNode]**，从而构成了一个树结构，接着借助 **transformDown** 递归函数不断将各个优化规则作用（Apply）于当前节点，之后再作用到 children 中的子节点，直到整棵树的叶子节点。

从 Analyzed Logical Plan 到 Optimized Logical Plan 的转化，其实就是从一个 TreeNode 生成另一个 TreeNode 的过程，不断将各组优化规则作用到整棵树，且结构不再变化为止。

以 `org.apache.spark.sql.catalyst.expressions.Expression` 为例：

```scala
//Expression的转换
import org.apache.spark.sql.catalyst.expressions._
val myExpr: Expression = Multiply(Subtract(Literal(6), Literal(4)), Subtract(Literal(1), Literal(9)))
val transformed: Expression = myExpr transformDown {
  case BinaryOperator(l, r) => Add(l, r)
  case IntegerLiteral(i) if i > 5 => Literal(1)
  case IntegerLiteral(i) if i < 5 => Literal(0)
}
```

最后将 （（6 - 4）*（1 - 9）） 通过自定义规则转化为了 （（1 + 0）+（0 + 1））。

#### 2.1.2 物理计划

在逻辑计划优化完成后，物理计划阶段会以优化后的逻辑计划为起点开始生成。

首先在优化 Spark Plan 过程中，Catalyst 共有 14 类优化策略：

![](https://static001.geekbang.org/resource/image/51/56/51ca111dfb9ebd60e2443c86e9b0cb56.jpeg)

同逻辑计划优化类似，优化 Spark Plan 阶段也是基于**偏函数**模式，把逻辑计划中的操作符映射为 Spark Plan 中的物理算子，例如 BasicOperators 策略直接把 Project、Filter、Sort 等逻辑操作符平行地映射为物理操作符。BasicOperators 中包含了一个 Expression CodeGen 优化，各个具体实现类都实现了 `org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate` 特质，在出现 WSCG 后，只会生成部分代码，与其他代码组成 WSCG。

优化后的逻辑计划，还并未选择具体的 Join 策略，此处以场景使用最广泛，对执行性能影响大最大的 **JoinSelection** 为例，Catalyst 共有以下几种 Join 策略：

![](https://static001.geekbang.org/resource/image/39/fb/39642808b292abb0b5b37ea69bfb19fb.jpeg)

本质上就是来自 2 种数据分发方式（Boradcast 和 Shuffle）与 3 种 Join 机制实现的组合：

![](https://static001.geekbang.org/resource/image/e9/48/e9bf1720ac13289a9e49e0f33a334548.jpg)

Catalyst 会按照性能从高到低依次选择出符合场景的策略，通过两大类条件做决策：

1. 条件性信息：
   - 按照 Join 类型（是否等值、连接形式等），来源于查询语句自身
   - 内表尺寸，可以来自于 Hive 的 ANALYZE YZE TABLE 语句、Spark 对于 Parquet、ORC 等文件尺寸的预估，甚至是 AQE 的动态统计信息
2. 指令型信息，也就是 Join Hints

在这之后，Spark Plan 就可以决定使用哪种 Join 策略了，接着 Catalyst 需要对 Spark Plan 作进一步转换为 Physical Plan：

![](https://static001.geekbang.org/resource/image/53/fd/534dd788609386c14d9e977866301dfd.jpg)

Spark Plan 转化为 Physical Plan 需要经过几组 Preparation Rules 的规则，对 Spark Plan 进行细节的补充，这几组规则如下：

![](https://static001.geekbang.org/resource/image/18/f7/187a85d53d585c5b3656353e3304fdf7.jpeg)

以 EnsureRequirements 为例，该规则会对执行计划中的每一个操作节点，使用 4 个属性描述数据输入和数据输出的分布状态：

![](https://static001.geekbang.org/resource/image/f8/yf/f8cae1364372a2a8c034a5ab00850yyf.jpeg)

该规则要求子节点的输出数据要满足父节点的输入要求，假设父节点为 SortMergeJoin：

![](https://static001.geekbang.org/resource/image/05/00/05467eecb3c983d4fc4a3db8a0e7e600.jpg)

此时 Project 节点的输出数据，并没有满足 SortMergeJoin 的要求（数据按照 partition 分区及排序），因此 EnsureRequirements 会介入添加必要的操作符，以满足父节点对子节点数据的要求：

![](https://static001.geekbang.org/resource/image/a8/15/a8c45d1d6ecb6a120205252e21b1b715.jpg)

其中 Exchange 表示 Shuffle 操作，Sort 表示排序，满足了 SortMergeJoin 的要求。

之后 Spark 通过调用 Physical Plan 的 doExecute 方法，把结构化查询的计算结果，转换成 **RDD[InternalRow]**（InternalRow 为 Tungsten 定制化的二进制数据结构）。

以下图的 Physical Plan 为例：

![](https://static001.geekbang.org/resource/image/65/33/656e29b2d25549488087fc1a4af8cd33.png)

可以看到物理计划添加了 Exchange、Sort 节点，Join 策略为 SortMergeJoin，* 表示 Tungsten 的 WSCG，括号里的数字表示 Stage 编号，括号中相同数字的操作，都会被捏合成一份 “手写代码”，由 Tungsten 的 WSCG 完成。

### 2.2 Tungsten

在 Catalyst 生成物理计划后，还会交给 Tungsten 继续进行优化，Tungsten 主要从数据存储以及全阶段代码生成两个方面进行优化。

#### 2.2.1 数据存储

Tungsten 在数据存储方面，有两个较大的改进，分别是**紧凑的数据格式 Unsafe Row** 以及**内存页管理**。

Tungsten 自定义了 Unsafe Row 数据格式，可以以十分高效的方式存储二进制数据，对比 Java 的数据结构实现，**可以节省大量的内存空间以及对象数量**，大大提高了存储效率以及 GC 效率。

Tungsten 依赖于 DataFrame 携带的 Schema 信息，根据 offset 以及 length，高效存储数据：

![](https://static001.geekbang.org/resource/image/20/02/20230c764200cfde05dedec1cae6b702.jpg)

- 对于定长的字段，直接安插到字节数组中
- 对于变长的字段，则会先在 Schema 的相应位置插入偏移地址，再把字段长度以及字段值存储到字节数组后面

也就是说，Tungsten 需要知道每一个字段的数据类型及长度，才能做出高效率的存储。

> NOTE：对于复杂类型，Tungsten 则很难入手优化，这也是 Spark SQL 对比 Spark RDD 高效的原因之一。

对比原来 JVM 存储多个字段值使用 GenericMutableRow 封装一条数据，Array 存储实际的值的方式，JVM 采取的方式主要存在两个问题：

1. **存储开销大**，Java 对象存储字段值，往往会有一定的膨胀，如对象头、指针、哈希码等信息。
2. **对象数量多**，为了存储字段值，需要多个对象进行辅助，而 Tungsten 则可以一个字节数组存储多个字段值。

![](https://static001.geekbang.org/resource/image/fd/69/fd00cf1364c800659a7d492cd25c6569.jpg)

由此看来，**Tungsten 字节数组的存储方式在消除存储开销的同时，仅用一个数组对象就能轻松完成一条数据的封装，显著降低 GC 压力**。

Tungsten 还采用**基于内存页的内存管理来定位数据**，对比起传统的 Java 标准库 HashMap 来看，降低了存储开销和 GC 负担，同时还能提高 CPU 命中率和访问效率。

为了统一管理 Off Heap 和 On Heap 内存空间，Tungsten 定义了统一的 128 位内存地址，简称 Tungsten 地址。Tungsten 地址分为两部分：前 64 位预留给 Java Object，后 64 位是偏移地址 Offset，并且 Off Heap 和 On Heap 的寻址方式也不同。

![](https://static001.geekbang.org/resource/image/90/47/904dc1d1846dddffe363e834ce892347.jpg)

- Off Heap：Spark 通过 Java Unsafe API 直接管理操作系统内存，不存在 Java Object 的概念，因此前 64 位为 null，后 64 位则用于在堆外空间中直接寻址操作系统的内存空间，直接定位数据。
- On Heap：Spark 借助一个叫**页表（Page Table）**的**数组结构**来定位数据，页表记录了一个又一个内存页（Memory Page），内存页的前 64 位记录了 Object 在 JVM 中的地址，后 64 位记录了数据在 Object 中的偏移量。Spark 定位数据的过程如下：
  1. 通过页表获取 Object 所在位置
  2. 通过页表获取数据在该 Object 里的偏移量

页表管理的方式使用 Java 中的 HashMap 也可以实现，但存在两个问题：

1. **存储开销和 GC 负担大**，HashMap 中存储真正数据的对象可能只有一半，其余皆为指针、长度等信息。
2. **CPU 缓存命中率低**，HashMap 是以数组 + 链表的方式实现的，链表可以利用零散的内存区域，提升内存利用效率，但在进行扫描的时候，这种零散的存储方式会引入大量的随机内存访问，降低 CPU 缓存命中率。

![](https://static001.geekbang.org/resource/image/1b/84/1bc7f9553dfe7yyb51a641f51093c284.jpg)

因此 Tungsten 采用数组 + 内存页的方式实现了 HashMap 对应的功能，内存页存储的则是 Object 引用以及数据偏移量，并且数组存储内存页的方式还可以充分利用 CPU 缓存，提高命中率。

#### 2.2.2 页表实现

Tungsten 管理自定义存储结构的实现在 TaskMemoryManager 中，该类使用了页表的思想对内存块进行了统一管理：

![](https://gitee.com/zhxuankun/Image/raw/5aa6cb7f4ef089ce18aae7493031dc2e34d10452/blog/image-20210508142411733.png)

实际存储数据的是一个内存页数组 `pageTable`，其类型为 `MemoryBlock[]`，一个内存页 `MemoryBlock` 可以存储多条数据。

外部需要寻址时需要提供一个 64 bit 的地址，其高 13 bit 用于定位内存页下标，低 51 bit 为数据在内存块中的 offset。

BytesToByteMap 就是使用了页表的一个例子，key 是一个用于寻址的 64 bit 地址, value 为哈希码，该 map 使用了数组结构，同时又能做 Hash 寻址，遇到冲突时采用**开放定址法**解决。

通过 Hash 取得内存页地址后，交给 TaskMemoryManager 就能获取到对应内存页。

#### 2.2.3 WSCG

**全阶段代码生成（WSCG，Whole Stage Code Generation），指的是基于同一 Stage 内操作符之间的调用关系，生成一份“手写代码”，真正把所有计算融合为一个统一的函数**。

相比 DAGScheduler 对同一 Stage 内函数的合并，DAGScheduler 只是简单地对函数进行了嵌套调用，但这样一来会**涉及多次虚函数调用以及内存的随机访问**，降低 CPU 的缓存命中率：

![](https://static001.geekbang.org/resource/image/03/03/03052d8fc98dcf1740ec4a7c29234403.jpg)

在有 WSCG 之前，Spark 只能使用**火山迭代模型（Volcano Iteration Model，简称 VI）**融合函数，VI 模型依托于 AST 语法树，对所有计算统一进行了封装，所有操作符都需要实现 VI 模型的迭代器抽象（如 hasNext、next 方法），之后只要任何一个算子实现了该抽象，即可加入到语法树种参与计算，灵活组合。

VI 模型中，语法树每个操作符都需要完成如下步骤：

1. 从内存中读取父操作符的输出结果作为输入数据
2. 调用 hasNext、next 方法，以操作符逻辑处理数据，如过滤、投影、聚合等等
3. 将处理后的结果以统一的标准形式输出到内存，供下游算子消费

任意两个操作符之间的交互都会涉及第 1、 2 点，即内存的随机存取和虚函数调用，导致 CPU 利用率低下。

WSCG 正是为了消除函数嵌套调用存在的，通过生成手写代码的方式完成：

![](https://static001.geekbang.org/resource/image/53/e7/5389b8bd80748dcc706b1c3c95ddbce7.jpg)

计算逻辑会一次性的应用到数据上，每条指令也是明确的，可以顺序加载到 CPU 寄存器上。

Catalyst 生成 Physical Plan 之前，有一条 Preparation Rules，其中一条 **CollapseCodegenStages** 就是将其交给 Tungsten 生成手写代码，分为两个步骤：

1. **从父节点到子节点，递归调用 doProduce，生成代码框架**
2. **从子节点到父节点，递归调用 doConsume，向框架填充每一个操作符的运算逻辑**

![](https://static001.geekbang.org/resource/image/68/2d/68cfc6aec121511303ccec179bd4a32d.jpg)

Tungsten 在 Stage 顶端节点添加 **WholeStageCodeGen** 节点，WholeStageCodeGen 节点调用 doExecute 触发代码生成过程：

1. doExecute 递归调用子节点的 doProduce 函数，直到遇到 Stage 边界，并生成手写代码的框架（图中蓝色代码部分）
2. 接下来 doProduce 函数会反向递归调用 doConsume 函数，将关系表达式转化为 Java 代码

Tungsten 利用 CollapseCodegenStages 规则，通过两层递归调用，把 Spark Plan 加工成了一份 “手写代码”，并将其交付给 DAGScheduler 执行。

WSCG 解决了两个核心痛点：

1. 操作符之间频繁的虚函数调用
2. 操作符之间数据交换引入的内存随机访问

## 3. 总结

Spark SQL  能够基于 DataFrame 简单的标量算子和明确的 Schema 定义，借助 Catalyst 优化器和 Tungsten，有能力在运行时构建起一套端到端的优化机制。这套机制运用启发式的规则与策略，以及运行时的执行信息，将原本次优、甚至是低效的查询计划转换为高效的执行计划，再从字节码层面生成代码，消除函数与函数之间的开销，从而提升端到端的执行性能。

https://www.waitingforcode.com/apache-spark-sql/why-code-generation-apache-spark-sql/read

