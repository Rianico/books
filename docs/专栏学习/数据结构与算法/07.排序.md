---
title: 07-排序
date: 2020-07-22 14:48:49
---
# 排序

排序算法有很多种，但其中最经典的、最常用的：冒泡排序、插入排序、选择排序、归并排序、快速排序、计数排序、基数排序、桶排序。

接下来主要按照时间复杂度把它们分成了三大节讲解。

| 排序算法 | 时间复杂度 | 是否基于比较 |
| :--- | :---: | :---: |
| 冒泡、插入、选择 | $O(n^2)$ | √ |
| 快速、归并 | $O(nlogn)$ | √ |
| 桶、计数、基数 | $O(n)$ | × |

Q：如何分析一个算法？

首先是得分析算法的效率：

1. 最好、最坏、平均时间复杂度，我们要知道算法在各个数据规模（大小、有序度）下的表现。
2. 时间复杂度的系数、常数、低阶，在**数据规模很小**的时候，这些指标往往也无法忽视。
3. 比较次数和交换（移动）次数，基于比较的算法中，会设计两种操作：元素的比较以及交换（移动），这个也是考察算法效率的一个影响因素。

其次再看排序算法的内存消耗，算法消耗的内存可以通过空间复杂度来计算。有个概念叫做**原地排序（Sorted in place）**，就是特指空间复杂度为 $O(1)$ 的排序算法，冒泡、插入、选择算法都属于此类。

## 1. 冒泡、插入、选择排序

### 1.1 冒泡排序（Buble Sort）

冒泡排序每次只会比较相邻两个位置上的元素，一旦不满足关系大小要求，就会交换相邻位置元素。每次冒泡排序会至少让一个（最大或者最小的）元素移动到它应该在的位置，如此重复n次，就能够让n个元素的序列达到有序状态。

![&#x5192;&#x6CE1;&#x6392;&#x5E8F;](https://static001.geekbang.org/resource/image/92/09/9246f12cca22e5d872cbfce302ef4d09.jpg)

实际上，冒泡排序还可以进一步优化，若某次比较中发现没有数据的交换，那么也可以判断这段序列已经有序，可以直接结束后面的比较了：

![&#x4F18;&#x5316;-&#x5192;&#x6CE1;&#x6392;&#x5E8F;](https://static001.geekbang.org/resource/image/a9/e6/a9783a3b13c11a5e064c5306c261e8e6.jpg)

代码实现：

```java
// 冒泡排序，a表示数组，n表示数组大小
public void bubbleSort(int[] a, int n) {
  if (n <= 1) return;

 for (int i = 0; i < n; ++i) {
    // 提前退出冒泡循环的标志位
    boolean flag = false;
    for (int j = 0; j < n - i - 1; ++j) {
      if (a[j] > a[j+1]) { // 交换
        int tmp = a[j];
        a[j] = a[j+1];
        a[j+1] = tmp;
        flag = true;  // 表示有数据交换
      }
    }
    if (!flag) break;  // 没有数据交换，提前退出
  }
}
```

冒泡的过程只涉及相邻数据的交换操作，只需要常量级的临时空间，所以它的空间复杂度为 O\(1\)，是一个原地排序算法。

在冒泡排序中，只有交换才可以改变两个元素的前后顺序。为了保证冒泡排序算法的稳定性，当有相邻的两个元素大小相等的时候，我们不做交换，相同大小的数据在排序前后不会改变顺序，所以冒泡排序是稳定的排序算法。

最好情况下，要排序的数据已经是有序的了，我们只需要进行一次冒泡操作，就可以结束了，所以最好情况时间复杂度是 $O(n)$ 。而最坏的情况是，要排序的数据刚好是倒序排列的，我们需要进行 n 次冒泡操作，所以最坏情况时间复杂度为 $O(n^2)$。

至于平均复杂度，由于需要算各种组合出现的概率 (n! 种组合)，设计概率论较为复杂，因此使用另一种方式来进行估算：“**有序度**”、“**逆序度**”、以及“**满序度**：。

**有序度**：序列中满足有序关系的元素对的个数，如 `a[i] <= a[j]`, 如果 i < j。

逆序度：有序的反面。

![&#x6709;&#x5E8F;&#x5EA6;](https://static001.geekbang.org/resource/image/a1/20/a1ef4cc1999d6bd0af08d8417ee55220.jpg)

对于一个完全有序的序列，我们把这种完全有序的数组的有序度叫作满有序度。满序度：$n*(n-1)/2$ 。

我们对序列排序进行排序，其实就是从一个初始有序度出发，不断减少逆序度，从而达到一个满序度的状态，所以有这样一个规律：逆序度=满序度-有序度。

![&#x6EE1;&#x5E8F;&#x5EA6;](https://static001.geekbang.org/resource/image/88/34/8890cbf63ea80455ce82490a23361134.jpg)

最好时间复杂度对应的逆序度为0，最坏时间复杂度的逆序度为n\*\(n-1\)/2，将两者相加平均下可以粗略的得到平均时间复杂度为$O\(n^2\)$。

### 1.2 插入排序

插入排序，就是将一个序列分为有序（通常取第一个元素）、无序两个区间，每次都从无序区间取一个值到有序区间中进行比较，知道找合适的位置进行插入，每次比较都要移动元素位置，以便腾出空间给元素进行插入：

![&#x63D2;&#x5165;&#x6392;&#x5E8F;](https://static001.geekbang.org/resource/image/b6/e1/b60f61ec487358ac037bf2b6974d2de1.jpg)

代码实现：

```java
// 插入排序，a表示数组，n表示数组大小
public void insertionSort(int[] a, int n) {
  if (n <= 1) return;

  for (int i = 1; i < n; ++i) {
    int value = a[i];
    int j = i - 1;
    // 查找插入的位置
    for (; j >= 0; --j) {
      if (a[j] > value) {
        a[j+1] = a[j];  // 数据移动
      } else {
        break;
      }
    }
    a[j+1] = value; // 插入数据
  }
}
```

插入排序算法的空间复杂度与时间复杂度跟冒泡排序一样。

在插入排序中，对于值相同的元素，我们可以选择将后面出现的元素，插入到前面出现元素的后面，这样就可以保持原有的前后顺序不变，所以插入排序是稳定的排序算法。

### 1.3 选择排序

选择排序与插入排序类似，也是分为有序、无序两个区间，只不过选择排序是每次都从无序区间找出最小（最大）的那个值，跟有序区间末尾进行位置交换：

![&#x9009;&#x62E9;&#x6392;&#x5E8F;](https://static001.geekbang.org/resource/image/32/1d/32371475a0b08f0db9861d102474181d.jpg)

选择排序算法的空间复杂度与时间复杂度跟插入排序一样。

选择排序是一种不稳定的排序算法。从上面画的那张图中，可以看出，选择排序每次都要找剩余未排序元素中的最小值，并和前面的元素交换位置，这样破坏了稳定性。

代码实现：

```scala
val arr = Array(4,5,6,3,2,1)

def selectionSort(arr: Array[Int]) = {
  if(arr.length <= 1) arr.foreach(println)

  for(i <- 0 until arr.length-1) {
    var value = arr(i)
    for(j <- i+1 until arr.length) {
      if(arr(j) < value) {
        arr(i) = arr(j)
        arr(j) = value
        value = arr(i)
      }
    }

  }
  arr.foreach(println)
}

selectionSort(arr)
```

### 1.4 思考

**Q**：冒泡排序和插入排序的时间复杂度都是 $O(n^2)$Z，都是原地排序算法，为什么插入排序要比冒泡排序更受欢迎呢？

> 虽然冒泡排序跟插入排序在时间复杂度以及空间复杂度上都一样，但是看一下代码：
>
> ```java
> // 冒泡排序中数据的交换操作：
> if (a[j] > a[j+1]) { // 交换
>    int tmp = a[j];
>    a[j] = a[j+1];
>    a[j+1] = tmp;
>    flag = true;
> }
>
> // 插入排序中数据的移动操作：
> if (a[j] > value) {
>   a[j+1] = a[j];  // 数据移动
> } else {
>   break;
> }
> ```
>
> 假设一行代码为一个unit\_time，那么很明显冒泡排序花费的时间会更多，当然这只是个大致的猜测，实际测试中随机生成 10000 个数组，每个数组中包含 200 个数据，然后在我的机器上分别用冒泡和插入排序算法来排序，冒泡排序算法大约 700ms 才能执行完成，而插入排序只需要 100ms 左右就能搞定！
>
> 插入排序实际上还能进一步优化， 详见[希尔排序](https://zh.wikipedia.org/wiki/希尔排序)。

![&#x4E09;&#x79CD;&#x6392;&#x5E8F;&#x7B97;&#x6CD5;&#x6BD4;&#x8F83;](https://static001.geekbang.org/resource/image/34/50/348604caaf0a1b1d7fee0512822f0e50.jpg)

## 2. 快速、归并排序

冒泡、插入、选择排序时间复杂度都为 $O(n^2)$，并不属于太常用的算法，在大规模数据中，快速、归并排序更为常用，且时间复杂度都为$O(nlogn)$。

### 2.1 归并排序（Merge Sort）

归并排序采用了**分治思想**，每次从中间将数组拆分为两个子数组，然后再对子数组进行排序，最后再排序好的数组重新合并，这样就得到了有序序列。

![&#x5F52;&#x5E76;&#x6392;&#x5E8F;](https://static001.geekbang.org/resource/image/db/2b/db7f892d3355ef74da9cd64aa926dc2b.jpg)

分治思想，顾名思义就是将一个大问题拆成子问题，一直拆分然后逐个解决，这个思维方式可以很好的跟递归结合起来。

使用递归方式编写归并排序的时候，我们可以先列出递推公式以及边界条件：

```vim
递推公式：
merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))

终止条件：
p >= r 不用再继续分解
```

**merge\_sort**表示给下标p到r之间的序列进行排序，我们将其拆分为了p到q，q+1到r两个子问题，其中q为p跟r的中间\(p+r\)/2，当p到q，q+1到r都为有序的时候，再将其**合并排序**到一起，这样下标p到r之间的排序也就好了。

上面提到的合并排序的具体方法，需要额外申请一个同样大小的新数组，并分别使用两个指针i，j指向两个子数组的开头，一旦哪一个指针符合我们指定的大小关系，我们就将其放入新数组，直到两个子数组都放入为止。

![&#x5408;&#x5E76;&#x6392;&#x5E8F;](https://static001.geekbang.org/resource/image/95/2f/95897ade4f7ad5d10af057b1d144a22f.jpg)

实现代码如下：

```scala
val arr = Array(4,6,3,5,6,3,2,6,3,1,1)

def merge(origin: Array[Int], start: Int, end: Int): Unit = {

  if(end - start < 1) return

  val medium = (start + end) / 2
  merge(origin, start, medium)
  merge(origin, medium+1, end)
  // 合并排序
  mergeSort(origin, start, medium+1, end)

}

def mergeSort(origin: Array[Int], start: Int, medium: Int, end: Int): Unit = {

  val tmp = new Array[Int](end - start + 1)
  var idx = 0
  var i = start
  var j = medium

  while(i < medium && j <= end) {
    if(origin(i) <= origin(j)) {
      tmp(idx) = origin(i)
      i = i+1
    } else {
      tmp(idx) = origin(j)
      j = j+1
    }
    idx = idx + 1
  }

  if(i < medium) {
    (i until medium).foreach { _idx =>
      tmp(idx) = origin(_idx)
      idx = idx + 1
    }
  } else {
    (j to end).foreach { _idx =>
      tmp(idx) = origin(_idx)
      idx = idx + 1
    }
  }

  idx = 0
  (start to end).foreach { _idx =>
    origin(_idx) = tmp(idx)
    idx = idx + 1
  }

}

merge(arr,0,arr.size - 1)
println(arr.mkString(","))
```

**Q**：归并排序是稳定的算法吗？

归并排序稳不稳定，主要看合并子数组的时候，遇到相同的值如何处理，通常是优先将下标较小的元素插入临时数组，从而达到稳定排序的目的，所以归并排序可以说是稳定的排序算法。

**Q**：归并排序的时间复杂度是多少？

归并排序涉及递归，因此分析其时间复杂度，我们也可以根据最开始的时间复杂度一个一个拆分来求得。**不仅递归求解的问题可以写成递推公式，递归代码的时间复杂度也可以写成递推公式**。

假设问题a的时间复杂度为T\(a\)，可以拆分为子问题b和c，对应时间复杂度分别为T\(b\)，T\(c\)，那么我们就可以获得如下递推公式：

$$
T(a) = T(b) + T(c) + K
$$

其中 K 表示将 b 跟 c 合并为 a 所花费的时间。

我们解决一个问题的时间复杂度为 T(n\)，那么两个子问题就是 2T(n/2)，我们知道，merge的时间复杂度为O\(n\)，所以套用前面的公式得到：

$$
\begin{align}
T(1) &= C\\
T(n) &= 2*T(n/2) + n； n>1
\end{align}
$$

当n等于1时，只需要常量级执行时间，因此设为C。

接下来继续分解：

$$
\begin{align}
T(n) &= 2*T(n/2) + n\\
     &= 2*(2*T(n/4) + n/2) + n = 4*T(n/4) + 2*n\\
     &= 4*(2*T(n/8) + n/4) + 2*n = 8*T(n/8) + 3*n\\
     &= 8*(2*T(n/16) + n/8) + 3*n = 16*T(n/16) + 4*n\\
     &= ......\\
     &= 2^k * T(n/2^k) + k * n\\
\end{align}
$$

当$T(n/2^k)=T(1)$时，也就是 $n/2^k=1$，我们得到 $k=log_{2}n$。我们将 k 值代入上面的公式，得到 $T(n)=Cn+nlog_{2}n$。因此时间复杂度为$O(nlogn)$。

**Q**：归并排序的时间复杂度是多少？

归并排序的时间复杂度任何情况下都是 O(nlogn)，看起来非常优秀。即便是快速排序，最坏情况下，时间复杂度也是 $O(n^2)$。但是，归并排序并没有像快排那样，应用广泛，因为是归并排序不是原地排序算法。

**Q**：归并排序的空间复杂是多少？

归并排序每次都需要新建一个额外的数组来存出结果，因此空间复杂度为O\(n\)。

### 2.2 快速排序（Quick Sort）

快速排序使用的也是分治思想，通常采用递归实现，看起来跟归并很类似，但核心点不一样。快速排序是一个原地算法，且不需要额外的数组空间存储临时结果，如下伪代码：

```vim
递推公式：
quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)

终止条件：
p >= r


// 快速排序，A是数组，n表示数组的大小
quick_sort(A, n) {
  quick_sort_c(A, 0, n-1)
}
// 快速排序递归函数，p,r为下标
quick_sort_c(A, p, r) {
  if p >= r then return

  q = partition(A, p, r) // 获取分区点
  quick_sort_c(A, p, q-1)
  quick_sort_c(A, q+1, r)
}
// 分区函数
partition(A, p, r) {
  pivot := A[r]
  i := p
  for j := p to r-1 do {
    if A[j] < pivot {
      swap A[i] with A[j]
      i := i+1
    }
  }
  swap A[i] with A[r]
  return i
}
```

快速排序最开始也是像归并排序一样拆分为一个个子区间，快速排序每次都会从区间中选择一个数作为pivot，小于pivot的放一边，大于pivot的放到另一边，当子区间的大小为1时就已经是有序的了。由于快速排序是一个原地函数，因此分区函数里面需要一点技巧，用到了类似于前面选择排序的技巧。

由于快速排序用到了交换，因此快速排序是一种不稳定的算法。

![&#x5206;&#x533A;&#x51FD;&#x6570;](https://static001.geekbang.org/resource/image/08/e7/086002d67995e4769473b3f50dd96de7.jpg)

通常来讲，快速排序找到一个数将序列均衡的分为两个长度类似的区间是很困难的，两边往往是不均衡的。

在极端情况下，比如 1，3，5，6，8。如果我们每次选择最后一个元素作为 pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约 n 次分区操作，才能完成快排的整个过程。每次分区我们平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O\(nlogn\) 退化成了 O\(n2\)。

快速排序的时间复杂度求起来非常复杂，除了递推公式之外，还有递归树，后面会再提。这里只先记住结论：快速排序的时间复杂度T\(n\) 在大部分情况下的时间复杂度都可以做到 O\(nlogn\)，只有在极端情况下，才会退化到 O\(n2\)，并且也有多种手段将这个概率降到很低。

### 2.3 两者对比

![&#x5F52;&#x5E76; VS &#x5FEB;&#x901F;](https://static001.geekbang.org/resource/image/aa/05/aa03ae570dace416127c9ccf9db8ac05.jpg)

从图中可以很容易的看出，虽然两者采用的都是分治思想，但归并是一种自下而上的排序，而快速是一种自上而下的排序，并且快速排序是一种原地函数，解决了归并排序占用内存过多的问题。

### 2.4 思考

**Q**：如何在 O\(n\) 的时间复杂度内查找一个无序数组中的第 K 大元素？

![](https://static001.geekbang.org/resource/image/89/91/898d94fc32e0a795fd65897293b98791.jpg)

我们选择最后一个数作为pivot进行快速排序，这样一来，序列arr\[0...r\]就被拆分成了arr\[0...p-1\], arr\[p\], arr\[p+1,r\]，若p+1=K，那么p就是我们正好在找的值，若p+1&lt;K，那么接下来我们就去arr\[p+1,r\]中找，p+1&lt;K则到arr\[0...p\]中找。

至于时间复杂度，第一次我们需要扫描n个，第二次n/2，那么总的时间就是$n+n/2+n/4+n/8+…+1 = 2n -1$\(等比求和\)。所以时间复杂度为O\(n\)。

**Q**：现在你有 10 个接口访问日志文件，每个日志文件大小约 300MB，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这 10 个较小的日志文件，合并为 1 个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有 1GB，你有什么好的解决思路，能“快速”地将这 10 个日志文件合并吗？

> 由于日志本身已经有序，因此只有同时开10个I/O流从头开始读取，每次都读取一行，每次共有10行日志，将其进行对比，将时间最早的插入新的日志文件，并让其对应的I/O流读取下一行，直到全部日志读取完成。

