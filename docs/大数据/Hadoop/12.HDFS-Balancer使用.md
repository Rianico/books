---
title: 12.HDFS-Balancer 使用
date: 2021-06-24
---

## HDFS Balancer 使用

首先检查 Balancer 的状态，以及限制每个节点的带宽，避免打满网络及占满磁盘 I/O：

```bash
# 如果 balancer 启动失败，则表示有正在运行的 balancer 或者之前的 balancer 运行失败了，确定清楚再决定是否删除
hdfs dfs -rmr /system/balancer.id

# 设置 Balancer 内存
export HADOOP_HEAPSIZE=12288

# 设置每个 datanode 用于 Balancer 的带宽，此处取 256MB/s
hdfs dfsadmin -fs hdfs://nzj-cluster-gdyd -setBalancerBandwidth 262144000
```

执行以下命令进行重平衡：

```bash
nohup hdfs balancer -Ddfs.balancer.moverThreads=4000 \
-Ddfs.datanode.balance.max.concurrent.moves=48 \
-Ddfs.balancer.max-size-to-move=53687091200 -threshold 10 &
```

各参数含义如下：

- **dfs.balancer.moverThreads**：用于**整个集群**再平衡的线程数，每个 block 的移动都对应一个线程，默认值 2000

- **dfs.datanode.balance.max.concurrent.moves**：每个 Datanode 可以并发迁移的 Block，取 Datanode 与 Balancer 的最小值。建议 Datanode 配的非常高，Balancer 指定为磁盘数量的整数倍（建议 4x），默认 5。

  > NOTE： 这个参数设置为磁盘整数倍有个好处，就是在带宽固定的前提下，越多 Block 并发传输，可以越细粒度的将压力分摊到多个磁盘上。

- **dfs.balancer.max-size-to-move**：每轮平衡的最大数据量，每轮平衡都会选择出一组节点，当网络与磁盘没有那么紧张的时候，可以适当增大该值。

- **dfs.datanode.balance.bandwidthPerSec=10737418240** ：设置 Balancer 带宽，上述命令中并未设置，而是通过 `setBalancerBandwidth` 命令在 DataNode 级别限速。

启动后日志如下：

```bash
21/06/24 16:49:34 INFO balancer.Balancer: 0 over-utilized: []
21/06/24 16:49:34 INFO balancer.Balancer: 2 underutilized: [173.31.1.87:1004:DISK, 173.31.2.74:1004:DISK]
21/06/24 16:49:34 INFO balancer.Balancer: Need to move 27.44 TB to make the cluster balanced.
21/06/24 16:49:34 INFO balancer.Balancer: chooseStorageGroups for SAME_RACK: overUtilized => underUtilized
21/06/24 16:49:34 INFO balancer.Balancer: chooseStorageGroups for SAME_RACK: overUtilized => belowAvgUtilized
21/06/24 16:49:34 INFO balancer.Balancer: chooseStorageGroups for SAME_RACK: underUtilized => aboveAvgUtilized
21/06/24 16:49:34 INFO balancer.Balancer: chooseStorageGroups for ANY_OTHER: overUtilized => underUtilized
21/06/24 16:49:34 INFO balancer.Balancer: chooseStorageGroups for ANY_OTHER: overUtilized => belowAvgUtilized
21/06/24 16:49:34 INFO balancer.Balancer: chooseStorageGroups for ANY_OTHER: underUtilized => aboveAvgUtilized
21/06/24 16:49:34 INFO balancer.Balancer: Decided to move 50 GB bytes from 173.31.3.110:1004:DISK to 173.31.1.87:1004:DISK
21/06/24 16:49:34 INFO balancer.Balancer: Decided to move 50 GB bytes from 173.31.3.29:1004:DISK to 173.31.2.74:1004:DISK
21/06/24 16:49:34 INFO balancer.Balancer: Will move 100 GB in this iteration
```

## 节点间不平衡的原因

导致 HDFS 节点间数据不平衡的原因较多，通常有以下几种原因：

- 新增了 DataNode 节点，已有的 Block 块并不会随之迁移到新增节点上
- 客户端程序的写入行为，有些客户端写入时会倾向于写入特定机器（如 HBase），并且 DataNode 默认也是i采用一个 rack 存储 1 个副本，另一个机架存储 2 个副本的策略
- Block 分配策略，HDFS 会从满足写入要求的节点中随机挑选节点进行数据分发，当随机分发不等于数据分布均匀，这是由于随机性挑选会存在短暂的不平衡，并且也没有考虑到 Block 大小。这种策略大部分情况下不存在问题，但在集群容量快耗尽的时候则会开始暴露出来。

## 磁盘间不平衡的原因

磁盘之间数据平衡的原因较简单，原因通常有二：

- 新增磁盘或者原有坏盘替换
- 存在不同容量大小的磁盘

解决的方法也有二：

- Hadoop 3.0 的 Balancer 支持磁盘间的数据平衡
- 调整 HDFS 的磁盘写入策略，设置 `dfs.datanode.fsdataset.volume.choosing.policy` 为 `org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy`，该策略会基于磁盘用量差距 `dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold`（默认 10G）以及权重 `dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction`（默认 0.75f）选择需要写入的磁盘。

**Refs：**

- [Why HDFS data Becomes unbalanced](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.0/data-storage/content/why_hdfs_data_becomes_unbalanced.html)
- [Scaling Namespaces and Optimizing Data Storage](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.0/data-storage/content/properties_for_configuring_the_balancer.html)
- [Cluster balancing algorithm](https://docs.cloudera.com/HDPDocuments/HDP3/HDP-3.1.0/data-storage/content/cluster_balancing_algorithm.html)
- [Increasing HDFS Balancer Performance](https://www.expecc.com/post/increasing-hdfs-balancer-performance)

