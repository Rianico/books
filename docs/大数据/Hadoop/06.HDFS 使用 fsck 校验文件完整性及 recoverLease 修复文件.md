---
title: HDFS 使用 fsck 校验文件完整性及 recoverLease 修复文件
date: 2020-07-22 10:57:04
---
# HDFS 使用 fsck 校验文件完整性及 recoverLease 修复文件

hdfs文件系统可以使用fsck来check\(校验\)文件完整性，语法格式：`hdfs fsck [file] [options]`

以下option，由上往下需要相继结合使用：

| options | comment |
| :---: | :---: |
| -files | 校验时展示文件信息 |
| -blocks | 展示文件的块信息，通常与-files结合使用 |
| -locations | 展示文件块所在的datanode地址 |
| -racks | 展示文件机架信息 |

```bash
# 检查某文件是否完整
hdfs fsck /zxk/sample.jar -files -blocks -locations -racks

# 检查某文件夹下文件是否完整
hdfs fsck /zxk
```

示例输出：

```vim
Connecting to namenode via http://localhost:50070/fsck?ugi=hdfs&files=1&blocks=1&racks=1&path=%2Fdata%2Fsw%2Fhttp%2FCZ%2F20190805%2F0000%2Fpart-0
FSCK started by hdfs (auth:SIMPLE) from /node1for path /zxk/sample.jar at Fri Aug 23 10:47:39 CST 2019
/zxk/sample.jar 185606618 bytes, 1 block(s):  OK
0. BP-1492896228-node2-1553135500323:blk_3942942813_2869317147 len=185606618 Live_repl=2 [/c/S9312-2/node3:50010, /c/S9312-1/node4:50010]

Status: HEALTHY
 Total size:    185606618 B
 Total dirs:    0
 Total files:   1
 Total symlinks:                0
 Total blocks (validated):      1 (avg. block size 185606618 B)
 Minimally replicated blocks:   1 (100.0 %)
 Over-replicated blocks:        0 (0.0 %)
 Under-replicated blocks:       0 (0.0 %)
 Mis-replicated blocks:         0 (0.0 %)
 Default replication factor:    2
 Average block replication:     2.0
 Corrupt blocks:                0
 Missing replicas:              0 (0.0 %)
 Number of data-nodes:          104
 Number of racks:               2
FSCK ended at Fri Aug 23 10:47:39 CST 2019 in 1 milliseconds


The filesystem under path '/zxk/sample.jar' is HEALTHY
```

假如文件有异常的话，我们可以观察到两种信息：

> Under replicated
>
> CORRUPT blockpool

**修复Under replicated：** 这种情况意味着还有其他replication存活，可使用`recoverLease`命令进行恢复：

```bash
# 寻找under replicated的文件
hdfs fsck / | grep -i "under replicated" | awk '{print $1}' |sort | uniq | sed -e 's/://g'> under_replicated.flst
```

接下来使用recoverLeae命令修复：

```bash
# [-path path]    HDFS path for which to recover the lease.
# [-retries num-retries]    Number of times the client will retry calling recoverLease. The default number of retries is 1.
for f in `cat under_replicated.flst`; { echo "Fixing $f" ;  hdfs debug recoverLease -path $f; }
```

**修复corrupt blocks：**

```bash
# 查找出损坏的文件并删除
 hdfs fsck / | egrep -v '^\.+$' | grep -v eplica| awk -F ':' '{system("sudo -u hdfs hadoop fs -rm -r "$1" 2>&1")}'
```

**参考**：

* [How to fix corrupt HDFS FIles](https://stackoverflow.com/questions/19205057/how-to-fix-corrupt-hdfs-files) [How to use hdfs fsck command to identify corrupted files?](http://fibrevillage.com/storage/658-how-to-use-hdfs-fsck-command-to-identify-corrupted-files)

