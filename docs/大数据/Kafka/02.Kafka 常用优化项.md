---
title: Kafka 常用优化项
date: 2020-05-26 17:06:05
---
# Kafka 常用优化项

机器配置：20台，256内存，40core，11个盘。 Consumer：峰值可输出1.8G/s（未达到极限）。 Producer：峰值可承受1.3G/s（未达到极限）。

### 1. 调整Kafka进程的jvm内存。

```bash
# 调整KAFKA_HEAP_OPTS="-Xmx12G -Xms12G”的值。
vim bin/kafka-server-start.sh
```

### 2. 网络和ios操作线程配置优化

```bash
# broker处理消息的最大线程数
num.network.threads=3
# broker 处理磁盘IO的线程数
num.io.threads=16
```

推荐配置：

* **num.network.threads**：主要处理网络io，读写缓冲区数据，基本没有io等待，属于CPU密集型，线程数并不用太多，避免无谓的上下文切换。
* **num.io.threads**：主要进行磁盘io操作，为了提高 CPU 与 磁盘的综合利用率，如果 CPU 核心大于磁盘数，则可以按照磁盘数目整数倍来设置，如果小于磁盘数，可以考虑在磁盘数目的基础上再添加一些线程。

### 3. 落盘策略

Kafka 接收到的 log 需要 flush 到磁盘，建议将落盘间隔（`log.flush.interval.ms`）与落盘最大消息条数（`log.flush.interval.messages`）调至最大，log 的落盘时机交由操作系统的脏页比例来控制：

```bash
# 永久修改脏页比例
# 当前系统默认10
echo "vm.dirty_background_ratio = 30" >> /etc/sysctl.conf
# 当前系统默认20
echo "vm.dirty_ratio = 70" >> /etc/sysctl.conf
# 令其生效
sysctl -p /etc/sysctl.conf
```

其中`vm.dirty_background_ratio`控制脏页到达了多少百分比内存的时候就会flush到磁盘，`vm.dirty_ratio`则是脏页到达了多少百分比内存时会进行阻塞，此时无法写入数据，直到操作系统将所有脏页flush到磁盘上。

### 4. 磁盘预读取

在顺序读取较多的场景下，调大磁盘预读取可以提高性能：

```
os_dev=`df |grep boot|sed -n '1p'|awk -F '/' '{print $3}'|awk '{print $1}'|sed 's![0-9]!!g'`
disk_List=`lsscsi | awk -F "/" '{print $NF}'|grep -vi Expander|grep -v $os_dev|grep -vi "storage"`
echo disk_List=$disk_List
for i in $disk_List
do
  echo 8192 > /sys/block/$i/queue/read_ahead_kb
done
```

### 5.日志设置策略

```bash
# 日志保留时长
log.retention.hours=72
# 段文件配置
log.segment.bytes=1073741824
```

`log.segment.bytes`设置为1GB有利于快速回收磁盘空间，若太小则会导致文件数太多。

### 6. replication 同步策略

```bash
num.replica.fetchers=3
replica.fetch.min.bytes=1
replica.fetch.max.bytes=5242880
```

每个follow从leader拉取消息进行同步数据，follow同步性能由这几个参数决定，分别为:

* **num.replica.fetchers**：拉取线程数，适当配置多可以提高follower的I/O并发度，单位时间内leader持有更多请求，相应负载会增大，需要根据机器硬件资源做权衡，通常建议1个磁盘对应一个线程；

* **replica.fetch.min.bytes**：副本拉取最小字节数，一般无需更改，默认值即可；

* **replica.fetch.max.bytes**：副本拉取最大字节数，默认为1MB，这个值太小，推荐5M，根据业务情况调整，但需要注意不要超过 broker 能接收的最大消息条数；

* **replica.fetch.wait.max.ms**：副本拉取最大等待时间，决定了follower拉取频率，频率过高，leader会积压大量无效请求情况，无法进行数据同步，导致cpu飙升。配置时谨慎使用，建议默认值，无需配置。

### 7. 调整 leader 重选举

```bash
leader.imbalance.check.interval.seconds=3600
leader.imbalance.per.broker.percentage=33
```

在broker宕掉重新恢复后，会发生 leader 重选举，此时 broker 上相关的 topic 会有段时间不可用（NotLeaderFoundException）。由于每个 partition 恢复并重新选出 leader 的时间有所错开，因此会造成不可用时间的叠加，调大 leader 允许的不平衡的比例，或者重选举间隔都可以解决此问题，视集群恢复时间而定。

**replica.lag.time.max.ms**： 只要 follower 副本每隔 500ms 都能发送 FetchRequest 请求给 leader，那么该副本就不会被标记成 dead从而被踢出ISR，根据具体情况设置。

参考：

* [Kafka性能调优](https://blog.51cto.com/xujpxm/1934572) 
* [大规模使用 Apache Kafka 的20个最佳实践](https://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&mid=2650716681&idx=1&sn=11fe2c40a70a051fa334da705ec6b0dd&chksm=887da77fbf0a2e693827144e30c374826e10a58b705475b8a02c43ba2d2f58fb5b065767fa67&scene=0&xtrack=1&key=6ccb6546e311818e665bec3ba4d0ac44ae289be54dc17fe544f9a9359355aa67b12dcfe64f93be28d00fac850b1bb6a401c3621e865990982982d96e92bf686830a759706b35fc913e47f01b50f3e758&ascene=1&uin=MTU0NzEwNTU4NA%3D%3D&devicetype=Windows+10&version=62060739&lang=zh_CN&pass_ticket=fFouQJcAQm8qWALuX23b1aCZDlo07A4VmKqNzIaaLry4w666H%2BiFIkKPe5hL62Jx) 
* [How can I send large messages with Kafka \(over 15MB\)?](https://stackoverflow.com/questions/21020347/how-can-i-send-large-messages-with-kafka-over-15mb)

