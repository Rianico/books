---
title: spark-shell 快速测试代码
date: 2020-05-26 17:06:05
---
# spark-shell 快速测试代码

## spark-dataframe + StructType

```scala
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import spark.implicits._
// val structype = StructType(Seq(StructField("name", StringType), StructField("time", LongType)))
val structype = StructType(Seq(StructField("name", StringType), StructField("time", StringType)))
val rdd = sc.parallelize(Seq("zxk,1584671303000")).map(row => Row.fromSeq(row.split(",")))
// val rdd = sc.parallelize(Seq("zxk,1584671303000")).map(row => row.split(",")).map(row => Row(row(0), row(1).toLong))
val df = spark.createDataFrame(rdd, structype)
df.withColumn("time2", from_unixtime($"time", "yyyy")).show
import org.apache.commons.lang3.StringEscapeUtils;
df.write.option("badRecordsPath", "/user/haohan/zxk/data/badRecordsPath").option("sep", StringEscapeUtils.unescapeJava(sep)).mode(SaveMode.Append).csv("/user/haohan/zxk/data")

// kerberos
export JVMFLAGS="-Djava.security.auth.login.config=/path/to/kafka_client_jaas.conf -Dsun.security.krb5.debug=true"
export KAFKA_OPTS="-Djava.security.auth.login.config=/path/to/kafka_client_jaas.conf"
spark2-shell --master local[1] --deploy-mode client  --keytab /path/to/xxx.keytab --principal xxx@xxx.xxx --jars /home/zxk/spark-sql-kafka-0-10_2.11-2.1.0.cloudera1.jar --files /path/to/kafka_client_jaas.conf#kafka_client_jaas_haohan.conf  --conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -Djava.security.auth.login.config=./kafka_client_jaas.conf" --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -Djava.security.auth.login.config=./kafka_client_jaas.conf"

// kafka指定允许消费的group前缀，要求kafka2.0及以上
/opt/cloudera/parcels/KAFKA/lib/kafka/bin/kafka-acls.sh  --authorizer kafka.security.auth.SimpleAclAuthorizer \
               --authorizer-properties zookeeper.connect=zookeeper1:2181 \
               --add --allow-principal User:'zxk' --operation READ \
               --topic CY_IM_JY_XDR --group='spark-kafka-source-' \
               --resource-pattern-type prefixed

```

