---
title: Spark指定Hadoop实际操作用户
date: 2020-05-26 17:06:05
---
# Spark指定Hadoop实际操作用户

在集群执行`spark-shell --master yarn`的时候，由于集群机器上的 spark 安装文件属于 **root** 用户，而 HDFS 文件系统上的用户名又是 **hdfs**，因此无法使用 `sudo -u hdfs` 来使用 hdfs 用户启动 spark，而使用 root 用户启动在对 HDFS 进行读写操作时又会出现权限问题，报错为：

```bash
org.apache.hadoop.security.AccessControlException: Permission denied: user=root, access=WRITE, inode="/xxx":hdfs:supergroup:drwxr-xr-x
```

因此需要指定 Spark 与 HDFS 交互的用户为 hdfs。

Spark 的 `saveAsTextFile(...)` 方法实质上调用的是 HDFS 中的 API，而 HDFS 获取用户的代码如下：

```java
if (!isSecurityEnabled() && (user == null)) {
  String envUser = System.getenv(HADOOP_USER_NAME);
  if (envUser == null) {
    envUser = System.getProperty(HADOOP_USER_NAME);
  }
  user = envUser == null ? null : new User(envUser);
}
```

由上可得知，在Hadoop是通过获取系统参数来指定用户的。

> **Note**：在Kerberos安全集群环境下，获取用户的方式又有所不同，详细可以参考 [Hadoop Authentication](http://www.udpwork.com/item/7047.html)，此处不做讨论。

到此思路比较明朗了，只要修改系统参数`HADOOP_USER_NAME`的值为hdfs即可，可以通过以下两种方式指定用户。

**方法一：**

在Linux环境下，可以先执行

```bash
export HADOOP_USER_NAME=hdfs
```

然后再启动 spark-shell，执行 `System.getProperty("HADOOP_USER_NAME")` ，即可看到用户已经为 hdfs。

**方法二（推荐）：**

对 spark-shell 的运行时环境进行设置，如下：

```bash
# spark.executor.extraJavaOptions：指定executor端的参数
# spark.driver.extraJavaOptions：指定driver端的参数
spark-shell --master yarn --conf spark.executor.extraJavaOptions=-DHADOOP_USER_NAME=hdfs --conf spark.driver.extraJavaOptions=-DHADOOP_USER_NAME=hdfs
```

执行 `System.getProperty("HADOOP_USER_NAME")` ，即可看到用户已经为hdfs。

再次执行Spark的保存文件操作，保存成功！

个人比较推荐第二种方法，不仅可以在 spark-shell 使用，通过 spark-submit 提交作业的时候也是通用的。

参考：

* [HDFS客户端的权限错误：Permission denied](http://www.huqiwen.com/2013/07/18/hdfs-permission-denied/)
* [Spark官方文档](http://spark.apache.org/docs/latest/configuration.html#runtime-environment)

