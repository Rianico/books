---
title: Spark 之 bucket
date: 2020-07-22 10:57:01
---
# Spark 之 bucket

Spark SQL 存储数据的时候有一个技巧，在做 partition 的时候，可以根据特定几个字段分配到特定数量的 bucket，从而将数据分布在同个 partition 下的不同 bucket。

这样做的好处就是，当我们后面进行数据分析的时候，如果需要根据那几个字段做统计分析（如 group by），那么可以避免 shuffle。

首先进行 partition 以及 bucket：

```scala
df.write.format("parquet")
.sortBy("id")
.partitionBy("src")
.bucketBy(4,"dst","carrier")
.option("path", "/user/mapr/data/flightsbkdc")
.saveAsTable("flightsbkdc")
```

接着根据这几个字段进行聚合分析：

```scala
spark.sql("ANALYZE TABLE flightsbkdc COMPUTE STATISTICS")
val df2  = spark.table("flightsbkdc")
df2.filter("src = 'DEN' and depdelay > 1")
.groupBy("src", "dst","carrier")
.avg("depdelay")
.sort(desc("avg(depdelay)")).show()
```

![img](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/20210421152416.png)

可以看到，DAG 里并没有 shuffle 过程。

需要注意的是，bucket 通常用于一个字段组合能够有多条数据的场景。

`ANALYZE TABLE` 用于 Spark 统计 table 一些元数据信息，语法格式如下：

```sql
ANALYZE TABLE table_name [ PARTITION ( partition_col_name [ = partition_col_val ] [ , ... ] ) ]
    COMPUTE STATISTICS [ NOSCAN | FOR COLUMNS col [ , ... ] | FOR ALL COLUMNS ]
```

其中 partition 部分可选，用于指定统计部分 partition 的元数据信息。至于最后部分的可选项，分为以下几种情况：

* 如果没有指定任何参数，则默认统计 table 的行数以及字节大小。
* **NOSCAN**：只统计 table 的字节大小，而不用扫描整个表。
* **FOR COLUMNS col \[ , ... \] `|`FOR ALL COLUMNS**：只统计某列的元数据信息，或者所有列信息。

参考：

* [Tips and Best Practices to Take Advantage of Spark 2.x](https://mapr.com/blog/tips-and-best-practices-to-take-advantage-of-spark-2-x/)
* [ANALYZE TABLE](https://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux-analyze-table.html)

