---
title: spark-shell 备用
date: 2020-05-26 17:06:05
---
# spark-shell 快速测试代码

```bash
export SPARK_DIST_CLASSPATH=$(hadoop classpath) 
export HADOOP_CONF_DIR=/opt/cloudera/parcels/CDH/etc/hadoop 
# 如果需要用到 lzo 且有安装的话
export LD_LIBRARY_PATH=LD_LIBRARY_PATH:/opt/cloudera/parcels/HADOOP_LZO/lib/hadoop/lib/native:/opt/cloudera/parcels/CDH/lib/hadoop/lib/native/
```

启动 spark2-shell：

```bash
// kerberos
export JVMFLAGS="-Djava.security.auth.login.config=/path/to/kafka_client_jaas.conf -Dsun.security.krb5.debug=true"
export KAFKA_OPTS="-Djava.security.auth.login.config=/path/to/kafka_client_jaas.conf"
spark2-shell --master local[1] --deploy-mode client  --keytab /path/to/xxx.keytab --principal xxx@xxx.xxx --jars /home/zxk/spark-sql-kafka-0-10_2.11-2.1.0.cloudera1.jar --files /path/to/kafka_client_jaas.conf --conf "spark.driver.extraJavaOptions=-XX:+UseG1GC -Djava.security.auth.login.config=./kafka_client_jaas.conf" --conf "spark.executor.extraJavaOptions=-XX:+UseG1GC -Djava.security.auth.login.config=./kafka_client_jaas.conf"

# 开启 Schema 推测
spark.read.format("csv")
.option("header", "true")
.option("inferSchema", "true")
.load(mnmFile)
```

## 1. Dataframe + StructType

```scala
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import spark.implicits._

// val structype = StructType(Seq(StructField("name", StringType), StructField("time", LongType)))
val structype = StructType(Seq(StructField("name", StringType), StructField("time", StringType)))
val rdd = sc.parallelize(Seq("zxk,1584671303000")).map(row => Row.fromSeq(row.split(",")))
// val rdd = sc.parallelize(Seq("zxk,1584671303000")).map(row => row.split(",")).map(row => Row(row(0), row(1).toLong))
val df = spark.createDataFrame(rdd, structype)
df.withColumn("time2", from_unixtime($"time", "yyyy")).show

import org.apache.commons.lang3.StringEscapeUtils;
df.write
  .option("badRecordsPath", "/user/haohan/zxk/data/badRecordsPath")
  .option("sep", StringEscapeUtils.unescapeJava(sep))
  .mode(SaveMode.Append)
  .csv("/user/haohan/zxk/data")
```

## 2. 开启 tungsten-sort

Spark RDD 如果想要使用 tungsten-sort 功能，则需要开启堆外内存并指定大小：`-conf spark.memory.offHeap.enabled=true --conf spark.memory.offHeap.size=1g`。

使用该功能需要满足几个条件：

1. partition > spark.shuffle.sort.bypassMergeThreshold（默认200），并且小于16777216
2. 无聚合操作（如groupBy，reduceBy等）
3. 序列化时，单条记录不能大于 128 MB
4. Serializer 需要支持 relocation，如 Kryo。

目前 SparkSql 在开启 tungsten-sort 后，在符合条件的场景下会自动应用，而 RDD 编程需要自己注意下使用的算子。

## 3. Spark 开启 Kerberos debug 信息

可以在 Hadoop 中设置环境变量开启 Kerberos 调试信息：

```bash
export HADOOP_JAAS_DEBUG=true
```

或者通过设置 Java 的系统参数来记录 Kerberos 以及 SPNEGO/REST 认证的日志：

```bash
-Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true
```

Spark 中可以在 AM 中开启：

```xml
spark.yarn.appMasterEnv.HADOOP_JAAS_DEBUG true
spark.yarn.am.extraJavaOptions -Dsun.security.krb5.debug=true -Dsun.security.spnego.debug=true
```

## 4. Kerberos 环境下与 Oozie 结合

这种场景下，Spark 需要的一些凭据都是交由 Oozie 完成：

- The YARN resource manager.
- The local HDFS filesystem.
- Any remote HDFS filesystems used as a source or destination of I/O.
- Hive —if used.
- HBase —if used.
- The YARN timeline server, if the application interacts with this.

如果 Spark 再次去获取，很可能会导致异常退出，因此需要设置一些参数避免 Spark 主动去获取一些服务的凭据：

2.2 之前：

```xml
spark.yarn.security.tokens.hive.enabled   false
spark.yarn.security.tokens.hbase.enabled  false
```

2.2 开始：

```xml
spark.yarn.security.credentials.hive.enabled false
spark.yarn.security.credentials.hbase.enabled false
```

同时不能设置 `spark.yarn.access.namenodes` 属性。

## 5. Spark 写 hdfs 超时时间

```scala
// spark config
val sc = new SparkConfig() {
	// 设定写 HDFS 超时时间
	setConfig("spark.hadoop.dfs.datanode.socket.write.timeout", timeout)
}.buildSparkContext()
```

