---
title: 12. LearingSpark2.0
date: 2021-06-25
---

## The Dataframe API

### 常用 API

Spark 支持以下基本数据类型：

![image-20210625220640953](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210625220640953.png)

除了最后的 `DecimalType` 之外，其他类型都位于 `org.apache.spark.sql.types._` 下。

Spark 还支持一些复杂格式：

![image-20210625220847599](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210625220847599.png)

在读取外部数据之前，通常建议先自定义好 Schema，有如下好处：

- 不用把数据类型推断的任务交给 Spark
- 防止 Spark 额外创建任务去读取文件的一部分以便推测出 Schema，在大文件场景下开销很高
- 在数据不符合预期 Schema 的时候可以尽早地获取到错误

定义 Schema 有两种方式，一种是以 API 编程方式，另一种是使用 DDL 字符描述，如下：

```scala
// 编程方式
import org.apache.spark.sql.types._
val schema = StructType(Array(StructField("author", StringType, false), StructField("title", StringType, false),
StructField("pages", IntegerType, false)))

// DDL 方式
val schema = "author STRING, title STRING, pages INT"
```

在 spark-shell 中执行如下代码：

```scala
val schema = "Id INT, First STRING, Last STRING, Url STRING, Published STRING, Hits INT, Campaigns ARRAY<STRING>"
val blogDF = spark.read.schema(schema).json("/usr/local/src/LearningSparkV2/chapter3/data/blogs.json")
blogDF show 10
```

添加额外的列：

```scala
import spark.implicits._
blogDF.withColumn("Big Hitters", $"Hits" > 10000).show(10)
// 排序
blogDF.sort($"Id".desc).show(10)
```

关于 Column 类型，更多方法详见 [Spark 3.1.2 ScalaDoc  - org.apache.spark.sql.Column](http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/Column.html)。

Column 对象无法独立存在，每个 Column 都是 Row 对象的一部分。

Row 在 Spark 中是一个有序的字段集合，

通常可以用 Row 对象来快速构造一个 DataFrame 用于快速测试：

```scala
val rows = Seq(("Matei Zaharia", "CA"), ("Reynold Xin", "CA"))
val authorsDF = rows.toDF("Author", "State")
authorsDF.show()
```

> NOTE：通常来说，我们绝大多数场景下都是通过读取一个大文件来构造 DataFrame 的，预先定义好一个 Schema 再来创建一个 DataFrame 更加高效。

### 使用 DataFrameReader 和 DataFrameWriter

Spark 提供了两个接口：DataFrameReader 和 DataFrameWriter，分别用于数据的读取与写入。支持的文件格式有 JSON, CSV, Parquet, Text, Avro, ORC 等。

Spark 读取文件如下：

```scala
val fireDF = spark.read
	.option("header", true)
	.option("inferSchema", true)
	.csv("/usr/local/src/LearningSparkV2/chapter3/data/sf-fire-calls.csv")
```

Spark 写入数据时默认格式为 Parquet，默认压缩格式为 snappy。数据的 Schema 会作为 Parquet 元数据的一部分进行存储，因此后续重新读取 Parquet 的时候不需要手动指定 Schema。

```scala
fire_df.write.format("parquet").save("/usr/local/src/LearningSparkV2/chapter3/data/")
```

### Transformations

```scala
fireDF.select("CallType")
	// 过滤
	.where($"CallType".isNotNull)
	// 聚合
	.agg(countDistinct("CallType") as "DistinctCallType")
	.show()

// 重命名列
val newFireDF = fireDF.withColumnRenamed("Delay", "ResponseDelayedinMins")
// String 格式转时间格式，其他时间函数还有 year()/month()/day() 等
val fireTsDF = newFireDF
	.withColumn("IncidentDate", to_timestamp(col("CallDate"), "MM/dd/yyyy")) .drop("CallDate")
	.withColumn("OnWatchDate", to_timestamp(col("WatchDate"), "MM/dd/yyyy")) .drop("WatchDate")
	.withColumn("AvailableDtTS", to_timestamp(col("AvailableDtTm"), "MM/dd/yyyy hh:mm:ss a"))
	.drop("AvailableDtTm")

// 使用 case when
val df = spark.read.option("inferSchema", "true").option("header", "true").csv("/usr/local/src/LearningSparkV2/databricks-datasets/learning-spark-v2/flights/departuredelays.csv")
df.select(col("delay"), col("origin"), col("destination"), when(col("delay") > 360, "Long Delyas") when(col("delay") < 360 && col("delay") > 100, "Short Delay") otherwise "Delays" alias "Long_Delays")
df.select("delay", "origin", "destination").withColumn("Filght_Delays", when(col("delay") > 360, "Long Delyas") otherwise("Delays"))
```

### DataSet

从概念上来说，可以认为 DataFrame 在 Scala 或 Java 中类似于泛型的集合，即 Dataset[Row]。

Dataset 通常都是一组有明确类型 JVM 对象的集合，可以借助编译器实现编译时类型安全检查。

> NOTE：Dataset 只在有编译时类型安全的语言中才有意义，两者其实最大的区别就在于编译时对类型安全检查程度的强弱。

Row 在 Spark 中可以认为类似于一个泛型对象，只能通过索引访问其中的元素，在编译时并无法确定真正的类型，直到运行时才会尝试将 Row 中的元素转化为其对应类型的对象。

相比之下，Dataset 里的类型则可以指定为一个明确的 class 类型，可以映射为一个具体的 JVM 对象。

创建 Dataset 方式如下：

```scala
case class DeviceIoTData (battery_level: Long, c02_level: Long, cca2: String, cca3: String, cn: String, device_id: Long,device_name: String, humidity: Long, ip: String, latitude: Double, lcd: String, longitude: Double, scale:String, temp: Long, timestamp: Long)

val ds = spark.read.json("xxx").as[DeviceIoTData]
val filterTempDS = ds.filter({d => {d.temp > 30 && d.humidity > 70})                         
```

可以看到，使用了 Dataset 后，我们可以在编程中使用很多 JVM 对象常用的方法。

> NOTE：当我们使用 Dataset 时，底层的 Spark SQL 引擎会负责 JVM 对象的创建、转化、序列化 和反序列化。同时也要注意对外内存管理，这块内存用于协助 Dataset 的编码器。

SQL、DataFrame、Dataset 三者之间在语法、类型检查的区别如下：

![image-20210626230810949](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210626230810949.png)

### Spark SQL 和底层引擎

Spark 可以让开发者通过 SQL 的方式，使用大量内建的结构化方法，除此之外 Spark SQL 引擎还支持如下：

![image-20210626231440893](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210626231440893.png)



## Spark SQL and DataFrames: Introduction to Built-in Data Sources

### SQL 表与视图

Spark 是通过 metadata 与表进行关联的，默认使用 *Hive metadata*，并在 `/user/hive/warehouse` 下存储关于表的 metadata，可以通过修改 `spark.sql.warehouse.dir` 指定本地或者外部存储路径。

### 托管表与非托管表

Spark 可以创建两种类型的表：托管表与非托管表。

- 托管表：Spark 同时管理其元数据与数据存储。当 Spark 删除数据时，外部数据源也会跟着删除实际数据。
- 非托管表：Spark 只接管元数据管理，具体数据则由外部数据源自行管理，删除数据时也只会删除元数据而非实际数据。

```scala
// 创建托管表
spark.sql("CREATE TABLE managed_us_delay_flights_tbl (date STRING, delay INT,   distance INT, origin STRING, destination STRING)")

// 创建非托管表
spark.sql("""CREATE TABLE us_delay_flights_tbl(date STRING, delay INT,   distance INT, origin STRING, destination STRING)
  USING csv 
  OPTIONS (PATH '/databricks-datasets/learning-spark-v2/flights/departuredelays.csv')""")
```

### 创建视图

Spark 可以基于已存在的表创建视图。视图可以基于集群范围跨 SparkSession 使用，也可以只限制在单个 SparkSession 中使用。

视图的生命周期与创建它们的 Spark 应用保持一致。

SQL 方式创建视图如下：

```sql
-- In SQL
CREATE OR REPLACE GLOBAL TEMP VIEW us_origin_airport_SFO_global_tmp_view AS SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'SFO';

CREATE OR REPLACE TEMP VIEW us_origin_airport_JFK_tmp_view AS
SELECT date, delay, origin, destination from us_delay_flights_tbl WHERE origin = 'JFK'
```

Dataframe 创建视图如下：

```scala
df_sfo = spark.sql("SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'SFO'")
df_jfk = spark.sql("SELECT date, delay, origin, destination FROM us_delay_flights_tbl WHERE origin = 'JFK'")
// Create a temporary and global temporary view
df_sfo.createOrReplaceGlobalTempView("us_origin_airport_SFO_global_tmp_view") df_jfk.createOrReplaceTempView("us_origin_airport_JFK_tmp_view")
```

需要注意当访问全局的视图时，需要加上 `global_temp` 前缀：

```sql
SELECT * FROM global_temp.us_origin_airport_SFO_global_tmp_view
```

删除视图：

```scala
// API 方式
spark.catalog.dropGlobalTempView("us_origin_airport_SFO_global_tmp_view") 
spark.catalog.dropTempView("us_origin_airport_JFK_tmp_view")

// SQL 方式
spark.sql("DROP VIEW IF EXISTS us_origin_airport_SFO_global_tmp_view")
spark.sql("DROP VIEW IF EXISTS us_origin_airport_JFK_tmp_view")
```

全局视图的作用主要在于跨 SparkSession 使用（一个 Spark 应用可以有多个 Spark Session），在多个 SparkSession 使用了不同 Hive metastore 的时候很方便。

Spark 将表、视图的 metastore 高度抽象为了 Catalog。Spark 提供了以下几个 API，可以方便的查看当前 metastore 与数据库、表、视图之间的关联关系：

```scala
spark.catalog.listDatabases()
spark.catalog.listTables()
spark.catalog.listColumns("xxx")
```

Spark 3.0 开始，允许指定 *LZAY* 关键字，表示这个表只会在初次使用时才缓存起来：

```sql
CACHE [LAZY] TABLE <table-name>
UNCACHE TABLE <table-name>
```

### Data Sources for DataFrames and SQL Tables

通过前面可以知道，Spark SQL 对于多种数据源提供了一个统一的接口，同时也通过 API 提供了一组公共方法用于数据源读写数据。

Spark 不仅提供了各种内置的数据源，还高度抽象了 **DataFrameReader** 以及 **DataFrameWriter** 用于与各种不同的数据源进行交互。

DataFrameReader 是读取数据源并将其转化为 DataFrame 的核心结构，其推荐的调用方式如下：

```scala
DataFrameReader.format(args).option("key", "value").schema(args).load()
```

当前只能通过 SparkSession 来获得其实例：

```scala
SparkSession.read 
// or
SparkSession.readStream
```

DataFrame 的各个方法入参如下：

![image-20210702225706489](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210702225706489.png)

> NOTE：通常在读取一个静态 Parquet 数据源的时候是不需要指定 schema 的，因为 Parquet 的 metadata 通常都包含了 schema。

DataFrameWriter 正好与 DataFramReader 相反，通常使用方式如下：

```scala
DataFrameWriter.format(args)
	.option(args)
	.bucketBy(args)
	.partitionBy(args)
	.save(path)

DataFrameWriter.format(args)
	.option(args)
	.sortBy(args)
	.saveAsTable(table)
```

DataFrameWriter 可以通过 DataFrame 获取：

```scala
DataFrame.write
// or
DataFrame.writeStream
```

各个方法的配置参数如下（仅列出常用部分）：

![image-20210703194418025](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210703194418025.png)

### Parquet

Parquet 是 Spark 默认的存储格式，存储在结构化目录下，包含了数据文件、metadata、一系列压缩后的文件以及一些状态文件。

读取 Parquet 有如下几种方式：

```scala
file = """/databricks-datasets/learning-spark-v2/flights/summary-data/parquet/2010-summary.parquet/"""
// API 方式
df = spark.read.format("parquet").load(file)
// 创建临时 view 方式
spark.sql("CREATE OR REPLACE TEMPORARY VIEW us_delay_flights_tbl USING parquet OPTIONS (path $file)")
spark.sql("SELECT * FROM us_delay_flights_tbl").show()
```

保存 Parquet 方式如下：

```scala
// API 方式保存文件
df.write.format("parquet")
	.mode("overwrite")
	.option("compression", "snappy")
	.save("/tmp/data/parquet/df_parquet")

// API 方式保存到表
df.write.mode("overwrite").saveAsTable("us_delay_flights_tbl")
```

## Spark SQL and DataFrames: Interacting with External Data Sources

Spark 可以与外部数据源交互，并提供了很高的灵活性。

### 外部数据源

Spark 可以像 Hive 一样定义一个 udf 函数：

```scala
// In Scala
// Create cubed function 
val cubed = (s: Long) => { s*s*s }
// Register UDF
spark.udf.register("cubed", cubed)
// Create temporary view
spark.range(1, 9).createOrReplaceTempView("udf_test")
// Query the cubed UDF
spark.sql("SELECT id, cubed(id) AS id_cubed FROM udf_test").show()

+---+--------+
| id|id_cubed| 
+---+--------+
|  1|       1|
|  2|       8|
|  3|      27|
|  4|      64|
|  5|     125|
|  6|     216|
|  7|     343|
|  8|     512|
+---+--------+
```

Spark SQL 并不保证表达式的执行顺序，例如，下列查询并不保证在执行表达式之前对 s 进行空校验：

```scala
spark.sql("SELECT s FROM test1 WHERE s IS NOT NULL AND strlen(s) > 1")
```

通常推荐如下做法：

1. 在 UDF 中做空检查
2. 使用 `IF`、`CASE WHEN` 表达式进行空校验。

Spark 可以连接多种数据源并运行 Spark SQL：

- **spark-sql**，并不会与 Spark Thrift Server 交互，而是直接与 Hive metastore 进行交互

- **beeline**，可以通过与 Spark Thrift Server 交互，以 SQL 形式直接执行 Spark SQL

- **JDBC**，Spark 也可以通过 spark-shell 连接传统的数据库，但需要先指定对应数据库的驱动包，并配置必要的选项：

  ```bash
  ./bin/spark-shell --driver-class-path $database.jar --jars $database.jar
  ```

  ![image-20210703230558794](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210703230558794.png)

当在 Spark SQL 与外部 JDBC 数据源之间传输大量数据的时候，数据源进行分区非常重要。

常用的几个配置如下：

- numPartitions：指定 Spark 读/写的并行度，对应着多少个 JDBC 连接
- partitionColumn：用于划分数据 partition 的列，可以是 numeric、date、timestamp 类型
- lowerBound：指定分区列的下界值
- upperBound：指定分区列的上界值

> NOTE：指定 lowerBound 与 upperBound 时需要注意数据的分布，如果指定 numPartitons 为 10，那么每个数据分片的取值范围为 （upperBound - lowerBound）/ 10。

以读写 PostgreSQL 为例：

```scala
// In Scala
// Read Option 1: Loading data from a JDBC source using load method val jdbcDF1 = spark
val jdbcDF1 = spark.read
	.format("jdbc")
	.option("url", "jdbc:postgresql:[DBSERVER]")
	.option("dbtable", "[SCHEMA].[TABLENAME]")
	.option("user", "[USERNAME]")
	.option("password", "[PASSWORD]")
	.load()

// Read Option 2: Loading data from a JDBC source using jdbc method // Create connection properties
import java.util.Properties
val cxnProp = new Properties()
cxnProp.put("user", "[USERNAME]")
cxnProp.put("password", "[PASSWORD]")
// Load data using the connection properties val jdbcDF2 = spark
val jdbcDF2 = spark.read
	.jdbc("jdbc:postgresql:[DBSERVER]", "[SCHEMA].[TABLENAME]", cxnProp) 

// Write Option 1: Saving data to a JDBC source using save method
jdbcDF1.write
	.format("jdbc")
	.option("url", "jdbc:postgresql:[DBSERVER]") 
	.option("dbtable", "[SCHEMA].[TABLENAME]")
	.option("user", "[USERNAME]")
	.option("password", "[PASSWORD]")
	.save()

// Write Option 2: Saving data to a JDBC source using jdbc method jdbcDF2.write
jdbcDF2.write
	.jdbc(s"jdbc:postgresql:[DBSERVER]", "[SCHEMA].[TABLENAME]", cxnProp)
```

还有其他几种数据源参考如下：

- Cassandra：[spark-cassandra-connector](https://github.com/datastax/spark-cassandra-connector)
- MongoDB：[MongoDB Connector for Spark](https://docs.mongodb.com/spark-connector/master/)

### 高阶函数

对于复杂类型的操作，有两种常见的方式：

- 将嵌套的结构展开为多行，处理完后重新构造为嵌套结构

  ```sql
  SELECT id, collect_list(value + 1) AS values FROM (SELECT id, EXPLODE(values) AS value
  FROM table) x
  GROUP BY id
  ```

  这种方式下，会涉及到数组里的排序问题，并且也会有 group by 这种昂贵的操作

- 使用 udf 函数

  ```scala
  spark.sql("SELECT id, plusOneInt(values) AS values FROM table").show()
  ```

  对比前种方式，不会有太大的开销，但处理过程中的序列化、反序列开销可能也会很大。

上述这两种方式都存在一定的缺陷。

#### 使用内置函数处理复杂数据类型

相比上面的解决方式，Spark 2.4 开始提供了针对复杂类型的内置函数。

**针对 Array 类型**：

![image-20210709222329791](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210709222329791.png)

![](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210709222329791.png)

![image-20210709222412924](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210709222412924.png)

**针对 Map 类型**：

![image-20210709222432452](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/image-20210709222432452.png)

#### 高阶函数

除了前面的内置函数，Spark 还提供了高阶函数，接收一个列（array 类型）与 lambda 表达式，并将表达式作用于列上，生成一个新的列，这类方法往往比 udf 更加高效。

首先创建一个 table：

```scala
// In Scala
// Create DataFrame with two rows of two arrays (tempc1, tempc2)
val t1 = Array(35, 36, 32, 30, 40, 42, 38)
val t2 = Array(31, 32, 34, 55, 56)
val tC = Seq(t1, t2).toDF("celsius") 
tC.createOrReplaceTempView("tC")
// Show the DataFrame
tC.show()

+--------------------+ 
|             celsius|
+--------------------+ 
|[35, 36, 32, 30, ...|
|[31, 32, 34, 55, 56]|
+--------------------+
```

**transform**，对数组进行转化，语法：*transform(array<T>, function<T, U>): array<U>*

```scala
// In Scala/Python
// Calculate Fahrenheit from Celsius for an array of temperatures 
spark.sql("""
SELECT celsius,
transform(celsius, t -> ((t * 9) div 5) + 32) as fahrenheit
FROM tC
""").show()
```

**filter**，过滤数组中符合条件的元素，语法：*filter(array<T>, function<T, Boolean>): array<T>*：

```scala
// In Scala/Python
// Filter temperatures > 38C for array of temperatures 
spark.sql("""
SELECT celsius,
 filter(celsius, t -> t > 38) as high
 FROM tC
""").show()
```

**exists**，一旦数组中存在符合条件的元素，就返回整个数组：*exists(array<T>, function<T, V, Boolean>): Boolean*

```scala
// In Scala/Python
// Is there a temperature of 38C in the array of temperatures 
spark.sql("""
SELECT celsius,
       exists(celsius, t -> t = 38) as threshold
FROM tC
""").show()
```

**reduce**，将数组中的元素进行合并，最后在进行一次处理，类似 rdd 的 reduce 方法：*reduce(array<T>, B, function<B, T, B>, function<B, R>)*：

```scala
// In Scala/Python
// Calculate average temperature and convert to F 
spark.sql("""
SELECT celsius,
	reduce(
	celsius,
    0,
    (t, acc) -> t + acc,
    acc -> (acc div size(celsius) * 9 div 5) + 32
  	) as avgFahrenheit
  FROM tC
""").show()
```

开窗函数：[Introducing Window Functions in Spark SQL](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)

行转列：Pivoting

## Spark SQL and Datasets

DataSet 需要传入一个 case class，并要求字段名称与顺序跟数据的 schema（如果有）保持一致。

DataFrame 可以轻松的转化为 DataSet：

```scala
case class erobook(name: String)
val df = // Generate a DataFrame
df.as[erobook]
```

DataSet 相比 DataFrame，提供了编译时类型安全、面向对象编程风格。

相比 DataFrame 直接使用 Tungsten 存储格式，DataSet 需要借助 Encoder 完成 Tungsten 内部格式到 JVM 对象之间的映射。DataFrame 可以看作是一个数据类型为 Row 的 Dataset，Row 内部为 Tungsten 存储结构。

> NOTE： Encoders 负责 Tungsten 二进制格式到 JVM 对象之间的序列化、反序列化，提供了了原始类型、自定义类型的支持。

Spark 对 Dataset 和 DataFrame 的内存管理演化：

- Spark 1.0 时，Spark 基于 RDD 的 Java 对象作为内存对象存储、序列/反序列化，受到 JVM 的限制。

- Spark 1.x 开始，Spark 推出了 Tungsten 项目，可以基于行将 DatsSet、DataFrame 存储在堆外，配合指针以及 offset 使用。Spark 使用 Encoders 在 JVM 对象以及内部 Tungsten 格式之间快速序列化、反序列化。

- Spark 2.x 带来了 wscg 以及基于列存储的向量化内存结构，可以充分利用近代处理器的 SIMD 特性。

相比 Java 对象，Tungsten 使用了紧凑的二进制数组结构存储数据，并且由于已经是二进制数组，因此可以灵活选择存储在堆内或堆外，同时也能节省部分操作（如网络传输时的序列化）。

一个对象基于 Tungsten 行存储的结构如下：

![image-20210819155126210](https://gitee.com/zhxuankun/Image/raw/master/blog/image-20210819155126210.png)

使用 Dataset 一个最大的问题就是存在 Tungsten 格式到 JVM 对象之间的序列化-反序列化开销，这也是为了使用面向对象编程的一个代价。

减少这个开销通常有两种途径：

- 在高阶函数中尽量使用 DSL，而不使用 lambda、匿名函数，对于 Spark 的 Catalyst 优化器来说，lambda 与匿名函数属于一个“黑盒”操作，难以优化（同 UDF）
- 尽可能使用同一种查询方式，而不是在不同方法之间（DSL 和面向对象）切换，这也是实践中最常用的方式

以下面代码为例：

```scala
import java.util.Calendar
val earliestYear = Calendar.getInstance.get(Calendar.YEAR) - 40
personDS
 // Everyone above 40: lambda-1
 .filter(x => x.birthDate.split("-")(0).toInt > earliestYear)
 // Everyone earning more than 80K
 .filter($"salary" > 80000)
 // Last name starts with J: lambda-2
 .filter(x => x.lastName.startsWith("J"))
 // First name starts with D
 .filter($"firstName".startsWith("D"))
 .count()
```

这段查询会不断地在 Tungsten 与 JVM 对象之间来回序列化：

![image-20210819160920302](https://gitee.com/zhxuankun/Image/raw/master/blog/image-20210819160920302.png)

修改为下：

```scala
personDS
 .filter(year($"birthDate") > earliestYear) // Everyone above 40
 .filter($"salary" > 80000) // Everyone earning more than 80K
 .filter($"lastName".startsWith("J")) // Last name starts with J
 .filter($"firstName".startsWith("D")) // First name starts with D
 .count()
```

这段代码统一使用了 DSL 方式，存在来回切换的开销。

## Optimizing and Tuning Spark for Efficiency

Spark 配置读取顺序（优先级从低到高）：

1. Spark 的 conf 目录下
2. `spark-submit` 指定 `--conf`
3. 应用程序内指定

开启 Spark 动态资源调整：

- spark.dynamicAllocation.enabled true 
- spark.dynamicAllocation.minExecutors 2 
- spark.dynamicAllocation.schedulerBacklogTimeout 1m 
- spark.dynamicAllocation.maxExecutors 20 
- spark.dynamicAllocation.executorIdleTimeout 2min

> NOTE： Spark 动态资源调整是基于 pending task 的等待时长来决定是否申请额外资源的。

