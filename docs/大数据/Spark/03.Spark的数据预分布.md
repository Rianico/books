---
title: Spark 的数据预分布
date: 2020-07-22 10:57:01
---
# Spark 之 bucket

Spark SQL 存储数据的时候有一个技巧，在存储数据时，可以相对数据做预分布，后面某些场景下可以利用数据预分布消除 Shuffle。

Spark 里的 Bucket Join 就使用了数据预分布的技巧，首先存储的时候指定 partition 以及 bucket 字段：

```scala
df.write.format("parquet")
.sortBy("id")
.partitionBy("src")
.bucketBy(4, "dst", "carrier")
.option("path", "/user/mapr/data/flightsbkdc")
.saveAsTable("flightsbkdc")
```

接着根据这几个字段进行聚合分析：

```scala
spark.sql("ANALYZE TABLE flightsbkdc COMPUTE STATISTICS")
val df2  = spark.table("flightsbkdc")
df2.filter("src = 'DEN' and depdelay > 1")
.groupBy("src", "dst","carrier")
.avg("depdelay")
.sort(desc("avg(depdelay)")).show()
```

![img](https://gitee.com/zhxuankun/Image/raw/master/ARTS_Tips/20210421152416.png)

可以看到，DAG 里并没有 shuffle 过程，但要实现去 Shuffle 需要满足以下几个条件：

1. 两边的分桶数互为整数倍
2. Becket Key 必须是 Join Key 的子集

bucket 存在的缺点：

- 生成的文件大小受到数据倾斜的影响
- 分桶会导致生成更多的文件数

通常我们也可以使用 `ANALYZE TABLE`  统计 table 一些元数据信息协助我们做分区字段选择的决策，语法格式如下：

```sql
ANALYZE TABLE table_name [ PARTITION ( partition_col_name [ = partition_col_val ] [ , ... ] ) ]
    COMPUTE STATISTICS [ NOSCAN | FOR COLUMNS col [ , ... ] | FOR ALL COLUMNS ]
```

其中 partition 部分可选，用于指定统计部分 partition 的元数据信息。至于最后部分的可选项，分为以下几种情况：

* 如果没有指定任何参数，则默认统计 table 的行数以及字节大小。
* **NOSCAN**：只统计 table 的字节大小，而不用扫描整个表。
* **FOR COLUMNS col [ , ... ]  | FOR ALL COLUMNS**：只统计某列的元数据信息，或者所有列信息。

参考：

* [Tips and Best Practices to Take Advantage of Spark 2.x](https://mapr.com/blog/tips-and-best-practices-to-take-advantage-of-spark-2-x/)
* [ANALYZE TABLE](https://spark.apache.org/docs/3.0.0-preview/sql-ref-syntax-aux-analyze-table.html)

