---
title: 09. SparkStreaming 实现 Exactly-Once 语义
date: 2021-05-15
---

## 1. 前置知识

要先了解 SparkStreaming 提供的语义，需要先了解 Spark RDD 的容错语义：

1. 一个 RDD 是一个不可变，可溯源重算的分布式数据集。
2. 如果 RDD 的某个分区数据由于 executor 失败丢失了，那么可以通过**原始的容错数据集**进行重算。
3. 假设 RDD 的全部转化操作都是确定的，那么无论发生什么故障，最终转化出来的 RDD 总是相同的。

从上面的语义来看，如果 Spark 的数据来自于一个可靠的文件系统（如 HDFS、S3），那么 Spark 的所有 RDD 也是可靠的。

但 SparkStreaming 的绝大部分使用场景都跟上面有所区别，SparkStreaming 是基于网络来接收数据的，数据源并非一定可靠。

SparkStreaming 要给所有生成的 RDD 实现同样的容错机制，通常有以下两种方法：

1. 接收数据，并在多个节点上做副本
2. 接收数据并进行缓存，但故障发生时只能重新从 Source 获取数据。

此外，还有两种类型的失败需要我们关注：

1. Worker 节点故障，此时 Worker 节点上所有的 executor 都会失败，并且存在内存中的数据都会丢失。
2. Driver 端故障，SparkContext 会丢失，此时所有 executor 都会失败且丢失数据。

对于任何流式处理系统，都可以简单抽象为三步：

1. **接收数据（Source）**，使用 Receivers 或者其他方式从数据源接收数据
2. **数据处理**，接收的数据会进行转化处理
3. **输出数据（Sink）**，最终处理好的数据会被推到外部存储系统，如文件系统、数据库、图表等。

如果一个流式应用必须实现**端到端**的 exactly-once 的保证，那么每个步骤都需要提供一个 exactly-once 的保证：

1. 接收数据，不同的数据源有不同的保证方式。
2. 数据处理，借助 RDD 提供的容错机制，只要数据源是可访问的，最后转化出来的 RDD 总会是一样的。
3. 输出数据，输出操作默认是 at-least-once 语义，这取决于输出操作（**是否幂等**）以及外部系统提供的语义（**是否支持事务**）。单用户也可以自行实现一套事务机制来实现 exactly-once 语义。

> NOTE：通常并不需要做到端到端的 exactly-once 语义，最后输出时能够做到 exactly-once 即可满足绝大部分场景。

## 2. Source 端 + 流式程序

### 2.1 文件系统数据源

如果所有文件都已经存放于一个可靠、容错的文件系统（如 HDFS），那么 SparkStreaming 总是能够从故障中恢复并处理所有的数据，实现 at-least-once 语义。

### 2.2 基于 Receiver 的数据源

对基于 Receiver 的输入源，容错的语义取决于故障场景以及 Receiver 的类型：

1. 可靠的 Receiver，这类 Receiver 会在确保数据做了多副本备份后再做确认。如果这类 Receiver 失败了，则数据源将不会接收到相关的 ack。因此，如果 Receiver 失败后重启，数据源会重新发送数据，此时不会有消息丢失。
2. 不可靠的 Receiver，这类 Receiver 不会发送 ack ，因此一旦出现故障，数据将会丢失。

为了避免数据丢失，Spark 在 1.2 退推出了 wal（*write ahead logs*）机制，可以将接收到的数据保存在一个可容错的存储中，可以通过 `spark.streaming.receiver.writeAheadLog.enable` 开启，Spark 会定期存储接收到的数据。这种方式下吞吐量会有影响。

SparkStreaming 消费 Kafka 的其中一种方式就是使用 Receiver 方式。

executor 拉取数据后会组装为 block，并将 block metadata 报告给 Driver。如果 executor 来不及将 block metadata 汇报给 Driver 就挂了，则会通过 zk 获取 offset 然后重新读取数据。

Driver 会根据这些信息进行管理，executor 端也会利用 wal 机制将数据存储到可靠的外部存储系统，实现 at-least 语义，因此需要输出操作实现幂等性

### 2.3 Kafka Direct API

从 Spark 1.3 开始推出了 Kafka 低阶 API，只需要实现 exactly-once 的输出操作，即可实现端到端 exactly-once 的保障。

以下面的 SparkStreaming + Kafka 为例：

```scala
import org.apache.kafka.clients.consumer.ConsumerRecord
import org.apache.kafka.common.serialization.StringDeserializer
import org.apache.spark.streaming.kafka010._
import org.apache.spark.streaming.kafka010.LocationStrategies.PreferConsistent
import org.apache.spark.streaming.kafka010.ConsumerStrategies.Subscribe

val kafkaParams = Map[String, Object](
  "bootstrap.servers" -> "localhost:9092,anotherhost:9092",
  "key.deserializer" -> classOf[StringDeserializer],
  "value.deserializer" -> classOf[StringDeserializer],
  "group.id" -> "use_a_separate_group_id_for_each_stream",
  "auto.offset.reset" -> "latest",
  "enable.auto.commit" -> (false: java.lang.Boolean)
)

val topics = Array("topicA", "topicB")
val stream = KafkaUtils.createDirectStream[String, String](
  streamingContext,
  PreferConsistent,
  Subscribe[String, String](topics, kafkaParams)
)

stream.map(record => (record.key, record.value))
```

LocationStrategies 的几种策略：

- **PreferConsistent**，绝大部分场景下都是使用该方式。这种方式会将 partition 均匀的分发到可用的 executor 上。
- **PreferBrokers**，如果允许在 Broker 上起 executor，则优先将 partition 分配到其 leader 节点上的 executor 上。
- **PreferFixed**，允许用户显式指定 host 与 partition 的映射关系，如果未指定则默认使用 PreferConsistent。

ConsumerStrategies 的几种策略：

- Subscribe，允许订阅一组固定的 topic
- SubscribePattern ，允许通过正则订阅一组固定的 topic
- Assign，允许指定固定的 partition。

SparkStreaming 可以通过以下方式获取当前消费的 offset：

```scala
stream.foreachRDD { rdd =>
  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
  rdd.foreachPartition { iter =>
    val o: OffsetRange = offsetRanges(TaskContext.get.partitionId)
    println(s"${o.topic} ${o.partition} ${o.fromOffset} ${o.untilOffset}")
  }
}
```

> NOTE： HasOffsetRanges 只能在作为 `createDirectStream` 生成结果调用的第一个方法才能成功转化。

Kafka 在故障场景下实现的语义**取决于 offset 是如何以及何时存储**。Spark 的输出操作是 at-least-once，因此如果想要实现 exactly-once，**要么在进行一个幂等性操作后存储 offset，要么将 offset 与输出以事务的方式进行存储**。

这里讨论几种 offset 存储的方式。

**Checkpoint** 方式，Spark 会将 offset 存储在 checkpoint 中，但存在以下几个缺点：

- 要求**输出操作必须是幂等的**，checkpoint 保证的时 at-least-once 语义，因此会存在重复的输出。
- 代码升级时会导致 checkpoint 失效，此时需要其他方式来定位 offset。

**Kafka 方式**，其自身也提供一个提交 offset 的 API，并会将 offset 存储到一个特定的 topic 里。默认情况下，新版本的 consumer 会定期自动提交 offset。通常用户并不希望会自动提交，而是手动控制。

由于 Kafka 提交 offset 与数据输出操作并不在一个事务中，只能保证消费是 at-least-once，因此需要**接收方能支持幂等性**来实现 exactly-once。

```scala
stream.foreachRDD { rdd =>
  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges
  // your computing ...
  // some time later, after outputs have completed
  stream.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
}
```

**自定义的数据存储**方式，这种方式最为万能，但也最为复杂，通过加数据输出与 offset 存储放到同个事务中，一旦故障发生，则会数据与 offset 会一同回滚，从而实现 exactly-once 语义。**这种方式最为万能，甚至可以针对存在 Shuffle、Aggregation 这类通常难以实现输出幂等性的场景**。

```scala
val fromOffsets = selectOffsetsFromYourDatabase.map { resultSet =>
  new TopicPartition(resultSet.string("topic"), resultSet.int("partition")) -> resultSet.long("offset")
}.toMap

val stream = KafkaUtils.createDirectStream[String, String](
  streamingContext,
  PreferConsistent,
  Assign[String, String](fromOffsets.keys.toList, kafkaParams, fromOffsets)
)

stream.foreachRDD { rdd =>
  val offsetRanges = rdd.asInstanceOf[HasOffsetRanges].offsetRanges

  val results = yourCalculation(rdd)

  // begin your transaction

  // update results
  // update offsets where the end of existing offsets matches the beginning of this batch of offsets
  // assert that offsets were updated correctly

  // end your transaction
}
```

> NOTE：幂等性解决方案通常只能应对 Map-Only 的场景，存在 Shuffle、Aggregation 等操作（即一个 executor 对应的数据会有变化）的场景，通常得使用事务来实现。

## 3. 流式程序 + Sink 端

前面提到，Source + SparkStreaming 能够实现 at-least-once 的输出操作，也就是说可能存在重复多次的输出。若要实现端到端的 exactly-once，则最后需要 Sink 端提供幂等性操作或者实现事务功能。

幂等性存在一种局限，就是有些操作会打乱分区对应关系，导致故障恢复后重新执行计算，分区的数据不一定与上次对应。因此当遇到并非 Map-Only 的输出时，需要注意是否会由于该问题导致 exactly-once 失效。

事务功能则可以应对一切场景，当数据成功输出时，SparkStreaming 数据处理成功；当数据输出出现问题时，SparkStreaming 与输出数据一起回滚，重新计算，流程如下：

- 使用 batch time 以及 RDD 的分区编号创建一个唯一标识，用于识别流式应用中的数据。
- 以事务方式原子性地提交这个唯一标识与数据；如果发现标识已被提交，则跳过更新。

下面讨论几种常见的场景，均以 SparkStreaming + Kafka 为前提。

### 3.1 文件系统实现幂等性

如果是输出到外部文件系统，那么只需要简单的执行 `saveAs***Files` 操作，通过 overwriten 覆盖，那么无论输出多少次都会是同样的结果，overwriten 是天生幂等性的，从而实现 exactly-once 语义。

### 3.2 MySql 实现幂等性

传统的 OLTP 数据库基本都提供了该功能，只要是通过主键 ID 或者指定某些 Key 进行去重。

首先通过 sbt 引入相关依赖包：

```scala
scalaVersion := "2.11.11"

libraryDependencies ++= Seq(
  "org.apache.spark" %% "spark-streaming" % "2.2.0" % "provided",
  "org.apache.spark" %% "spark-streaming-kafka-0-10" % "2.2.0",
  "org.scalikejdbc" %% "scalikejdbc" % "3.0.1",
  "mysql" % "mysql-connector-java" % "5.1.43"
)
```

建表语句如下：

```sql
create table error_log (
  log_time datetime primary key,
  log_count int not null default 0
);
```

借助主键实现幂等性输出：

```scala
  DB.autoCommit { implicit session =>
    result.foreach { case (time, count) =>
      sql"""
      insert into error_log (log_time, log_count)
      value (${time}, ${count})
      on duplicate key update log_count = log_count + values(log_count)
      """.update.apply()
    }
  }
  // 提交 offset
  messages.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
```

### 3.3 Mysql 实现事务

Mysql 在存储数据时查询一下 SparkStreamig 对数据的唯一标识是否已经提交，如果是跳过更新。

```scala
  DB.autoCommit { implicit session =>
    result.foreach { case (time, count) =>
      sql"""
      insert into error_log (log_time, log_count)
      value (${time}, ${count})
      where not exists (
      select batch_time, partitionId from topic_offset
      )
      """.update.apply()
    }
  }
  // 提交 offset
  messages.asInstanceOf[CanCommitOffsets].commitAsync(offsetRanges)
```

### 3.4 Kafka 实现幂等性

http://matt33.com/2018/10/24/kafka-idempotent/

### 3.5 Kafka 实现事务性

http://matt33.com/2018/11/04/kafka-transaction/



## 参考：

- [Fault-tolerance Semantics](https://spark.apache.org/docs/2.1.2/streaming-programming-guide.html#fault-tolerance-semantics)
- [Spark Streaming 中如何实现 Exactly-Once 语义](http://shzhangji.com/cnblogs/2017/08/01/how-to-achieve-exactly-once-semantics-in-spark-streaming/)

