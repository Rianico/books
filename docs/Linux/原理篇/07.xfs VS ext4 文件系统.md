---
title: xfs VS ext4 文件系统
date: 2020-07-23 09:51:57
---

## 1. xfs安装

最新搭建Hadoop集群，了解到了xfs文件系统，xfs官网地址：[https://xfs.org/index.php/Main\_Page](https://xfs.org/index.php/Main_Page)

磁盘：

| device | parted | comment |
| :--- | :--- | :--- |
| /dev/sdb | /data1 | HDD，7200转 |
| /dev/sdc | /data2 | HDD，7200转 |
| ... | ... | ... |
| /dev/sdm | /data12 | HDD，7200转 |

1.首先安装xfs

```bash
yum -y install xfsprogs
```

2.对磁盘进行分区，可参考下列脚本，注意parted的时候使用`mkpart primary 1 100%`达到逻辑分区对齐的作用

```bash
#! /bin/bash
i=1
while [ $i -le 12 ]
do
j=`echo $i|awk '{printf "%c",97+$i}'`
umount /data$i
parted /dev/sd$j <<FORMAT
rm 1
mklabel gpt
Ignore
Yes
mkpart primary 1 100%
quit
FORMAT
let i=i+1
done
```

3.格式化分区并挂载

```bash
#! /bin/bash
i=1
while [ $i -le 12 ]
do
j=`echo $i|awk '{printf "%c",97+$i}'`
mkfs.xfs -f /dev/sd${j}1
mkdir -p /data$i
mount /dev/sd${j}1 /data$i
FORMAT
let i=i+1
done
```

4.修改`/etc/fstab`添加以下配置

```properties
/dev/sdb1              /data1                  xfs    defaults        1 2
/dev/sdc1              /data2                  xfs    defaults        1 2
...
/dev/sdm1              /data12                  xfs    defaults        1 2
```

5.执行`df -Th`检查挂载以及格式化

```properties
/dev/sdb1      xfs    5.5T   34M  5.5T   1% /data1
/dev/sdc1      xfs    5.5T   34M  5.5T   1% /data2
...
/dev/sdm1      xfs    5.5T   34M  5.5T   1% /data12
```

## 2. xfs 与ext4 的性能对比

为验证 xfs 对比原来 ext4 的读写性能，现对两种文件系统的 I/O 表现做一个测试对比

各磁盘文件系统参数如下：

```bash
# xfs,默认配置
mkfs.xfs -f /dev/sdb1
# ext4,默认配置
mkfs.ext4 /dev/sdc1
```

### 2.1 dd命令测试

首先使用 dd 命令进行测试，总共 10G 文件读取写入，测试磁盘的顺序读写性能，绕过磁盘 cache，测试命令如下（以 `/dev/sdb1` 挂载在 `/data1` 为例）：

```bash
# 写测试，每次写入4K，共10G,使用oflag=direct绕过cache
sync;time dd if=/dev/zero of=/data1/test.log bs=4k count=2500000 oflag=direct
# 读测试，每次读取4K，共10G,使用iflag=direct绕过cache
sync;time dd if=/dev/sdb1 of=/dev/null bs=4k count=2500000 iflag=direct

# 写测试，每次写入1M，共10G,使用oflag=direct绕过cache
sync;time dd if=/dev/zero of=/data1/test.log bs=1M count=10000 oflag=direct
# 读测试，每次读取1M，共10G,使用iflag=direct绕过cache
sync;time dd if=/dev/sdb1 of=/dev/null bs=1M count=10000 iflag=direct
```

| 磁盘类型 | 读速度\(4KB/per block\) | 写速度（4KB/per block\) | 读速度\(1MB/per block\) | 写速度（1MB/per block\) |
| :---: | :---: | :---: | :---: | :---: |
| xfs | **47.8 MB/秒** | **42.2 MB/秒** | 225 MB/秒 | **265 MB/秒** |
| ext4 | 47.2 MB/秒 | 38.3 MB/秒 | 225 MB/秒 | 264 MB/秒 |

结论：从上面可以看出，xfs 从综合读写来看，都会比 ext4 略强，但差距十分小，还不能够说有明显提升。

### 2.2 fio测试

接下来使用 fio 工具进行更加具体的测试，安装详见[github](https://github.com/axboe/fio)。

fio 执行命令如下（以`/dev/sdb1`为例）：

```bash
sync;fio -name=disktest -runtime=300 -iodepth=32 -ioengine=libaio -group_reporting -direct=1 -numjobs=1 -thread -rw=rw -bs=4k -filename=/dev/sdb1
```

* name：测试名称，随便取个即可
* runtime：运行时间为 300s
* rw：读写模式，以 rw（读写混合），randrw（随机读写混合）为测试用例
* bs：测试使用的块大小
* iodepth：io 队列最大深度
* filename：测试的磁盘
* direct：1 表示绕过磁盘 cache
* numjobs：指定线程数量
* thread：fio 默认会使用 fork\(\) 创建 job，如果这个选项设置的话，fio 将使用 pthread\_create 来创建线程
* group\_reporting：查看统计后的综合报告

**单线程下的吞吐量 & iops（numjobs=1）**

| 读写类型 | xfs\_4k | ext4\_4k | xfs\_1M | ext4\_1M |
| :---: | :---: | :---: | :---: | :---: |
| 顺序读 | 118MB/s & 28.8k | **120MB/s & 29.3k** | 145MB/s & 138 | **151MB/s & 143** |
| 顺序写 | 118MB/s & 28.8k | **120MB/s & 29.3k** | 144MB/s & 137 | **151MB/s & 142** |
| 随机读 | 780kB/s & 190 | **791kB/s & 193** | **50.6MB/s & 48** | 38.2MB/s & 36 |
| 随机写 | 776kB/s & 189 | **788kB/s & 192** | **50.4MB/s & 48** | 37.0MB/s & 36 |

**16 线程下的吞吐量 & iops（numjobs=16）**

| 读写类型 | xfs\_4k | ext4\_4k | xfs\_1M | ext4\_1M |
| :---: | :---: | :---: | :---: | :---: |
| 顺序读 | 194MB/s & 47.5k | **196MB/s & 47.9k** | **2268MB/s & 2182** | 2229MB/s & 2125 |
| 顺序写 | 195MB/s & 47.5k | **197MB/s & 47.0k** | **2279MB/s & 2172** | 2241MB/s & 2136 |
| 随机读 | **850kB/s & 207** | 845kB/s & 206 | 49.0MB/s & 47 | **50.7MB/s & 48** |
| 随机写 | **858kB/s & 209** | 852kB/s & 208 | 50.8MB/s & 48 | **51.5MB/s & 49** |

**32 线程下的吞吐量 & iops（numjobs=32）**

| 读写类型 | xfs\_4k | ext4\_4k | xfs\_1M | ext4\_1M |
| :---: | :---: | :---: | :---: | :---: |
| 顺序读 | **199MB/s & 48.6k** | 195MB/s & 47.6k | **2208MB/s & 2107** | 2207MB/s & 2104 |
| 顺序写 | **199MB/s & 48.6k** | 195MB/s & 47.6k | **2211MB/s & 2109** | 2209MB/s & 2107 |
| 随机读 | **809kB/s & 197** | 804kB/s & 196 | **49.9MB/s & 47** | 49.8MB/s & 47 |
| 随机写 | **809kB/s & 197** | 802kB/s & 195 | **50.2MB/s & 47** | 50.1MB/s & 47 |

可以看到，随着线程数增加，xfs 性能逐渐超过 ext4，但差距仍然很小，最后再看下 32 线程下，5MB（取Kafka接收消息的最大大小）顺序读写的对比。

**32 线程，5MB 的吞吐量 & iops（numjobs=32）**

| 读写类型 | xfs\_5M | ext4\_5M |
| :---: | :---: | :---: |
| 顺序读 | **1399MB/s & 266** | 1392MB/s & 265 |
| 顺序写 | **1396MB/s & 266** | 1390MB/s & 265 |

由上趋势可以看出，**xfs 并不能带来显著的提升，但是可以看出在负载越大的情况下，xfs的稳定性相对会更好**。

> NOTE：xfs 其实已经相当成熟，基本不需要其他优化，网上的调优资料都是好几年前的了，按着调整只会造成反效果（个人亲身经历，瞎折腾一天），官网也写明了默认参数基本已经有了最好的性能表现。

上述测试是在 HDD 硬盘上进行的，在 HDD 硬盘方面得出的结论跟 [XFS vs EXT4 – Comparing MongoDB Performance on AWS EC2](https://scalegrid.io/blog/xfs-vs-ext4-comparing-mongodb-performance-on-aws-ec2) 基本一致，文章大致表达的意思是：在高性能磁盘下，xfs 对比 ext4 性能上高了一个级别，而在中小型机器的资源下，xfs 并没法带来肉眼可见的提升。由于没有条件进行 SDD 的测试，所以这篇文章对 SDD 的测试可以作为一个参考。

## 3. 结论

关于 xfs 以及 ext4 的对比网上有很多资料，结合知乎的一个问答[为什么CENTOS 7.0开始选择XFS作为默认的文件系统？XFS相比ext有什么优点？](https://www.zhihu.com/question/24413471/answer/38883787)，个人认为，在未来数据量未来越大的情况下，ext4 的单个文件大小最大只支持到 16T，未来可能会做改进或者出现 ext5 等新格式，但是省电事情总是好的，因此 xfs 从稳定性上来看会更好。

## 4. xfs 的一些注意事项

xfs 格式化的时候，可以指定 block 块的大小，但需要主要不能超过系统的 `PAGE_SIZE`，否则会挂载失败。

```

```

